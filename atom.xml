<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>JacobKong Blog</title>
  <subtitle>路漫漫其修远兮......</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-02-01T01:48:25.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Jacob Kong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深度学习论文笔记：Faster R-CNN</title>
    <link href="http://yoursite.com/posts/3802700508/"/>
    <id>http://yoursite.com/posts/3802700508/</id>
    <published>2016-12-16T22:32:24.000Z</published>
    <updated>2017-02-01T01:48:25.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li><strong>Region Proposal的计算</strong>是基于Region Proposal算法来假设物体位置的物体检测网络比如：<strong>SPPnet, Fast R-CNN</strong>运行时间的瓶颈。</li>
<li>Faster R-CNN引入了<strong>Region Proposal Network（RPN）</strong>来和检测网络共享整个图片的卷积网络特征，因此使得region proposal几乎是<strong>cost free</strong>的。</li>
<li>RPN-&gt;预测物体边界（object bounds）和在每一位置的分数（objectness score）</li>
<li>通过在一个网络中共享RPN和Fast R-CNN的卷积特征来融合两者——使用“attention”机制。</li>
<li>300 proposals pre image.</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>RP是当前许多先进检测系统的瓶颈。</li>
<li>Region proposal methods:<ul>
<li>Selective Search: one of the most popular method </li>
<li>EdgeBoxes: trade off between proposal quality and speed.</li>
<li>region proposal这一步依旧和检测网络花费同样多的时间。</li>
</ul>
</li>
<li>Fast R-CNN生成的feature map 也能用来生成RP。在这些卷积特征之上我们通过这样的方式构建RPN：通过添加几个额外的卷积层来模拟一个regular grid上每一个位置的regress region bounds和objectness scores。<strong>所以RPN也是一种fully convolutional network(FCN)</strong>，从而可以端到端训练来产生detection proposals。</li>
<li><strong>anchor boxes</strong>：references at multiple scales and aspect ratios. 我们的方法可以看成pyramid of regression reference，从而避免枚举多尺寸、多横纵比的images或者filters</li>
</ul>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul>
<li>R-CNN主要是一个分类器，他不能预测object bounds，他的准确性依赖于Region proposal模块的表现</li>
</ul>
<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><ul>
<li>由两个模块组成：<ul>
<li>第一个模块：A deep fuuly convolutional network that proposes regions.</li>
<li>第二个模块：Fast R-CNN检测器</li>
</ul>
</li>
<li><strong>Attention mechanisms</strong>：RPN module告诉Fast R-CNN module 往哪里看（where to look）</li>
</ul>
<h3 id="Region-Proposal-Networks"><a href="#Region-Proposal-Networks" class="headerlink" title="Region Proposal Networks"></a>Region Proposal Networks</h3><ul>
<li>输入：一张任意尺寸的图片</li>
<li>输出：一组矩形object proposal</li>
<li>A fully convolutional network</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Region Proposal的计算&lt;/strong&gt;是基于Region Proposal算法来假设物体位置的物体检测网络比如：&lt;strong&gt;SPPnet, Fast R-CNN&lt;/strong&gt;运行时间的瓶颈。&lt;/li&gt;
&lt;li&gt;Faster R-CNN引入了&lt;strong&gt;Region Proposal Network（RPN）&lt;/strong&gt;来和检测网络共享整个图片的卷积网络特征，因此使得region proposal几乎是&lt;strong&gt;cost free&lt;/strong&gt;的。&lt;/li&gt;
&lt;li&gt;RPN-&amp;gt;预测物体边界（object bounds）和在每一位置的分数（objectness score）&lt;/li&gt;
&lt;li&gt;通过在一个网络中共享RPN和Fast R-CNN的卷积特征来融合两者——使用“attention”机制。&lt;/li&gt;
&lt;li&gt;300 proposals pre image.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://yoursite.com/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：Fast R-CNN</title>
    <link href="http://yoursite.com/posts/1679631826/"/>
    <id>http://yoursite.com/posts/1679631826/</id>
    <published>2016-12-07T22:32:24.000Z</published>
    <updated>2017-01-28T09:01:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><ul>
<li>mAP：detection quality.</li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>本文提出一种基于快速区域的卷积网络方法（快速R-CNN）用于对象检测。</li>
<li>快速R-CNN采用多项创新技术来提高训练和测试速度，同时提高检测精度。</li>
<li>采用VGG16的网络：VGG: 16 layers of 3x3 convolution interleaved with max pooling + 3 fully-connected layers</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>物体检测相对于图像分类是更复杂的，应为需要物体准确的位置。<ul>
<li>首先，必须处理许多候选对象位置（通常称为“proposal”）。</li>
<li>其次，这些候选者只提供粗略的定位，必须进行精确定位才能实现精确定位。</li>
<li>这些问题的解决方案经常损害 <strong>速度</strong> ， <strong>准确性</strong> 或 <strong>简单性</strong> 。</li>
</ul>
</li>
</ul>
<h3 id="R-CNN-and-SPPnet"><a href="#R-CNN-and-SPPnet" class="headerlink" title="R-CNN and SPPnet"></a>R-CNN and SPPnet</h3><ul>
<li>R-CNN(Region-based Convolution Network)具有几个显著的缺点：<ul>
<li>训练是一个多级管道。</li>
<li>训练在空间和时间上是昂贵的。</li>
<li>物体检测速度很慢。</li>
</ul>
</li>
<li>R-CNN是慢的，因为它对每个对象proposal执行ConvNet正向传递，而不共享计算（sharing computation）。</li>
<li>Spatial pyramid pooling networks（SPPnets），利用sharing computation对R-CNN进行了加速，但是SPPnets也具有明显的缺点，像R-CNN一样，SPPnets也需要：<ul>
<li>训练是一个多阶段流程，</li>
<li>涉及提取特征，</li>
<li>用对数损失精简网络</li>
<li>训练SVM</li>
<li>赋予边界框回归。</li>
<li>特征也需要也写入磁盘。</li>
</ul>
</li>
<li>但与R-CNN <strong>不同</strong> ，在[11]中提出的fine-tuning算法不能更新在空间金字塔池之前的卷积层。 不出所料，这种限制（固定的卷积层）限制了非常深的网络的精度。</li>
</ul>
<h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><ul>
<li>Fast R-CNN优点：</li>
</ul>
<ol>
<li>比R-CNN，SPPnet更高的检测质量（mAP）</li>
<li>训练是单阶段的，使用多任务损失（multi-task loss）</li>
<li>训练可以更新所有网络层</li>
<li>特征缓存不需要磁盘存储</li>
</ol>
<h2 id="Fast-R-CNN-architecture-and-training"><a href="#Fast-R-CNN-architecture-and-training" class="headerlink" title="Fast R-CNN architecture and training"></a>Fast R-CNN architecture and training</h2><ul>
<li><p>整体框架<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfo7na6fij30jk0efdju.jpg" alt=""></p>
</li>
<li><p>快速R-CNN网络将整个图像和一组对象位置作为输入。</p>
<ul>
<li>网络首先使用几个卷积（conv）和最大池层来处理整个图像，以产生conv feature map。</li>
<li>然后，对于每个对象proposal， <strong>感兴趣区域（RoI）池层</strong> 从特征图中抽取固定长度的特征向量。</li>
<li>每个特征向量被馈送到完全连接（fc）层序列，其最终分支成两个同级输出层：<ul>
<li>一个产生对K个对象类加上全部捕获的“背景”类的softmax概率估计(one that produces softmax probability estimates over K object classes plus a catch-all “background” class)</li>
<li>另一个对每个K对象类输出四个实数，每组4个值编码提炼定义K个类中的一个的的边界框位置。(another layer that outputs four real-valued numbers for each of the K object classes. Each set of 4 values encodes reﬁned bounding-box positions for one of the K classes.)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="The-RoI-pooling-layer"><a href="#The-RoI-pooling-layer" class="headerlink" title="The RoI pooling layer"></a>The RoI pooling layer</h3><ul>
<li>Rol pooling layer的作用主要有两个：<ul>
<li>一个是将image中的RoI定位到feature map中对应patch</li>
<li>另一个是用一个单层的SPP layer将这个feature map patch下采样为大小固定的feature再传入全连接层。</li>
</ul>
</li>
<li>RoI池层使用最大池化将任何有效的RoI区域内的特征转换成具有H×W（例如，7×7）的固定空间范围的小feature map，其中H和W是层超参数 它们独立于任何特定的RoI。</li>
<li>在本文中，RoI是conv feature map中的一个矩形窗口。</li>
<li>每个RoI由定义其左上角（r，c）及其高度和宽度（h，w）的四元组（r，c，h，w）定义。</li>
<li>RoI层仅仅是Sppnets中的spatial pyramid pooling layer的特殊形式，其中只有一个金字塔层</li>
</ul>
<h3 id="Initializing-from-pre-trained-networks"><a href="#Initializing-from-pre-trained-networks" class="headerlink" title="Initializing from pre-trained networks"></a>Initializing from pre-trained networks</h3><ul>
<li>用了3个预训练的ImageNet网络（CaffeNet/ VGG_CNN_M_1024 /VGG16）。预训练的网络初始化Fast RCNN要经过三次变形：</li>
</ul>
<ol>
<li>最后一个max pooling层替换为RoI pooling层，设置H’和W’与第一个全连接层兼容。</li>
<li>最后一个全连接层和softmax（原本是1000个类）替换为softmax的对K+1个类别的分类层，和bounding box 回归层。</li>
<li>输入修改为两种数据：一组N个图形，R个RoI，batch size和ROI数、图像分辨率都是可变的。</li>
</ol>
<h3 id="Fine-tuning-for-detection"><a href="#Fine-tuning-for-detection" class="headerlink" title="Fine-tuning for detection"></a>Fine-tuning for detection</h3><ul>
<li>利用反向传播算法进行训练所有网络的权重是Fast R-CNN很重要的一个能力。</li>
<li>我们提出了一种更有效的训练方法，利用在训练期间的特征共享（feature sharing during training）。</li>
<li>在Fast R-CNN训练中， <strong>随机梯度下降（SGD）小批量分层采样</strong> ，首先通过采样N个图像，然后通过从每个图像采样 <strong>R/N个</strong> RoIs。</li>
<li>关键的是，来自同一图像的RoI在向前和向后传递中 <strong>共享计算</strong> 和存储。</li>
<li>此外为了分层采样，Fast R-CNN使用了一个流水线训练过程，利用一个fine-tuning阶段来联合优化一个softmax分类器和bounding box回归，而非训练一个softmax分类器，SVMs，和regression在三个独立的阶段。</li>
<li><p>Multi-task loss：</p>
<ul>
<li>两个loss，以下分别介绍：<ul>
<li>对于分类loss，是一个N+1路的softmax输出，其中的N是类别个数，1是背景。</li>
<li>对于回归loss，是一个4xN路输出的regressor，也就是说对于每个类别都会训练一个单独的regressor的意思，比较有意思的是，这里regressor的loss不是L2的，而是一个平滑的L1，形式如下：</li>
</ul>
</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfo7oia0ij30bg05n74f.jpg" alt=""></p>
</li>
<li><p>我们利用一个multi-task loss L 在每个被标注的RoI上来联合训练分类器和bounding box regression</p>
</li>
<li>Mini-batch sampling：在微调时，每个SGD的mini-batch是随机找两个图片，R为128，因此每个图上取样64个RoI。从object proposal中选25%的RoI，就是和ground-truth交叠至少为0.5的。剩下的作为背景。</li>
<li><p>Back-propagation through RoI pooling layers：</p>
<ul>
<li><p>RoI pooling层计算损失函数对每个输入变量x的偏导数，如下：</p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfo7owdcpj306q01mwee.jpg" alt=""></p>
<p>y是pooling后的输出单元，x是pooling前的输入单元，如果y由x pooling而来，则将损失L对y的偏导计入累加值，最后累加完R个RoI中的所有输出单元。下面是我理解的x、y、r的关系：</p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfoo7cuv7j30qf0ardgq.jpg" alt="20151208163114338"></p>
</li>
</ul>
</li>
</ul>
<h3 id="Scale-invariance"><a href="#Scale-invariance" class="headerlink" title="Scale invariance"></a>Scale invariance</h3><ul>
<li>这里讨论object的scale问题，就是网络对于object的scale应该是要不敏感的。这里还是引用了SPP的方法，有两种:<ul>
<li>brute force （single scale），也就是简单认为object不需要预先resize到类似的scale再传入网络，直接将image定死为某种scale，直接输入网络来训练就好了，然后期望网络自己能够学习到scale-invariance的表达。</li>
<li>image pyramids （multi scale），也就是要生成一个金字塔，然后对于object，在金字塔上找到一个大小比较接近227x227的投影版本，然后用这个版本去训练网络。</li>
</ul>
</li>
<li>可以看出，2应该比1更加好，作者也在5.2讨论了，2的表现确实比1好，但是好的不算太多，大概是1个mAP左右，但是时间要慢不少，所以作者实际采用的是第一个策略，也就是single scale。</li>
<li>这里，FRCN测试之所以比SPP快，很大原因是因为这里，因为SPP用了2，而FRCN用了1。</li>
</ul>
<h2 id="Fast-R-CNN-detection"><a href="#Fast-R-CNN-detection" class="headerlink" title="Fast R-CNN detection"></a>Fast R-CNN detection</h2><ul>
<li>大型全连接层很容易的可以通过将他们与 <strong>truncated SVD(奇异值分解)</strong> 压缩来加速计算。</li>
</ul>
<h2 id="Main-results"><a href="#Main-results" class="headerlink" title="Main results"></a>Main results</h2><ul>
<li>All Fast R-CNN results in this paper using VGG16 ﬁne-tune layers conv3 1 and up; all experments with models S and M ﬁne-tune layers conv2 and up.</li>
</ul>
<h2 id="Design-evaluation"><a href="#Design-evaluation" class="headerlink" title="Design evaluation"></a>Design evaluation</h2><h3 id="Do-we-need-more-training-data"><a href="#Do-we-need-more-training-data" class="headerlink" title="Do we need more training data?"></a>Do we need more training data?</h3><ul>
<li>在训练期间，作者做过的唯一一个数据增量的方式是水平翻转。 作者也试过将VOC12的数据也作为拓展数据加入到finetune的数据中，结果VOC07的mAP从66.9到了70.0，说明对于网络来说， <strong>数据越多就是越好的。</strong></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;知识点&quot;&gt;&lt;a href=&quot;#知识点&quot; class=&quot;headerlink&quot; title=&quot;知识点&quot;&gt;&lt;/a&gt;知识点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;mAP：detection quality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;本文提出一种基于快速区域的卷积网络方法（快速R-CNN）用于对象检测。&lt;/li&gt;
&lt;li&gt;快速R-CNN采用多项创新技术来提高训练和测试速度，同时提高检测精度。&lt;/li&gt;
&lt;li&gt;采用VGG16的网络：VGG: 16 layers of 3x3 convolution interleaved with max pooling + 3 fully-connected layers&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://yoursite.com/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</title>
    <link href="http://yoursite.com/posts/3054155989/"/>
    <id>http://yoursite.com/posts/3054155989/</id>
    <published>2016-12-06T22:32:24.000Z</published>
    <updated>2017-01-28T09:00:55.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>现有的深卷积神经网络（CNN）需要固定尺寸（例如，224×224）的输入图像。</li>
<li>新的网络结构，称为SPP-net，可以生成固定长度的表示，而不管图像大小/规模。</li>
<li>使用SPP-net，我们从整个图像只计算一次特征图，然后在任意区域（子图像）中池特征以生成固定长度表示以训练检测器。</li>
</ul>
<a id="more"></a>
<h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><ul>
<li>在CNN的训练和测试中存在技术问题：普遍的CNN需要固定的输入图像大小（例如，224×224），其限制了输入图像的宽高比和比例。</li>
<li>Cropping</li>
<li>Warping-&gt;unwanted geometric distortion(不需要的几何失真)</li>
<li><p>那么为什么CNN需要固定输入大小？</p>
<ul>
<li>CNN主要由两部分组成：卷积层和跟随的完全连接的层。</li>
<li>事实上，卷积层不需要固定的图像大小，并且可以生成任何大小的特征图</li>
<li>另一方面，根据定义：完全连接的层需要具有固定尺寸/长度输入。所以固定尺寸完全来自于 <strong>全连接层</strong></li>
</ul>
</li>
<li><p>我们提出了一个spatial pyramid pooling（空间金字塔池化层）来去掉额昂罗固定输入的约束。</p>
</li>
<li><p>具体来说，我们在最后一个卷积层的顶部添加一个SPP层。 SPP层汇集特征并产生固定长度的输出，然后馈送到完全连接的层（或其他分类器）。</p>
</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfnxkqg21j30jl03hwf2.jpg" alt=""></p>
<ul>
<li><p>SPP对于深度CNN有着一些显著的特性：</p>
<ul>
<li>1）SPP能够生成固定长度的输出，而不管输入大小，而在以前的深度网络[3]中使用的滑动窗口池不能;</li>
<li>2）SPP使用多级空间仓，而滑动窗口池仅使用单个窗口大小。 多层池化已被证明对于对象变形是鲁棒的[15];</li>
<li>3）由于输入尺度的灵活性，SPP可以在可变尺度上提取的特征。</li>
</ul>
</li>
<li><p>实验表明，这种多尺寸训练与传统的单尺寸训练一样收敛，并导致更好的测试精度。</p>
</li>
<li><p>SPP的优点是与特定的CNN设计是正交的。</p>
</li>
<li><p>Caltech101: L. Fei-Fei, R. Fergus, and P. Perona, “Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories,” CVIU, 2007.</p>
</li>
<li><p>VOC 2007: M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results,” 2007.</p>
</li>
<li><p>但是R-CNN中的特征计算是耗时的，因为它对每个图像的数千个wraped区域的原始像素重复应用深卷积网络。而本文提出的方法可以在一整张图像上只跑一次卷积层</p>
</li>
</ul>
<h2 id="DEEP-NETWORKS-WITH-SPATIAL-PYRAMID-POOLING"><a href="#DEEP-NETWORKS-WITH-SPATIAL-PYRAMID-POOLING" class="headerlink" title="DEEP NETWORKS WITH SPATIAL PYRAMID POOLING"></a>DEEP NETWORKS WITH SPATIAL PYRAMID POOLING</h2><ul>
<li>输入图像中的这些形状激活在相应位置的feature map</li>
</ul>
<h3 id="The-Spatial-Pyramid-Pooling-Layer"><a href="#The-Spatial-Pyramid-Pooling-Layer" class="headerlink" title="The Spatial Pyramid Pooling Layer"></a>The Spatial Pyramid Pooling Layer</h3><ul>
<li>Bag-of-Words (BoW) approach-&gt;用来将生成的特征进行pool从而产生固定长度的向量。</li>
<li>空间金字塔池提高BoW，因为它可以通过在局部空间仓中汇集来 <strong>维护空间信息</strong> 。</li>
<li><p>“global pooling” operation</p>
<ul>
<li>a global average pooling</li>
<li>a global average pooling</li>
</ul>
</li>
</ul>
<h3 id="Training-the-Network"><a href="#Training-the-Network" class="headerlink" title="Training the Network"></a>Training the Network</h3><ul>
<li>Single-size training</li>
<li>Multi-size training</li>
</ul>
<h2 id="SPP-NET-FOR-IMAGE-CLASSIFICATION"><a href="#SPP-NET-FOR-IMAGE-CLASSIFICATION" class="headerlink" title="SPP-NET FOR IMAGE CLASSIFICATION"></a>SPP-NET FOR IMAGE CLASSIFICATION</h2><h2 id="SPP-NET-FOR-OBJECT-DETECTION"><a href="#SPP-NET-FOR-OBJECT-DETECTION" class="headerlink" title="SPP-NET FOR OBJECT DETECTION"></a>SPP-NET FOR OBJECT DETECTION</h2><ul>
<li>对于R-CNN来说，Feature extraction is the major timing bottleneck in testing.</li>
<li>对于我们的SPP-net来说，我们从一整张图片中值提取一次特征。</li>
<li>On the contrary, our method enables feature extraction in <strong>arbitrary windows</strong> from the deep convolutional feature maps.</li>
</ul>
<h3 id="Detection-Algorithm"><a href="#Detection-Algorithm" class="headerlink" title="Detection Algorithm"></a>Detection Algorithm</h3><p><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfnxjqg0ej30h50lfai3.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;现有的深卷积神经网络（CNN）需要固定尺寸（例如，224×224）的输入图像。&lt;/li&gt;
&lt;li&gt;新的网络结构，称为SPP-net，可以生成固定长度的表示，而不管图像大小/规模。&lt;/li&gt;
&lt;li&gt;使用SPP-net，我们从整个图像只计算一次特征图，然后在任意区域（子图像）中池特征以生成固定长度表示以训练检测器。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://yoursite.com/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：Rich feature hierarchies for accurate object detection and semantic segmentation</title>
    <link href="http://yoursite.com/posts/4241353321/"/>
    <id>http://yoursite.com/posts/4241353321/</id>
    <published>2016-12-05T22:32:24.000Z</published>
    <updated>2017-01-28T09:00:43.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>mAP: mean average precision，平均准确度</li>
<li><p>我们的方法结合两个关键的见解：</p>
<ul>
<li>第一：采用高容量的卷积神经网络来从上到下的进行region proposal，从而实现定位和分割物体。</li>
<li>当标记的训练数据稀缺时，可以先对辅助数据集（任务）进行受监督的预训练， 随后是基于域进行特定调整，产生显着的性能提升。</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>关于各种视觉识别任务的上一个十年的进展主要基于SIFT和HOG的使用</li>
<li><p>实现这个结果需要解决两个问题：</p>
<ul>
<li>利用深度网络将对象定位</li>
<li>仅利用少量的注释检测数据来训练训练高容量模型。</li>
</ul>
</li>
<li>我们通过在“使用区域识别”范例内操作，来解决CNN定位问题</li>
<li>在测试时，我们的方法为输入图像生成大约2000个类别无关区域提案，使用CNN从每个proposal中提取固定长度的特征向量，然后使用类别特定的线性SVM对每个区域进行分类。</li>
<li>检测中面临的第二个挑战是标记的数据不足，目前可用的数据数量不足以训练大型CNN。这个问题的常规解决方案是使用无监督预训练，然后是监督 fine-tuning。</li>
<li>我们发现，对于CNN，有很大比例的参数（94%）可以在检测精度的适度降低的情况下被去除。</li>
<li>我们证明一个简单的 <strong>边界框回归方法（bounding box regression）</strong> 显着减少误定位，这是主要的误差模式(error mode)。</li>
<li>在开发技术细节之前，我们注意到，因为R-CNN在是区域上操作，所以很自然将其扩展到语义分割（semantic segmentation）的任务。</li>
</ul>
<h2 id="Object-detection-with-R-CNN"><a href="#Object-detection-with-R-CNN" class="headerlink" title="Object detection with R-CNN"></a>Object detection with R-CNN</h2><ul>
<li><p>我们的对象检测系统由三个模块组成:</p>
<ul>
<li>首先生成类别独立(category-independent)区域proposal。 这些proposal定义了可用于检测器的候选检测集合。</li>
<li>第二个模块是大卷积神经网络，从每个区域提取固定长度的特征向量。</li>
<li>第三个模块是一类特定类型的线性SVM。</li>
</ul>
</li>
</ul>
<h3 id="Module-design"><a href="#Module-design" class="headerlink" title="Module design"></a>Module design</h3><ul>
<li><p>Region proposals: 目前有很多用来生成category-independent的region proposal的方法：</p>
<ul>
<li>Objectness</li>
<li>selective search</li>
<li>category-independent object proposals</li>
<li>constrained parametric min-cuts (CPMC)</li>
<li>multi-scale combinatorial grouping</li>
<li>detect mitotic cells by applying a CNN to regularly-spaced square crops, which are a special case of region proposals.(通过将CNN应用于规则间隔的方形作物来检测有丝分裂细胞，这是区域提案的特殊情况。)</li>
</ul>
</li>
<li><p>虽然R-CNN与特定区域建议方法无关，但我们使用选择性搜索(selective search)来实现与先前检测工作的受控比较</p>
</li>
<li><p>Feature extraction:我们从每个区域提案中提取一个4096维特征向量，特征通过前向传播对227×227 RGB图像通过 <strong>五个卷积层和两个完全连接的层</strong> 计算。</p>
</li>
<li>无论候选区域的大小或宽高比如何，我们都会将其周围的紧密边界框中的所有像素装到所需的大小(227x227像素尺寸)。</li>
</ul>
<h3 id="Test-time-detection"><a href="#Test-time-detection" class="headerlink" title="Test-time detection"></a>Test-time detection</h3><ul>
<li>在测试时，我们对测试图像运行选择性搜索以提取大约2000个区域建议（我们在所有实验中使用选择性搜索的“快速模式（fast mode）”）。</li>
<li>给定图像中的所有得分区域，我们应用贪心非最大抑制(greedy non-maximum suppression)（对于每个类独立地），如果与的饭较高的区域有重叠，且IoU大于学习到的阈值，则该拒绝区域。</li>
<li><p>Run-time analysis.两个属性使检测更高校。</p>
<ul>
<li>首先，所有CNN参数在所有类别中共享。</li>
<li>第二，CNN计算的特征向量与其他常见方法（例如具有视觉词袋编码的空间棱金字塔）相比是 <strong>低维的</strong> 。</li>
<li>唯一的类特定(class-specific)计算是特征和SVM权重之间的点积和非最大抑制。</li>
</ul>
</li>
</ul>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ul>
<li>除了用随机初始化的21路分类层（对于20个VOC类加上背景）替换CNN的ImageNet特定的1000路分类层之外，CNN架构是不变的。</li>
<li>我们将所有region proposal与一个ground-truth重叠为IoU&gt;0.5，作为该框类的阳性，其余作为阴性。</li>
<li>我们以0.001的学习速率（初始预训练速率的1/10）开始SGD，这允许精细调整进行，而不是破坏初始化。</li>
<li>一旦提取特征并应用训练标签，我们对每个类优化一个线性SVM。</li>
<li>由于训练数据太大，无法记忆，我们采用标准 <strong>hard negative mining method</strong> 。</li>
</ul>
<h3 id="Results-on-PASCAL-VOC-2010-12"><a href="#Results-on-PASCAL-VOC-2010-12" class="headerlink" title="Results on PASCAL VOC 2010-12"></a>Results on PASCAL VOC 2010-12</h3><h2 id="Visualization-ablation-and-modes-of-error"><a href="#Visualization-ablation-and-modes-of-error" class="headerlink" title="Visualization, ablation, and modes of error"></a>Visualization, ablation, and modes of error</h2><h3 id="Visualizing-learned-features"><a href="#Visualizing-learned-features" class="headerlink" title="Visualizing learned features"></a>Visualizing learned features</h3><ul>
<li>pool-5，是网络第五个也是最后一个卷基层的max-pool层的输出。（是一个max-pooling层）</li>
<li>The pool-5 feature map is 6 × 6 × 256 = 9216维。</li>
<li>忽略边界效应，每个pool-5单元在原始227×227像素输入中具有195×195像素的接收场。</li>
</ul>
<h3 id="Ablation-studies"><a href="#Ablation-studies" class="headerlink" title="Ablation studies"></a>Ablation studies</h3><ul>
<li>Fc6与pool-5全连接，为了计算特征，他它将 <strong>4096×9216的权重矩阵乘以pool-5的feature map</strong> （重新形成为9216维矢量），然后添加偏差矢量。</li>
<li>Fc7是网络的最后一层，通过将由fc 6计算的特征乘以 <strong>4096×4096</strong> 权重矩阵，并类似地添加偏置矢量和应用半波整流来实现。</li>
<li>大多数CNN的表示能力来自它的卷积层，而不是来自大得多的密集连接的层。</li>
<li>All R-CNN variants strongly outperform the three DPM baselines</li>
</ul>
<h3 id="Detection-error-analysis"><a href="#Detection-error-analysis" class="headerlink" title="Detection error analysis"></a>Detection error analysis</h3><h3 id="Bounding-box-regression"><a href="#Bounding-box-regression" class="headerlink" title="Bounding box regression"></a>Bounding box regression</h3><h2 id="Semantic-segmentation"><a href="#Semantic-segmentation" class="headerlink" title="Semantic segmentation"></a>Semantic segmentation</h2><ul>
<li>full</li>
<li>fg</li>
<li>full+fg</li>
<li>The fg strategy slightly outperforms full, indicating that the masked region shape provides a stronger signal, matching our intuition.</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>之前最好的性能系统是将多个低级图像特征与来自对象检测器和场景分类器的高级上下文组合在一起的复杂集合。</li>
<li>本文提出了一个简单和可扩展的对象检测算法，与PASCAL VOC 2012上的最佳以前的结果相比提供30％的相对改进。</li>
<li>我们推测“supervised pre-training/domain-speciﬁc ﬁne-tuning”范例将对各种数据缺乏的视觉问题高度有效。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;mAP: mean average precision，平均准确度&lt;/li&gt;
&lt;li&gt;&lt;p&gt;我们的方法结合两个关键的见解：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一：采用高容量的卷积神经网络来从上到下的进行region proposal，从而实现定位和分割物体。&lt;/li&gt;
&lt;li&gt;当标记的训练数据稀缺时，可以先对辅助数据集（任务）进行受监督的预训练， 随后是基于域进行特定调整，产生显着的性能提升。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://yoursite.com/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>行人检测论文笔记：Fused DNN - A deep neural network fusion approach to fast and robust pedestrian detection</title>
    <link href="http://yoursite.com/posts/2553947436/"/>
    <id>http://yoursite.com/posts/2553947436/</id>
    <published>2016-12-04T22:32:24.000Z</published>
    <updated>2017-01-28T09:00:31.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="相关知识点"><a href="#相关知识点" class="headerlink" title="相关知识点"></a>相关知识点</h2><ul>
<li><strong>L1范数</strong> 也称为最小绝对偏差（LAD），最小绝对误差（LAE）。它基本上最小化目标值(Yi)和估计值(f(xi))之间的绝对差(S)的和</li>
</ul>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfqk8unvtj306d0273ye.jpg" alt=""></p>
<ul>
<li>L2范数也称为最小二乘。它基本上最小化目标值(Yi)和估计值(f(xi))之间的差(S)的平方的和</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfqk9chp0j305w01wjra.jpg" alt=""></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>所提出的网络融合架构允许多个网络的并行处理来提高速度。</li>
<li>首先是一个深度卷积网络被训练为一个物体检测器来生成所有有可能的不同尺寸和遮挡的行人候选集。</li>
<li>然后，多个深度神经网络被并行使用来之后提炼这些行人候选集。</li>
<li>我们引入基于软拒绝的网络融合方法将来自所有网络的软度量融合在一起，以产生最终置信分数。</li>
<li>此外，我们提出了一种用于将逐像素语义分割网络（ pixel-wise semantic segmentation network）集成到网络融合架构中作为行人检测器的加强的方法。</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>Tradeoff between accuracy and speed.</li>
<li>其他因素，如拥挤的场景，非人堵塞物体(non-person occluding objects)或不同的行人外观（不同的姿势或服装风格）也使这个Real-time行人检测问题具有挑战性。</li>
<li>行人检测的一般框架可以分解为：</li>
<li>region proposal generation,</li>
<li>feature extraction,</li>
<li><p>pedestrian verification</p>
</li>
<li><p>Fused Deep Neural Network(F-DNN)</p>
</li>
<li>该架构包括行人pedestrian candidiate generator，其通过训练深卷积神经网络获得以，从而具有高检测率，虽然有大的假阳性率。</li>
<li>使用深度扩展卷积和上下文聚合的并行语义分割网络[30]为候选行人提供了另一个软的信任投票，它进一步与候选生成器和分类网络融合。</li>
</ul>
<h2 id="The-Fused-Deep-Neural-Network"><a href="#The-Fused-Deep-Neural-Network" class="headerlink" title="The Fused Deep Neural Network"></a>The Fused Deep Neural Network</h2><ul>
<li>提出的网络架构包括行人候选生成器，分类网络和像素级语义分割网络。</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfqk6f5o3j30q00iumyv.jpg" alt=""></p>
<ul>
<li><p>SSD: a single shot multi-box detector(单镜头多箱检测器)，行人候选生成器是一个single shot multi-box detector（SSD）</p>
</li>
<li><p>每个行人候选者与其定位BB坐标和置信度得分相关联。</p>
</li>
<li>我们提出了一种新的网络融合方法——称为基于软拒绝的网络融合（SNF）。并非是执行接受或拒绝候选者的硬二进制分类，而是基于来自分类器的候选者的 <strong>聚合度</strong> 来提升或折扣行人候选者的置信度分数。</li>
<li>我们进一步提出了一种利用具有语义分割（SS）的上下文聚集扩展卷积网络（context aggregation dilated convolutional network with semantic segmentation）作为另一个分类器并将其集成到我们的网络融合架构中的方法。但是在速度上会变得特别慢。</li>
</ul>
<h3 id="Pedestrian-Candidate-Generator"><a href="#Pedestrian-Candidate-Generator" class="headerlink" title="Pedestrian Candidate Generator"></a>Pedestrian Candidate Generator</h3><ul>
<li>SSD是具有截断VGG16(truncated VGG16)作为基础网络的前馈卷积网络。</li>
<li>SSD的结构：</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfqk7e7e6j30pe07rgmh.jpg" alt=""></p>
<ul>
<li><p>L2归一化技术用于缩小特征量</p>
</li>
<li><p>对于大小为m×n×p的每个输出层，在每个位置处设置不同尺度和纵横比的一组默认BB。 将3×3×p个卷积内核应用于每个位置以产生关于默认BB位置的分类分数和BB位置偏移。</p>
</li>
<li>训练的目标函数是：</li>
</ul>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfqka1k2nj306u01nq2u.jpg" alt=""></p>
<h3 id="Classiﬁcation-Network-and-Soft-rejection-based-DNN-Fusion"><a href="#Classiﬁcation-Network-and-Soft-rejection-based-DNN-Fusion" class="headerlink" title="Classiﬁcation Network and Soft-rejection based DNN Fusion"></a>Classiﬁcation Network and Soft-rejection based DNN Fusion</h3><ul>
<li>分类网络由多个二元分类深层神经网络组成，这些网络在第一阶段的生成的行人候选集中训练。</li>
<li>SNF：考虑一个行人候选人和一个分类器。如果分类器对候选人有高的信任度，我们通过乘以大于1的置信因子乘以候选发生器来提高其原始分数。否则，我们以小于1的缩放因子减小其得分。我们将“置信”定义为至少为ac的分类概率。为了融合所有M个分类器，我们将候选者的原始信任得分与分类网络中所有分类器的信任缩放因子的乘积相乘。</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfqkaszbpj30q202qt9i.jpg" alt=""></p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfqkaaiiij3086024t8n.jpg" alt=""></p>
<ul>
<li>SNF背后的关键思想是，我们不直接接受或拒绝任何候选行人，而是基于分类概率的因素来扩展它们。</li>
</ul>
<h3 id="Pixel-wise-semantic-segmentation-for-object-detection-reinforcement"><a href="#Pixel-wise-semantic-segmentation-for-object-detection-reinforcement" class="headerlink" title="Pixel-wise semantic segmentation for object detection reinforcement"></a>Pixel-wise semantic segmentation for object detection reinforcement</h3><ul>
<li>为了执行密集预测，SS网络由完全卷积的VGG16网络组成，其适应于作为前端预测模块的扩展卷积，其输出被馈送到多尺度上下文聚合模块，该多尺度上下文聚合模块由完全卷积网络组成，其卷积层具有增加扩张因子。</li>
<li>输入图像被缩放并由SS网络直接处理，SS网络产生具有显示出行人类激活像素的一种颜色和显示出背景的其他颜色的二进遮罩。</li>
<li>我们使用以下策略来融合结果：如果行人像素占据候选BB区域的至少20％，我们接受候选者并保持其得分不变; 否则，我们应用SNF来缩放原始的信任分数。</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfqkbeg8wj30g002cwen.jpg" alt=""></p>
<h2 id="Experiments-and-result-analysis"><a href="#Experiments-and-result-analysis" class="headerlink" title="Experiments and result analysis"></a>Experiments and result analysis</h2><h3 id="Data-and-evaluation-settings"><a href="#Data-and-evaluation-settings" class="headerlink" title="Data and evaluation settings"></a>Data and evaluation settings</h3><h3 id="Training-details-and-results"><a href="#Training-details-and-results" class="headerlink" title="Training details and results"></a>Training details and results</h3><ul>
<li><strong>硬拒绝（Hard Rejection）</strong> 被定义为消除由任何分类器分类为假阳性的任何候选者。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;相关知识点&quot;&gt;&lt;a href=&quot;#相关知识点&quot; class=&quot;headerlink&quot; title=&quot;相关知识点&quot;&gt;&lt;/a&gt;相关知识点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;L1范数&lt;/strong&gt; 也称为最小绝对偏差（LAD），最小绝对误差（LAE）。它基本上最小化目标值(Yi)和估计值(f(xi))之间的绝对差(S)的和&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://ww2.sinaimg.cn/large/006tKfTcgw1fbfqk8unvtj306d0273ye.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L2范数也称为最小二乘。它基本上最小化目标值(Yi)和估计值(f(xi))之间的差(S)的平方的和&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://ww4.sinaimg.cn/large/006tKfTcgw1fbfqk9chp0j305w01wjra.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;所提出的网络融合架构允许多个网络的并行处理来提高速度。&lt;/li&gt;
&lt;li&gt;首先是一个深度卷积网络被训练为一个物体检测器来生成所有有可能的不同尺寸和遮挡的行人候选集。&lt;/li&gt;
&lt;li&gt;然后，多个深度神经网络被并行使用来之后提炼这些行人候选集。&lt;/li&gt;
&lt;li&gt;我们引入基于软拒绝的网络融合方法将来自所有网络的软度量融合在一起，以产生最终置信分数。&lt;/li&gt;
&lt;li&gt;此外，我们提出了一种用于将逐像素语义分割网络（ pixel-wise semantic segmentation network）集成到网络融合架构中作为行人检测器的加强的方法。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://yoursite.com/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>行人检测论文笔记：How Far are We from Solving Pedestrian Detection?</title>
    <link href="http://yoursite.com/posts/2397281138/"/>
    <id>http://yoursite.com/posts/2397281138/</id>
    <published>2016-12-03T22:32:24.000Z</published>
    <updated>2017-01-28T09:00:05.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="文章疑问点"><a href="#文章疑问点" class="headerlink" title="文章疑问点"></a>文章疑问点</h2><ul>
<li>Human Baseline 的标准是如何确定的?</li>
<li><p>Ground-truth是什么意思？</p>
<ul>
<li>Groun-truth 指的是正确的标注（真实值）</li>
<li>在有监督学习中，数据是有标注的，以(x, t)的形式出现，其中x是输入数据，t是标注.正确的t标注是ground truth，错误的标记则不是。（也有人将所有标注数据都叫做ground truth）。</li>
</ul>
</li>
<li><p>Intersection over Union（IoU）是什么？</p>
<ul>
<li><p>Intersection over Union is an evaluation metric used to measure the accuracy of an object detector on a particular dataset.</p>
</li>
<li><p>Any algorithm that provides predicted bounding boxes as output can be evaluated using IoU.</p>
</li>
<li><p>As long as we have these two sets of bounding boxes we can apply Intersection over Union.</p>
</li>
<li><p>An Intersection over Union score &gt; 0.5 is normally considered a “good” prediction.</p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfsw1e3pcj308c06i0ss.jpg" alt=""></p>
</li>
</ul>
</li>
</ul>
<ul>
<li>FPPI: False Positive Per Image</li>
<li>Oracle Experiment: An oracle experiment is used to compare your actual system to how your system would behave if some component of it always did the right thing.</li>
</ul>
<a id="more"></a>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>调查了当前最先进的方法与“完美单帧检测器”之间的差距。</li>
<li>基于Caltech数据集创建了一个人工的基准。</li>
<li>手工聚合了顶级检测器经常出现的错误。</li>
<li><p>刻画了定位，前景 vs 背景两方面的错误</p>
<ul>
<li>针对定位错误：研究了训练集标记噪声对检测器性能的影响</li>
<li>前景 vs 背景错误：研究了convnets，讨论了哪些因素影响其性能</li>
</ul>
</li>
<li><p>提供了一个新的、更纯净的训练/测试标注集。</p>
</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><h3 id="Caltech-USA-pedestrian-detection-benchmark"><a href="#Caltech-USA-pedestrian-detection-benchmark" class="headerlink" title="Caltech-USA pedestrian detection benchmark"></a>Caltech-USA pedestrian detection benchmark</h3><ul>
<li><p>最流行的数据集：Caltech-USA、KITTI</p>
<ul>
<li>Caltech-USA有2.5小时、30Hz的从LA街道的一个check里面录制的</li>
<li>一共350000个标注、覆盖2300各单一的行人</li>
<li>测试集：4024帧</li>
</ul>
</li>
<li><p>MR: miss rate</p>
</li>
</ul>
<h3 id="Filtered-channel-features-detector"><a href="#Filtered-channel-features-detector" class="headerlink" title="Filtered channel features detector"></a>Filtered channel features detector</h3><ul>
<li>截止到最近的主要会议（CVPR 15），最好的方法是 <strong>Checkerboards</strong></li>
<li>Checkerboards：是ICF的一种，ICF(Integral Channels Feature detector)</li>
<li>目前最好的执行convnets方法对底层检测建议很敏感，因此我们首先通过优化过滤的通道特征检测器来关注这些建议。</li>
<li>环境和光流可以提高检测（额外的提示）</li>
</ul>
<h2 id="Analyzing-the-state-of-the-art"><a href="#Analyzing-the-state-of-the-art" class="headerlink" title="Analyzing the state of the art"></a>Analyzing the state of the art</h2><h3 id="Are-we-reaching-saturation"><a href="#Are-we-reaching-saturation" class="headerlink" title="Are we reaching saturation?"></a>Are we reaching saturation?</h3><ul>
<li>在现在的基准上，我们还有多少提升空间？为了回答这个问题，我们提出可一个人工的基准线作为最低极限。</li>
<li>机器检测算法应该达到至少人类水平，最终超过人类水平。</li>
<li>人工基准线——为了公平比较，关注于单帧单目检测，注释器需要根据行人外表和单帧环境来注释。</li>
<li>Intersection over Union (IoU) ≥ 0.5 matching criterion。</li>
<li>在所有情况下人类基准线表现远远超过当前最好的检测器，说明对于自动方法来说，还有提升空间。</li>
</ul>
<h3 id="Failure-analysis"><a href="#Failure-analysis" class="headerlink" title="Failure analysis"></a>Failure analysis</h3><h4 id="Error-sources"><a href="#Error-sources" class="headerlink" title="Error sources"></a>Error sources</h4><ul>
<li><p>一个检测器可以有两类错误：</p>
<ul>
<li>假阳性（检测到了背景，或者很弱的定位检测）</li>
<li>假阴性（低得分率或者错过某些行人检测，检测不全）</li>
</ul>
</li>
<li><p>FP聚类成11个分类</p>
</li>
<li>FN聚类成6个分类，其中side view 和 cyclists是由于数据集偏差导致的，用这些案例的外部图像增强训练集可能是一个有效的策略。</li>
<li>对于small pedestrains，发现低像素是主要困难来源，所以合理的利用所有像素，以及周围上下文是很必要的。</li>
</ul>
<h4 id="Oracle-test-cases"><a href="#Oracle-test-cases" class="headerlink" title="Oracle test cases"></a>Oracle test cases</h4><ul>
<li>对于大多数执行最好的方法，localization和background-vs-forground误差对检测质量具有相等的影响。 他们同样重要。</li>
</ul>
<h4 id="Improved-Caltech-USA-annotations"><a href="#Improved-Caltech-USA-annotations" class="headerlink" title="Improved Caltech-USA annotations"></a>Improved Caltech-USA annotations</h4><ul>
<li>原始注释是基于跨越多个帧内插稀疏注释（interpolating sparse annotations ），并且这些稀疏注释不一定位于评估的帧上。</li>
<li><p>我们的目标是两方面：</p>
<ul>
<li>在一方面，我们希望提供对现有技术的更准确的评估，特别是适合于接近该问题的“最后20％”的评估。</li>
<li>另一方面，我们希望有训练注释，并评估改进的注释导怎么样更好的检测。</li>
</ul>
</li>
<li><p>总之，我们的新注释与人类基线在以下方面不同：训练和测试集都被注释，忽略区域和闭塞也被注释，完整的视频数据用于决策，并且允许同一图像的多个修订。</p>
</li>
</ul>
<h3 id="Improving-the-state-of-the-art"><a href="#Improving-the-state-of-the-art" class="headerlink" title="Improving the state of the art"></a>Improving the state of the art</h3><h4 id="Impact-of-training-annotations"><a href="#Impact-of-training-annotations" class="headerlink" title="Impact of training annotations"></a>Impact of training annotations</h4><ul>
<li><p>Pruning benefits:</p>
<ul>
<li>从原始到修剪注释的主要变化是删除注释错误，从修剪到新的，主要的变化是更好的对齐。</li>
<li>我们在MRN-2中看到，更强的检测器更好地受益于更好的数据，并且检测质量的最大增益来自移除注释错误。</li>
</ul>
</li>
<li><p>Alignment benefits:</p>
<ul>
<li>为了利用新的1×注释来利用9×剩余数据，我们在新的注释上训练模型，并使用该模型在9×部分上重新对准原始注释。<br> <img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfsntx4jxj30hn06zwg1.jpg" alt="Snip20161204_2"></li>
<li><p>因为新的注释更好地对齐，所以我们期望该模型能够修复原始注释中的轻微位置和缩放错误。</p>
</li>
<li><p>结果表明，使用检测器模型来提高整体数据对准确实是有效的，并且更好地对准训练数据导致更好的检测质量（在MRO和MRN中）。</p>
</li>
<li><p>使用高质量注释进行训练可提高整体检测质量，这得益于改进的对齐和减少的注释错误。</p>
</li>
</ul>
</li>
</ul>
<h4 id="Convnets-for-pedestrian-detection"><a href="#Convnets-for-pedestrian-detection" class="headerlink" title="Convnets for pedestrian detection"></a>Convnets for pedestrian detection</h4><ul>
<li><p>AlexNet 和 VGG16都在ImageNet上进行了预先训练，并使用SquaresChnFtrs建议对Caltech 10×（原始注释）进行了微调。</p>
</li>
<li><p>可以看出，VGG显着地减少了背景误差，而同时稍微增加了定位误差。</p>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfsqv4dcdj30df0c7gnt.jpg" alt="Snip20161204_3"></p>
</li>
<li><p>虽然卷积在图像分类和一般物体检测中具有很强的结果，但是当在小物体周围产生良好的局部检测分数时，它们似乎有局限性。 边界框回归（和NMS）是当前架构的一个关键因素。</p>
</li>
<li><p>表明神经网络的原始分类能力仍有改进的余地。</p>
</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li><p>相对于human baseline, there is a 10× gap still to be closed.</p>
</li>
<li><p>误差特性导致关于如何设计更好的检测器（在3.2节中提及;例如，对于人side-view的数据增加或在垂直轴上延伸检测器接收场）的具体建议。</p>
</li>
<li><p>我们通过衡量更好的注释对本地化准确性的影响，以及通过调查使用convnets来改善the background to foreground discrimination，来部分解决了一些问题。我们的研究结果表明，通过适当训练的ICF检测器可以实现显着更好的Alignment，并且，对于行人检测，Convent在localization上能力不强，但是可以通过边界框回归（bounding box regression）部分解决。 对于原始和新注释，所描述的检测方法都能达到最高性能。</p>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfsrgcdtaj30dh077jue.jpg" alt="Snip20161204_4"></p>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;文章疑问点&quot;&gt;&lt;a href=&quot;#文章疑问点&quot; class=&quot;headerlink&quot; title=&quot;文章疑问点&quot;&gt;&lt;/a&gt;文章疑问点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Human Baseline 的标准是如何确定的?&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ground-truth是什么意思？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Groun-truth 指的是正确的标注（真实值）&lt;/li&gt;
&lt;li&gt;在有监督学习中，数据是有标注的，以(x, t)的形式出现，其中x是输入数据，t是标注.正确的t标注是ground truth，错误的标记则不是。（也有人将所有标注数据都叫做ground truth）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Intersection over Union（IoU）是什么？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Intersection over Union is an evaluation metric used to measure the accuracy of an object detector on a particular dataset.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Any algorithm that provides predicted bounding boxes as output can be evaluated using IoU.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As long as we have these two sets of bounding boxes we can apply Intersection over Union.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An Intersection over Union score &amp;gt; 0.5 is normally considered a “good” prediction.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ww3.sinaimg.cn/large/006tKfTcgw1fbfsw1e3pcj308c06i0ss.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;FPPI: False Positive Per Image&lt;/li&gt;
&lt;li&gt;Oracle Experiment: An oracle experiment is used to compare your actual system to how your system would behave if some component of it always did the right thing.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Pedestrian Detection" scheme="http://yoursite.com/tags/Pedestrian-Detection/"/>
    
      <category term="行人检测" scheme="http://yoursite.com/tags/%E8%A1%8C%E4%BA%BA%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Review" scheme="http://yoursite.com/tags/Review/"/>
    
      <category term="综述" scheme="http://yoursite.com/tags/%E7%BB%BC%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>行人检测论文笔记：Robust Real-Time Face Detection</title>
    <link href="http://yoursite.com/posts/2903903730/"/>
    <id>http://yoursite.com/posts/2903903730/</id>
    <published>2016-12-03T22:32:24.000Z</published>
    <updated>2017-01-28T07:37:22.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><ul>
<li>傅里叶变换的一个推论：<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrudkda8j308h01d3yg.jpg" alt=""></li>
</ul>
<p>一个时域下的复杂信号函数可以分解成多个简单信号函数的和，然后对各个子信号函数做傅里叶变换并再次求和，就求出了原信号的傅里叶变换。</p>
<ul>
<li><p>卷积定理(Convolution Theorem)：信号f和信号g的卷积的傅里叶变换，等于f、g各自的傅里叶变换的积<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfrue2zlyj304n01gdfp.jpg" alt=""></p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrufxyw0j306z0263yf.jpg" alt=""></p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfru90t5uj30jt09j75c.jpg" alt=""><br>整个过程的核心就是“（反转），移动，乘积，求和”</p>
</li>
</ul>
<a id="more"></a>
<ul>
<li><p>二维卷积</p>
<ul>
<li><p>数学定义<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfru5ml16j30f901kdfv.jpg" alt=""></p>
<p>二维卷积在图像处理中会经常遇到，图像处理中用到的大多是二维卷积的离散形式：<br><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfrum5q4ej30cv01o74a.jpg" alt=""></p>
</li>
<li>图像处理中的二维卷积，二维卷积就是一维卷积的扩展，原理差不多。核心还是（反转），移动，乘积，求和。这里二维的反转就是将卷积核沿反对角线翻转，比如：<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfru4uj25j307m02wjrd.jpg" alt=""><br>之后，卷积核在二维平面上平移，并且卷积核的每个元素与被卷积图像对应位置相乘，再求和。通过卷积核的不断移动，我们就有了一个新的图像， <strong>这个图像完全由卷积核在各个位置时的乘积求和的结果组成。</strong></li>
</ul>
</li>
<li><p>巴拿赫空间：更精确地说，巴拿赫空间是一个具有范数并对此范数完备的向量空间。</p>
</li>
<li><p>许多在数学分析中学到的无限维函数空间都是巴拿赫空间。</p>
</li>
<li><p>巴拿赫空间有两种常见的类型：“实巴拿赫空间”及“复巴拿赫空间”，分别是指将巴拿赫空间的向量空间定义于由实数或复数组成的域之上。</p>
</li>
<li><p>Overcomplete：</p>
<ul>
<li>对于Banach space X中的一个子集，如果X中的每一个元素都可以利用子集中的元素在范数内进行有限线性组合来良好近似，则该系统X是完备Complete的。</li>
<li>该完备系统是过完备（Overcomplete）的，如果从子集中移去一个元素，该系统依旧是完备的，则该系统称为过完备的。</li>
<li>在不同的研究中，比如信号处理和功能近似，过完备可以帮助研究人员达到一个更稳定、更健壮，或者相比于使用基向量更紧凑的分解。</li>
<li>如果 # (basis vector基向量)&gt;输入的维度，则我们有一个overcomplete representation.</li>
</ul>
</li>
<li><p>ROC曲线：在信号检测理论中，接收者操作特征曲线（receiver operating characteristic curve，或者叫ROC曲线）是一种坐标图式的分析工具，用于</p>
<ul>
<li>(1) 选择最佳的信号侦测模型、舍弃次佳的模型。</li>
<li>(2) 在同一模型中设定最佳阈值。</li>
<li>从 (0, 0) 到 (1,1) 的对角线将ROC空间划分为左上／右下两个区域，在这条线的 <strong>以上的点</strong> 代表了一个 <strong>好</strong> 的分类结果（胜过随机分类），而在这条线 <strong>以下的点</strong> 代表了 <strong>差</strong> 的分类结果（劣于随机分类）。</li>
<li>完美的预测是一个在左上角的点.</li>
<li>曲线下面积（AUC）：ROC曲线下方的面积，若随机抽取一个阳性样本和一个阴性样本，分类器正确判断阳性样本的值高于阴性样本之机率=AUC。简单说：AUC值越大的分类器，正确率越高。</li>
</ul>
</li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>介绍一个脸部检测框架。</li>
<li>三个贡献：</li>
<li>引入新图像表示——称为“积分图像”，其允许我们的检测器非常快速地计算所使用的特征。</li>
<li>提出一个利用AdaBost学习算法构建的简单有效的分类器，来从极大潜在特征集中选出很少的关键视觉特征。</li>
<li>在级联中组合分类器，从而快速丢弃图像的背景区域，同时在有可能的面部区域上花费更多的计算。</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><p>Haar Basis 函数：<br><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfru9jaxij308q05st8t.jpg" alt=""></p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrucbw2pj30fn07kaaj.jpg" alt=""></p>
</li>
<li><p>Integral image: 类似于计算机图形学中利用求和区域表来进行纹理映射。</p>
</li>
<li><p>Haar-like features：就是mount两个或多个区域的像素值之和的差值。</p>
</li>
<li>AdaBoost：自适应增强， 具体说来，整个Adaboost 迭代算法就3步：</li>
<li>初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。</li>
<li>训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。</li>
<li>将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。</li>
<li>级联检测过程的结构基本上是简并决策树的结构</li>
</ul>
<h2 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h2><ul>
<li>基于特征的系统操作肯定比一个基于像素的系统更更快</li>
<li>（Two-rectangle feature）两矩形特征的值是两个矩形区域内的像素之和的差</li>
<li>(Three-rectangle feature)三矩形特征计算从中心矩形中的和减去的两个外部矩形的和。</li>
<li>(Four-rectangle feature)四矩形特征计算矩形对角线对之间的差异。<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrul9r26j30aw09ydg6.jpg" alt=""></li>
</ul>
<p>矩阵特征=从灰色矩形中的像素的和中减去位于白色矩形内的像素的和。</p>
<h3 id="Integral-Image"><a href="#Integral-Image" class="headerlink" title="Integral Image"></a>Integral Image</h3><ul>
<li><p>矩阵特征可以通过图像的中间表示来快速计算，从而成为Integral Image.</p>
</li>
<li><p>积分图的每一点（x, y）的值是原图中对应位置的左上角区域的所有值得和。</p>
</li>
<li><p>积分图每一点的（x, y）值是：<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrujq2ygj30f601g749.jpg" alt=""></p>
</li>
<li><p>位置x，y处的积分图像包含x，y（包括端点）上方和左侧的像素的和:<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfruhirv4j309d021t8p.jpg" alt=""></p>
</li>
</ul>
<p>ii(x, y) is the integral image</p>
<p>i(x, y) is the original image<br><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfru76drdj30au02gwem.jpg" alt=""></p>
<p>s（x，y）是累积行和</p>
<p>s(x, −1) = 0, ii(−1, y) = 0)</p>
<p>积分图可以只遍历一次图像即可有效的计算出来</p>
<ul>
<li><p>使用积分图像，可以在四个阵列参考中计算任何矩形和。<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfru8nyo9j30c8096jrh.jpg" alt=""></p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfruav2urj30d602jwej.jpg" alt=""></p>
</li>
<li><p>Two-rectangle feature：需要6个阵列参考<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrughx0aj30ga0gr408.jpg" alt=""></p>
</li>
<li><p>Three-rectangle feature：需要8个阵列参考</p>
</li>
<li><p>Four-rectangle feature：需要9个阵列参考</p>
</li>
<li><p>在线性运算（例如f.g）的情况下，如果其逆被应用于结果，则任何可逆线性算子可以应用于f或g。</p>
</li>
<li><p>例如在卷积的情况下，如果导数运算符被应用于图像和卷积核，则结果必须被双重积分.<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfru56d7dj307g01ra9z.jpg" alt=""></p>
</li>
<li><p>如果f和g的导数稀疏（或可以这样做），卷积可以显着加速。</p>
</li>
<li><p>类似的一个认识是：如果其逆被应用于g，则一个可逆线性算子可以应用于f。<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfru68n1lj307x01tdfs.jpg" alt=""></p>
</li>
<li><p>在该框架中观察，矩形和的计算可以表示为点积i·r，其中i是图像，r是box car图像（在感兴趣的矩形内的值为1，外面是0）。 此操作可以重写：<br><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfrufk96vj306c02gwee.jpg" alt=""></p>
</li>
</ul>
<p>积分图像实际上是图像的二重积分（首先沿行，然后沿列）。</p>
<ul>
<li>矩形的二阶导数（第一行在行中，然后在列中）在矩形的角处产生四个delta函数。 第二点积的评估通过四个阵列访问来完成。</li>
</ul>
<h3 id="Feature-Discussion"><a href="#Feature-Discussion" class="headerlink" title="Feature Discussion"></a>Feature Discussion</h3><ul>
<li>与可操纵滤波器（Steerable filters）等替代方案相比，矩形特性有点原始。</li>
<li>可控滤波器对边界的详细分析，图像压缩和纹理分析的非常有用。</li>
<li>由于正交性不是这个特征集的中心，我们选择生成一个非常大而且各种各样的矩形特征集。</li>
<li>从经验上看，似乎矩形特征集提供了丰富的图像表示，能支持有效的学习。</li>
<li>为了利用积分图像技术的计算有事，考虑用更常规的方法去计算图像金字塔。</li>
<li>像大多数面部检测系统一样，我们的检测器在许多尺度扫描输入; 从以尺寸为24×24像素检测面部的基本刻度开始，在12个刻度以大于上一个的1.25倍的因子扫描384×288像素的图像。</li>
</ul>
<h2 id="Learning-Classification-Functions"><a href="#Learning-Classification-Functions" class="headerlink" title="Learning Classification Functions"></a>Learning Classification Functions</h2><ul>
<li>给定检测器的基本分辨率是24×24，矩形特征的穷尽集是相当大的，160000.</li>
<li>我们的假设是，由实验证明，非常少数的矩形特征可以组合形成一个有效的分类器。 <strong>主要的挑战是找到这些功能。</strong></li>
<li>Adaboost：将多个弱分类器组合成一个强分类器。（一个简单学习算法叫做weak learner）。</li>
<li>传统的的AdaBoost过程可以容易地解释为贪心特征选择过程。</li>
<li>一个 <strong>挑战</strong> 是将大的权重与每个良好的分类函数相关联，并将较小的权重与较差的函数相关联。</li>
<li>AdaBoost是一个用于搜索少数具有显着品种的良好“特征”的有效程序。</li>
<li>将一个weak learn限制到分类函数几何中，每一个函数都只依赖于一个单一的特征。</li>
<li>若学习宣发选择单一的能够最好分开正和负样本的矩形特征。</li>
<li>对于每一个特征，weak learner决定最优分类函数阈值，从而可以使得最少数目的样本被错分。</li>
<li>一个弱分类器h(x, f, p, θ)因此包含一个特征f，一个阈值θ，一个显示不等式方向的极性p：<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfruf310ij30ai01odfv.jpg" alt=""></li>
</ul>
<p>这里x是一个图片24*24像素的子窗口。</p>
<ul>
<li>我们使用的弱分类器（阈值单一特征）可以被视为单节点决策树。</li>
<li><strong>Boosting 算法</strong> ：T是利用每个单个特征构造的假设，最终假设是T个假设的加权线性组合，其中权重与训练误差成反比。</li>
</ul>
<ol>
<li>给定样本图片(x1, y1), (x2, y2), …, (xn, yn)。其中yi=0, 1分别为负样本和正样本。</li>
<li>初始化权值w1, i=1/(2m), 1/(2l)分别当yi=0, 1。其中m和l分别是负样本和正样本的数量。</li>
<li><p>For t=1, …, T:</p>
</li>
<li><p>归一化权重,<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrucsnrpj305e01bgli.jpg" alt=""></p>
</li>
<li><p>根据加权错误选择最佳弱分类器：<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfruidyr6j30cx01zgln.jpg" alt=""></p>
</li>
<li><p>定义 ht(x) = h(x, ft, pt,θt) 其中ft, pt, 和 θt 是εt的最小值.</p>
</li>
<li><p>更新权值：<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfruadstnj306101c0sm.jpg" alt=""><br>其中ei=0当样例xi被正确的分类，否则ei=1，并且<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfruklba8j303c017mx0.jpg" alt=""></p>
</li>
<li><p>最后的强分类器是：</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfru83b9xj30c104xjrk.jpg" alt=""><br>其中<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfru6hdsij303t0160sl.jpg" alt=""></p>
<h3 id="Learning-Discussion"><a href="#Learning-Discussion" class="headerlink" title="Learning Discussion"></a>Learning Discussion</h3></li>
</ol>
<ul>
<li><p>弱分类器选择算法过程如下：</p>
<ul>
<li>对于每个特征，根据特征值对样例进行排序。</li>
<li>该特征的AdaBoost最佳阈值可以在该排序列表上的单次通过中计算。</li>
<li>对于排序列表中的每个元素，四个和被维护和评估：</li>
<li>正实例权重T+的总和。</li>
<li>负实例权重T-的总和。</li>
<li>当前示例S+之下的正权重的和。</li>
<li>当前示例S-之下的负权重的和。</li>
</ul>
</li>
<li><p>在排序一个划分当前和上一示例之间的范围的阈值的错误是：<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrud40xsj30dk01gjrd.jpg" alt=""></p>
</li>
</ul>
<h3 id="Learning-Results"><a href="#Learning-Results" class="headerlink" title="Learning Results"></a>Learning Results</h3><ul>
<li>在现实应用中，假正例率必须接近1/1000000。</li>
<li>所选择的 <strong>第一特征</strong> 似乎集中于属性即眼睛的区域通常比鼻子和脸颊的区域更暗。</li>
<li>所选择的 <strong>第二特征</strong> 依赖于眼睛比鼻梁更暗的特性。</li>
<li>提高性能最直接技术是添加更多的特征，但这样直接导致计算时间的增加。</li>
<li>Receiver operating characteristic (ROC)曲线：<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfruhvzrpj30e30ao3yq.jpg" alt=""></li>
</ul>
<h2 id="The-Attentional-Cascade"><a href="#The-Attentional-Cascade" class="headerlink" title="The Attentional Cascade"></a>The Attentional Cascade</h2><ul>
<li>本节描述了用于构造级联的分类器的算法，其实现了提高的检测性能，同时从根本上减少了计算时间。</li>
<li>阈值越低，检测率越高，假正例率越高。</li>
<li>从双特征强分类器开始，可以通过 <strong>调整强分类器阈值</strong> 以最小化假阴性来获得有效的面部滤波器。</li>
<li>可以调整双特征分类器以50％的假阳性率来检测100％的面部。</li>
<li>整体的检测过程形式是简并决策树的形式，我们称之为“级联”。</li>
<li>在任何点上的否定结果立即导致对该子窗口的拒绝。</li>
<li>更深的分类器面临的更困难的例子,将整个ROC曲线向下推。 在给定的检测率下，较深的分类器具有相应较高的假阳性率。</li>
</ul>
<h3 id="Training-a-Cascade-of-Classifiers"><a href="#Training-a-Cascade-of-Classifiers" class="headerlink" title="Training a Cascade of Classifiers"></a>Training a Cascade of Classifiers</h3><ul>
<li><p>Given a trained cascade of classifiers, the false positive rate of the cascade is：<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfru7u7f1j303g02bjr9.jpg" alt=""></p>
</li>
<li><p>The detection rate is:</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrumlwqej303p02idfp.jpg" alt=""></p>
</li>
<li><p>The expected number of features which are evaluated is:</p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrubq3n2j307s02c0sq.jpg" alt=""></p>
</li>
<li><p>用于训练后续层的负样例集合是通过运行检测器收集通过在不包含任何面部实例的一组图像上而找到的所有错误检测来获得。</p>
</li>
<li><p>构建一个练级检测器的训练算法：</p>
</li>
</ul>
<h3 id="Simple-Experiment"><a href="#Simple-Experiment" class="headerlink" title="Simple Experiment"></a>Simple Experiment</h3><h3 id="Detector-Cascade-Discussion"><a href="#Detector-Cascade-Discussion" class="headerlink" title="Detector Cascade Discussion"></a>Detector Cascade Discussion</h3><ul>
<li>将检测器训练为分类器序列的隐藏好处是:最终检测器看到的有效数目的负样例数目可能非常大。</li>
<li>在实践中，由于我们的检测器的形式和它使用的特性是非常高效的，所以在每个尺度和位置评估我们的检测器的 <strong>摊销成本</strong> 比在整个图像中找到并分组边缘更快。</li>
</ul>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><h3 id="Training-Dataset"><a href="#Training-Dataset" class="headerlink" title="Training Dataset"></a>Training Dataset</h3><ul>
<li>事实上，包含在较大子窗口中的附加信息可以用于在检测级联中较早地拒绝non-face。</li>
</ul>
<h3 id="Structure-of-the-Detector-Cascade"><a href="#Structure-of-the-Detector-Cascade" class="headerlink" title="Structure of the Detector Cascade"></a>Structure of the Detector Cascade</h3><ul>
<li>最终的检测器是38层分级器，包括总共6060个特征。</li>
<li>级联中的第一个分类器是使用两个特征构造的，在检测100%的面部时可以拒绝50%的non-faces.</li>
<li>下一个分类器具有十个特征，并且在检测几乎100％的面部时拒绝80％的非面部。</li>
<li>接下来的两层是25个特征分类器，其后是三个50特征分类器，再之后是具有根据表2中的算法选择的各种不同数目的特征的分类器。</li>
<li>添加更多层，直到验证集上的假阳性率接近零，同时仍保持高的正确检测率。</li>
</ul>
<h3 id="Speed-of-the-Final-Detector"><a href="#Speed-of-the-Final-Detector" class="headerlink" title="Speed of the Final Detector"></a>Speed of the Final Detector</h3><ul>
<li>级联检测器的速度直接与每个被扫描的子窗口的特征数量相关。</li>
</ul>
<h3 id="Image-Processing"><a href="#Image-Processing" class="headerlink" title="Image Processing"></a>Image Processing</h3><ul>
<li>用于训练的所有示例子窗口被 <strong>方差归一</strong> 化以使不同照明条件的影响最小化。</li>
<li>可以使用 <strong>一对积分图像</strong> 来快速计算图像子窗口的方差。</li>
<li>在扫描期间，可以通过对特征值进行后乘，而不是对像素进行操作来实现图像归一化的效果。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;知识点&quot;&gt;&lt;a href=&quot;#知识点&quot; class=&quot;headerlink&quot; title=&quot;知识点&quot;&gt;&lt;/a&gt;知识点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;傅里叶变换的一个推论：&lt;br&gt;&lt;img src=&quot;https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrudkda8j308h01d3yg.jpg&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个时域下的复杂信号函数可以分解成多个简单信号函数的和，然后对各个子信号函数做傅里叶变换并再次求和，就求出了原信号的傅里叶变换。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;卷积定理(Convolution Theorem)：信号f和信号g的卷积的傅里叶变换，等于f、g各自的傅里叶变换的积&lt;br&gt;&lt;img src=&quot;https://ww3.sinaimg.cn/large/006tKfTcgw1fbfrue2zlyj304n01gdfp.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrufxyw0j306z0263yf.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ww1.sinaimg.cn/large/006tKfTcgw1fbfru90t5uj30jt09j75c.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;整个过程的核心就是“（反转），移动，乘积，求和”&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://yoursite.com/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>行人检测论文笔记：Taking a Deeper Look at Pedestrians</title>
    <link href="http://yoursite.com/posts/285415955/"/>
    <id>http://yoursite.com/posts/285415955/</id>
    <published>2016-12-03T22:32:24.000Z</published>
    <updated>2017-01-28T08:13:25.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><ul>
<li>第一篇使用convnet进行行人检测的文章：Pedestrian detection with unsupervised multi-stage feature learning.</li>
<li>DBN-Isol: A different line of work extends a deformable parts model (DPM) [15] with a stack of Restricted Boltzmann Ma- chines (RBMs) trained to reason about parts and occlu- sion (DBN-Isol)</li>
<li>DBN-Mut: extended to ac- count for person-to-person relations</li>
<li>JointDeep: jointly optimize all these aspects: optimizes features, parts deformations, occlusions, and person-to-person relations.</li>
<li>MultiSDP: 网络为每层提供在不同尺度计算的关于行人检测的上下文特征。</li>
<li>SDN: 使用附加的“可切换层”（RBM变体）来自动学习低级特征和高级部分（例如“头”，“腿”等）。</li>
<li>DBN-Isol和DBN-Mut利用DPM作为检测方法。</li>
<li>JointDeep, MultiSDP, and SDN利用HOG+CSS+linear SVM detector作为检测。</li>
<li><p>重要的是要强调ConvNet [37]学习从YUV输入像素预测，而所有其他方法使用额外的手工制作的特征。</p>
<ul>
<li>DBN-Isol and DBN-Mut use HOG features as input.</li>
<li>MultiSDP uses HOG+CSS features as input.</li>
<li>JointDeep and SDN uses YUV+Gradients as input (and HOG+CSS for the detection proposals).</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h3 id="Training-data"><a href="#Training-data" class="headerlink" title="Training data"></a>Training data</h3><ul>
<li>Caltech</li>
<li>Caltech validation set</li>
<li>Caltech10x: we increase the training data tenfold by sampling one out of three frames</li>
<li>KITTI</li>
<li>ImageNet, Places</li>
</ul>
<h3 id="From-decision-forests-to-neural-networks"><a href="#From-decision-forests-to-neural-networks" class="headerlink" title="From decision forests to neural networks"></a>From decision forests to neural networks</h3>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;h3 id=&quot;Related-work&quot;&gt;&lt;a href=&quot;#Related-work&quot; class=&quot;headerlink&quot; title=&quot;Related work&quot;&gt;&lt;/a&gt;Related work&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;第一篇使用convnet进行行人检测的文章：Pedestrian detection with unsupervised multi-stage feature learning.&lt;/li&gt;
&lt;li&gt;DBN-Isol: A different line of work extends a deformable parts model (DPM) [15] with a stack of Restricted Boltzmann Ma- chines (RBMs) trained to reason about parts and occlu- sion (DBN-Isol)&lt;/li&gt;
&lt;li&gt;DBN-Mut: extended to ac- count for person-to-person relations&lt;/li&gt;
&lt;li&gt;JointDeep: jointly optimize all these aspects: optimizes features, parts deformations, occlusions, and person-to-person relations.&lt;/li&gt;
&lt;li&gt;MultiSDP: 网络为每层提供在不同尺度计算的关于行人检测的上下文特征。&lt;/li&gt;
&lt;li&gt;SDN: 使用附加的“可切换层”（RBM变体）来自动学习低级特征和高级部分（例如“头”，“腿”等）。&lt;/li&gt;
&lt;li&gt;DBN-Isol和DBN-Mut利用DPM作为检测方法。&lt;/li&gt;
&lt;li&gt;JointDeep, MultiSDP, and SDN利用HOG+CSS+linear SVM detector作为检测。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;重要的是要强调ConvNet [37]学习从YUV输入像素预测，而所有其他方法使用额外的手工制作的特征。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DBN-Isol and DBN-Mut use HOG features as input.&lt;/li&gt;
&lt;li&gt;MultiSDP uses HOG+CSS features as input.&lt;/li&gt;
&lt;li&gt;JointDeep and SDN uses YUV+Gradients as input (and HOG+CSS for the detection proposals).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Pedestrian Detection" scheme="http://yoursite.com/tags/Pedestrian-Detection/"/>
    
      <category term="行人检测" scheme="http://yoursite.com/tags/%E8%A1%8C%E4%BA%BA%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>行人检测论文笔记：Ten Years of Pedestrian Detection, What Have We Learned?</title>
    <link href="http://yoursite.com/posts/287090227/"/>
    <id>http://yoursite.com/posts/287090227/</id>
    <published>2016-12-01T22:32:24.000Z</published>
    <updated>2017-01-28T08:21:18.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>这种新的决策林探测器在挑战性的Caltech-USA数据集上实现了当前最好的已知性能。</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>更重要的是，这是一个有着已建立的基准和评估指标的良好定义的问题。</li>
<li>用于对象检测的的主要范例有——”Viola＆Jones变体“，HOG + SVM模板，可变形部分检测器（DPM）和卷积神经网络（ConvNets）都已经被探索用于此任务。</li>
</ul>
<a id="more"></a>
<h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><ul>
<li>INRIA, ETH, TUD-Brussels, Daimler, Caltech-USA, and KITTI是使用最广的数据集。</li>
<li>INRIA：INRIA是最古老的，因此具有相对较少的图像。 然而，从不同设置（城市，海滩，山脉等）的行人的高质量注释，这是为什么它被普遍选择用来训练。</li>
<li>Daimler没有被所有的方法考虑，因为它缺乏颜色通道。</li>
<li>Daimler stereo，ETH和KITTI提供立体声信息。</li>
<li>所有数据集但INRIA都是从视频获取的，因此可以使用光流作为附加提示。</li>
<li>今天，Caltech-USA和KITTY是行人检测的主要基准。 两者都相对较大和具有挑战性。</li>
</ul>
<h2 id="Main-approaches-to-improve-pedestrian-detection"><a href="#Main-approaches-to-improve-pedestrian-detection" class="headerlink" title="Main approaches to improve pedestrian detection"></a>Main approaches to improve pedestrian detection</h2><ul>
<li><p>40+种行人检测的方法：</p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfron76nbj30gx0p8gv7.jpg" alt="Snip20161202_22"></p>
</li>
</ul>
<ul>
<li>我们不是讨论方法的个体特性，而是识别区分每种方法（表1的对号）的关键方面，并对其进行分组。 我们在下面的小节讨论这些方面。</li>
</ul>
<h3 id="Training-data"><a href="#Training-data" class="headerlink" title="Training data"></a>Training data</h3><h3 id="Solution-families"><a href="#Solution-families" class="headerlink" title="Solution families"></a>Solution families</h3><ul>
<li>总体上，我们注意到在40多种方法中，我们可以辨别三个家庭：</li>
</ul>
<ol>
<li>DPM变体（MultiResC [33]，MT-DPM [39]等）</li>
<li>深度网络（JointDeep [40]，ConvNet [13] ]等）</li>
<li><p>决策林（ChnFtrs，Roerei等）。</p>
</li>
<li><p>在表1中，我们将这些家族分别识别为DPM，DN和DF。</p>
</li>
</ol>
<h3 id="Better-classiﬁers"><a href="#Better-classiﬁers" class="headerlink" title="Better classiﬁers"></a>Better classiﬁers</h3><ul>
<li>特征和分类器之间的没有明确的界限</li>
</ul>
<h3 id="Additional-data"><a href="#Additional-data" class="headerlink" title="Additional data"></a>Additional data</h3><ul>
<li>一些方法探索在训练和测试时间利用额外的信息来改进检测。 他们考虑立体图像[45]，光流（使用以前的帧，例如MultiFtr + Motion [22]和ACF + SDt [42]），跟踪[46]或来自其他传感器 。</li>
<li>到目前为止，仅基于单个单目图像帧的方法已经能够跟上由附加信息引入的性能改进。</li>
</ul>
<h3 id="Exploiting-context"><a href="#Exploiting-context" class="headerlink" title="Exploiting context"></a>Exploiting context</h3><ul>
<li>上下文为行人检测提供了一致的改进，虽然改进的规模比额外的测试数据（§3.4）和深层架构（§3.8）要低。 大部分检测质量必须来自其他来源。</li>
</ul>
<h3 id="Deformable-parts"><a href="#Deformable-parts" class="headerlink" title="Deformable parts"></a>Deformable parts</h3><ul>
<li>对于行人检测，结果是有竞争性的，但不显着。</li>
<li>对于行人检测，除了遮挡处理的情况之外，仍然没有关于部件和部件的必要性的明确证据。</li>
</ul>
<h3 id="Multi-scale-models"><a href="#Multi-scale-models" class="headerlink" title="Multi-scale models"></a>Multi-scale models</h3><ul>
<li>最近已经注意到，不同分辨率的训练不同模型系统地将性能提高1〜2MR百分点</li>
<li>尽管不断改进，他们对最终质量的贡献是相当小的。</li>
</ul>
<h3 id="Deep-architectures"><a href="#Deep-architectures" class="headerlink" title="Deep architectures"></a>Deep architectures</h3><ul>
<li>尽管有共同的叙述，仍然没有明确的证据表明深层网络有利于行人检测的学习功能</li>
<li>最成功的方法使用这样的架构来模拟部件，遮挡和上下文的更高级别方面。 获得的结果与DPM和决策林方法相同，使得使用这样涉及的结构的 <strong>优点仍不清楚</strong> 。</li>
</ul>
<h3 id="Better-features"><a href="#Better-features" class="headerlink" title="Better features"></a>Better features</h3><ul>
<li>特征更多，具有更丰富和更高维的表示，分类任务变得更容易，从而改善结果。</li>
<li>越来越多样化的特性已经显示系统地提高性能。</li>
<li><p>尽管通过添加许多渠道的改进，顶级性能检测器仍然达到仅有10个通道：</p>
<ul>
<li>6个梯度方向，</li>
<li>1个梯度幅度</li>
<li><p>3个颜色通道</p>
</li>
<li><p>我们命名这些 <strong>HOG + LUV</strong> 。</p>
</li>
</ul>
</li>
<li><p>应当注意，还没有更好的用于行人检测的特征可以通过深度学习方法获得。</p>
</li>
<li><p>下一个科学的步骤将是开发一个更深刻的理解，什么使好的功能更哈珀，以及如何设计更好的特征。</p>
</li>
</ul>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><ul>
<li><p>基于我们在上一节中的分析，在对检测质量的影响方面，三个方面似乎是最有希望的：</p>
<ul>
<li>更好的特征（§3.9</li>
<li>附加数据（§3.4）</li>
<li>上下文信息（§3.5）</li>
</ul>
</li>
</ul>
<h3 id="Reviewing-the-eﬀect-of-features-特征的影响"><a href="#Reviewing-the-eﬀect-of-features-特征的影响" class="headerlink" title="Reviewing the eﬀect of features(特征的影响)"></a>Reviewing the eﬀect of features(特征的影响)</h3><ul>
<li>DCT: (discrete cosine transform)离散余弦变换</li>
<li>自VJ以来的许多进展可以通过使用基于定向梯度和颜色信息的更好的特征来解释。 对这些众所周知的特征（例如，基于DCT的投影）的简单调整仍然可以产生显着的改进。</li>
</ul>
<h3 id="Complementarity-of-approaches"><a href="#Complementarity-of-approaches" class="headerlink" title="Complementarity of approaches"></a>Complementarity of approaches</h3><ul>
<li>在重新审视4.1节中单帧特征的影响之后，我们现在考虑更好的特征（HOG + LUV + DCT），附加数据（通过光流）和上下文（通过人对人的交互）的互补。</li>
<li>我们的实验表明，即使从强检测器开始，添加额外的特征，流量和上下文信息在很大程度上是互补的（增加12％，而不是3 + 7 + 5％）。</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>虽然这些功能中的一些可能是由学习驱动的，但它们主要是通过尝试和错误手工制作的。</li>
<li>Better features + optical flow + context的结合可以在Caltech-USA上产生最好的检测性能。</li>
<li>The main challenge ahead seems to develop a deeper understanding of <strong>what makes good features good</strong>, so as to enable the <strong>design of even better ones</strong>.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;这种新的决策林探测器在挑战性的Caltech-USA数据集上实现了当前最好的已知性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;更重要的是，这是一个有着已建立的基准和评估指标的良好定义的问题。&lt;/li&gt;
&lt;li&gt;用于对象检测的的主要范例有——”Viola＆Jones变体“，HOG + SVM模板，可变形部分检测器（DPM）和卷积神经网络（ConvNets）都已经被探索用于此任务。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Pedestrian Detection" scheme="http://yoursite.com/tags/Pedestrian-Detection/"/>
    
      <category term="行人检测" scheme="http://yoursite.com/tags/%E8%A1%8C%E4%BA%BA%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Review" scheme="http://yoursite.com/tags/Review/"/>
    
      <category term="综述" scheme="http://yoursite.com/tags/%E7%BB%BC%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>《DeepLearning》读书笔记：DL - Chapter 9 - Conventional Networks</title>
    <link href="http://yoursite.com/posts/783616645/"/>
    <id>http://yoursite.com/posts/783616645/</id>
    <published>2016-11-30T22:32:24.000Z</published>
    <updated>2017-01-28T08:59:38.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Chapter-9-Convolutional-Networks（卷积神经网络）"><a href="#Chapter-9-Convolutional-Networks（卷积神经网络）" class="headerlink" title="Chapter 9 Convolutional Networks（卷积神经网络）"></a>Chapter 9 Convolutional Networks（卷积神经网络）</h2><ul>
<li>卷积网络仅仅是在其至少一个层中使用卷积代替一般矩阵乘法的神经网络。</li>
</ul>
<h3 id="The-Convolution-Operation"><a href="#The-Convolution-Operation" class="headerlink" title="The Convolution Operation"></a>The Convolution Operation</h3><ul>
<li>The convolution operation is typically denoted with an asterisk:</li>
</ul>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfpf9zrgzj307m025t8n.jpg" alt=""></p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfpdsn8cgj305r01l0sm.jpg" alt=""></p>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfpdhwyqcj30b5028q2y.jpg" alt=""></p>
<ul>
<li>在卷积网络术语中，卷积的第一个参数（在本例中为函数x）通常称为 <strong>输入</strong> ，第二个参数（在本例中为函数w）作为 <strong>内核</strong> 。 <strong>输出</strong> 有时称为 <strong>特征映射(feature map)</strong> 。</li>
<li>在机器学习应用中， <strong>输入</strong> 通常是多维数据数组，并且 <strong>内核</strong> 通常是由学习算法调整的多维参数数组。</li>
<li>我们将这些多维数组称为 <strong>张量（tensors）</strong> 。</li>
</ul>
<a id="more"></a>
<ul>
<li>这意味着在实践中，我们可以实现无限求和作为对有限数量的数组元素的求和。</li>
<li>二维卷积：<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfpdof15bj30ge01yglp.jpg" alt=""><br>two-dimensional kernel K 卷积是可交换的，这意味着我们可以等价地写，但这样会带来 <strong>kernel-ﬂipping</strong><br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfpdiudboj30gf01wglq.jpg" alt=""><br>后一种会更更容易用机器学习库来实现。</li>
<li>许多神经网络库实现了一个称为互相关（cross-correlation）的相关函数，它与卷积相同，但是没有翻转（flipping）内核：</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfpdwxyypj30fw01vdfy.jpg" alt=""></p>
<ul>
<li>卷积的一个例子：</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfpf8xjmyj30oq0lqn0c.jpg" alt=""></p>
<ul>
<li>离散卷积可以被看作是乘以矩阵的乘法。</li>
<li>Toeplitz矩阵：常对角矩阵（又称特普利茨矩阵）是指每条左上至右下的对角线均为常数的矩阵，不论是正方形或长方形的。</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfpdpayklj30c30au74r.jpg" alt=""></p>
<ul>
<li>在二维中，双块循环矩阵（doubly block circulant matrix）对应于卷积。</li>
<li>卷积通常对应于非常稀疏的矩阵。</li>
<li>任何与矩阵乘法一起作用并且不依赖于矩阵结构的特定属性的神经网络算法都应该与卷积一起工作，这样就不需要对神经网络的任何进一步的改变。</li>
</ul>
<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul>
<li><p>卷积利用三个重要的想法，可以帮助改进机器学习系统:</p>
<ul>
<li>稀疏的连接（sparse interactions）</li>
<li>参数共享（parameter sharing）</li>
<li>等值表示（equivariant representations）</li>
<li>此外卷积可以处理各种大小输入。</li>
</ul>
</li>
<li><p>Sparse Interactions：这是通过使内核小于输入来实现的。</p>
<ul>
<li>我们需要存储更少的参数，这既减少了模型的内存需求，又提高了其统计效率。</li>
<li>计算输出需要更少的操作。</li>
<li>在深卷积网络中，较深层中的单元可以与输入的较大部分间接交互，This allows the network to eﬃciently describe complicated interactions between many variables by constructing such interactions from simple building blocks that each describe only sparse interactions.<br><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfpduscrij30ee0eqdho.jpg" alt=""></li>
<li>Sparse connectivity, viewed from below<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfpdvqhssj30ea0em0ul.jpg" alt=""></li>
<li>Sparse connectivity, viewed from above<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfpdvqhssj30ea0em0ul.jpg" alt=""></li>
<li>在卷积网络的较深层中的单元的接收场大于在浅层中的单元的接收场。这意味着即使卷积网络中的直接连接非常稀疏，更深层中的单元也可以间接地连接到所有或大部分输入图像。</li>
</ul>
</li>
<li><p>Parameter sharing：参数共享是指对模型中的多个函数使用相同的参数。</p>
<ul>
<li>也可以叫 <strong>Tied Weights（捆绑权值），因为应用于一个输入的权重的值与在其他地方应用的权重的值有关。</strong></li>
<li>在卷积神经网络中，卷积核中的每一个元素都会在input中的每一个位置使用。</li>
<li>在存储器要求和统计效率方面，卷积比密集矩阵乘法显着更有效。</li>
</ul>
</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfpdpjpefj30ee0bzgmu.jpg" alt=""></p>
<ul>
<li><p>黑色箭头表示在卷积模型中3元素核的中心元素的使用。</p>
</li>
<li><p>Equivariant：说一个函数是等变的意味着如果输入改变，输出以相同的方式改变。</p>
<ul>
<li>一个函数f(x)与函数g <strong>等变</strong> 如果f(g(x)) = g(f(x)).</li>
<li>当处理时间序列数据时，这意味着卷积产生一种时间线，显示输入中不同特征的出现。</li>
<li>This is useful for when we know that some function of a small number of neighboring pixels is useful when applied to multiple input locations.</li>
<li>卷积不是自然地等同于一些其他变换，例如图像的尺度或旋转的变化。</li>
</ul>
</li>
<li><p>卷积是描述在整个输入上应用小的局部区域的相同线性变换的变换的非常有效的方式。</p>
</li>
</ul>
<h3 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h3><ul>
<li><p>卷积网络的一个典型层由三个阶段组成:</p>
<ul>
<li>在第一阶段，该层并行执行几个卷积以产生一组线性激活（linear activation）。</li>
<li>在第二阶段，每个线性激活通过非线性激活函数，例如整流线性激活函数。这一阶段成为 <strong>detector stage</strong>.</li>
<li>在第三阶段，我们使用池化函数（pooing function）来进一步修改层的输出。</li>
</ul>
</li>
</ul>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfpdnsj0rj30k00je76j.jpg" alt=""></p>
<ul>
<li><p>pooling function是用附近的汇总统计来替换特定位置的网络的输出。</p>
</li>
<li><p>在所有情况下，池化有助于使表示变得对于相对于 <strong>输入的小平移</strong> 几乎不变（invariant）。</p>
</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfpdmenf3j30vy0qlwk3.jpg" alt=""></p>
<ul>
<li><p><strong>如果我们更关心某些特征是否存在而不是完全在哪里，那么本地变换的不变性可以是非常有用的属性。</strong></p>
</li>
<li><p>池的使用可以被视为添加一个无限强的先验，层所学习的函数必须对小的变换是不变的。</p>
</li>
<li>如果我们在单独的参数化的卷积输出上进行赤化，则特征可以学习到对哪一种变换进行不变。</li>
<li>利用卷积和pooling的卷积网络架构：</li>
</ul>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfpdrfrclj30jn0orjya.jpg" alt=""></p>
<h3 id="Convolution-and-Pooling-as-an-Inﬁnitely-Strong-Prior"><a href="#Convolution-and-Pooling-as-an-Inﬁnitely-Strong-Prior" class="headerlink" title="Convolution and Pooling as an Inﬁnitely Strong Prior"></a>Convolution and Pooling as an Inﬁnitely Strong Prior</h3><ul>
<li>弱先验是具有高熵的先验分布，例如具有高方差的高斯分布。</li>
<li>强先验具有非常低的熵，例如具有低方差的高斯分布。这样的先验在确定参数在哪里结束方面起更积极的作用。</li>
<li>总的来说，我们可以认为卷积可以用来为一个层的参数引入一个无限强的先验概率分布。</li>
<li>同样，池的使用是保证每个单位对小的变换保持不变性的无限强的先验。</li>
<li><p>但是将卷积网视为具有无限强的先验的完全连接的网络可以给我们一些关于卷积网如何工作的见解。</p>
<ul>
<li>一个关键的见解是卷积和池化可能导致欠拟合。</li>
<li>从这个观点的另一个关键的见解是，我们应该只比较卷积模型与统计学习性能基准中的其他卷积模型。对于许多图像数据集，对于排列不变的模型存在单独的基准，并且必须通过学习发现拓扑的概念，以及具有由他们的设计者硬编码到其中的空间关系的知识的模型。</li>
</ul>
</li>
</ul>
<h3 id="Variants-of-the-Basic-Convolution-Function"><a href="#Variants-of-the-Basic-Convolution-Function" class="headerlink" title="Variants of the Basic Convolution Function"></a>Variants of the Basic Convolution Function</h3><ul>
<li><p>首先，当我们在神经网络的上下文中提到卷积时，我们通常实际上意味着一种由许多卷积应用并行组成的操作。</p>
<ul>
<li>这是因为单个内核的卷积只能提取一种特征，虽然在许多空间位置。 通常我们希望我们网络的每一层都能在许多位置提取多种特征。</li>
</ul>
</li>
<li><p>此外，输入通常不仅仅是是一个网格的真是数据，反而是矢量值的观测网格。</p>
</li>
<li><p>这些多通道操作可交换，仅当每个操作具有与输入通道相同数量的输出通道。</p>
</li>
<li>我们可能想跳过内核的一些位置，以减少计算成本，我们可以认为这是下采样全卷积函数的输出。</li>
<li>如果我们只想对输出中每个方向的每s个像素进行采样，那么我们可以定义下采样卷积函数c：<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfpdkzazkj30dk01jwei.jpg" alt=""><br>我们将s称为这个下采样卷积的步幅。 也可以为每个运动方向定义一个单独的步幅。</li>
</ul>
<ul>
<li><p>零填充：</p>
<ul>
<li>任何卷积网络实现的一个基本特征是具备隐含地对输入V进行零填充以便使其更宽的能力。</li>
<li>零填充输入允许我们独立地控制内核宽度和输出的大小。</li>
<li>没有零填充，我们被迫选择快速缩小网络的空间范围并且使用小内核 - 这两个方案，显着地限制了网络的表达力。</li>
</ul>
</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfpdu1l9yj30jk0f376p.jpg" alt=""></p>
<ul>
<li><p>三种zero-padding的情况：</p>
<ul>
<li>valid convolution</li>
<li>same convolution</li>
<li>full convolution</li>
</ul>
</li>
<li><p>Unshared convolution</p>
</li>
<li>Tiled convolution：在卷积层和本地连接层之间提供了折中。我们不是在每个空间位置学习一组独立的权重，而是学习一组内核，从而我们在空间移动时旋转内核。</li>
<li><p>这三个操作做够计算所有训练一个前向卷积网络需要的梯度，以及基于卷积的转置来训练具有重建函数的卷积网络。</p>
<ul>
<li>卷积</li>
<li>从输出到权值反向传播</li>
<li>从输出到输入反向传播</li>
</ul>
</li>
</ul>
<h3 id="Structured-Outputs"><a href="#Structured-Outputs" class="headerlink" title="Structured Outputs"></a>Structured Outputs</h3><ul>
<li>卷积网络可以用于输出高维度的结构化对象，而不仅仅是预测分类任务的类标签或回归任务的实际值。</li>
<li>Pixel labeling：像素标记</li>
<li>循环周期卷积网络（recurrent convolutional network）：</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfpfbmq7mj307o07d0t3.jpg" alt=""></p>
<h3 id="Data-Types"><a href="#Data-Types" class="headerlink" title="Data Types"></a>Data Types</h3><ul>
<li><p>与卷积网络一起使用的数据通常由几个通道组成，每个通道是在空间或时间的某个点观察不同的量。</p>
<ul>
<li>卷积网络的一个优点是它们还可以处理具有变化的空间范围的输入。</li>
<li>有时，网络的输出允许具有 <strong>可变大小以及输入</strong> ，例如如果我们想要为输入的每个像素分配类标签。 <strong>在这种情况下，不需要额外的设计工作了</strong> 。</li>
<li>在其他情况下，网络必须产生一些固定大小的输出，例如，如果我们要为整个图像分配单个类标签。 <strong>在这种情况下，我们必须进行一些额外的设计步骤</strong> ，例如插入一个池化层，其池区域的大小与输入的大小成比例，以 <strong>保持固定数量</strong> 的池化输出。</li>
</ul>
</li>
</ul>
<h3 id="Eﬃcient-Convolution-Algorithms"><a href="#Eﬃcient-Convolution-Algorithms" class="headerlink" title="Eﬃcient Convolution Algorithms"></a>Eﬃcient Convolution Algorithms</h3><ul>
<li>现代卷积网络应用通常涉及包含超过一百万个单元的网络。</li>
<li><strong>卷积等效于使用傅立叶变换将输入和内核两者转换到频域，执行两个信号的逐点乘法，并使用逆傅里叶变换转换回到时域。</strong></li>
<li>当内核是可分离的，原始的卷积是效率低下的。</li>
<li>设计更快的执行卷积或近似卷积的方法，而不损害模型的准确性是一个活跃的研究领域。</li>
</ul>
<h3 id="Random-or-Unsupervised-Features"><a href="#Random-or-Unsupervised-Features" class="headerlink" title="Random or Unsupervised Features"></a>Random or Unsupervised Features</h3><ul>
<li>通常，卷积网络训练中最昂贵的部分是学习特征。</li>
<li>降低卷积网络训练成本的一种方式是使用未以受监督方式训练的特征。</li>
<li><p>有三种不需要监督学习来获得卷积内核的策略：</p>
<ul>
<li>一个是简单地 <strong>随机初始化</strong> 它们。</li>
<li>另一个是用手设计它们，例如通过设置每个内核以在特定方向或尺度检测边缘。</li>
<li>最后，可以使用无监督标准来学习内核。</li>
</ul>
</li>
<li><p>随机滤波器在卷积网络中通常工作得很好</p>
</li>
<li>一个折中的方法是学习特征，但使用： <strong>每个梯度步骤不需要完全正向和反向传播的</strong> 方法。 与多层感知器一样，我们使用 <strong>贪婪层式预训练</strong> ，独立地训练第一层，然后从第一层提取一次所有特征，然后利用这些特征隔离的训练第二层，等等。</li>
<li>不是一次训练整个卷积层，我们可以训练一个小补丁的模型，如用k-means。 然后，我们可以使用来自这个patch-based的模型的参数来定义卷积层的内核。</li>
<li>今天，大多数卷积网络以纯粹监督的方式训练，在每次训练迭代中使用通过整个网络的完全正向和反向传播。</li>
</ul>
<h3 id="The-Neuroscientiﬁc-Basis-for-Convolutional-Networks"><a href="#The-Neuroscientiﬁc-Basis-for-Convolutional-Networks" class="headerlink" title="The Neuroscientiﬁc Basis for Convolutional Networks"></a>The Neuroscientiﬁc Basis for Convolutional Networks</h3><h3 id="Convolutional-Networks-and-the-History-of-Deep-Learning"><a href="#Convolutional-Networks-and-the-History-of-Deep-Learning" class="headerlink" title="Convolutional Networks and the History of Deep Learning"></a>Convolutional Networks and the History of Deep Learning</h3><ul>
<li>为了处理一维，顺序数据，我们接下来转向神经网络框架的另一个强大的专业化： <strong>循环神经网络（Recurrent neural networks）</strong> 。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Chapter-9-Convolutional-Networks（卷积神经网络）&quot;&gt;&lt;a href=&quot;#Chapter-9-Convolutional-Networks（卷积神经网络）&quot; class=&quot;headerlink&quot; title=&quot;Chapter 9 Convolutional Networks（卷积神经网络）&quot;&gt;&lt;/a&gt;Chapter 9 Convolutional Networks（卷积神经网络）&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;卷积网络仅仅是在其至少一个层中使用卷积代替一般矩阵乘法的神经网络。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;The-Convolution-Operation&quot;&gt;&lt;a href=&quot;#The-Convolution-Operation&quot; class=&quot;headerlink&quot; title=&quot;The Convolution Operation&quot;&gt;&lt;/a&gt;The Convolution Operation&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;The convolution operation is typically denoted with an asterisk:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://ww1.sinaimg.cn/large/006tKfTcgw1fbfpf9zrgzj307m025t8n.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ww3.sinaimg.cn/large/006tKfTcgw1fbfpdsn8cgj305r01l0sm.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ww2.sinaimg.cn/large/006tKfTcgw1fbfpdhwyqcj30b5028q2y.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在卷积网络术语中，卷积的第一个参数（在本例中为函数x）通常称为 &lt;strong&gt;输入&lt;/strong&gt; ，第二个参数（在本例中为函数w）作为 &lt;strong&gt;内核&lt;/strong&gt; 。 &lt;strong&gt;输出&lt;/strong&gt; 有时称为 &lt;strong&gt;特征映射(feature map)&lt;/strong&gt; 。&lt;/li&gt;
&lt;li&gt;在机器学习应用中， &lt;strong&gt;输入&lt;/strong&gt; 通常是多维数据数组，并且 &lt;strong&gt;内核&lt;/strong&gt; 通常是由学习算法调整的多维参数数组。&lt;/li&gt;
&lt;li&gt;我们将这些多维数组称为 &lt;strong&gt;张量（tensors）&lt;/strong&gt; 。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="http://yoursite.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>行人检测论文笔记：Histograms of Oriented Gradients for Human Detection</title>
    <link href="http://yoursite.com/posts/3084343215/"/>
    <id>http://yoursite.com/posts/3084343215/</id>
    <published>2016-11-28T22:32:24.000Z</published>
    <updated>2017-01-28T08:59:18.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="相关知识点"><a href="#相关知识点" class="headerlink" title="相关知识点"></a>相关知识点</h2><ul>
<li><p>从TP、FP、TN、FN到ROC曲线、miss rate</p>
<ul>
<li>TP：true positive，实际是正例，预测为正例</li>
<li>FP：false positive，实际为负例，预测为正例</li>
<li>TN：true negative，实际为负例，预测为负例</li>
<li>FN：false negative，实际为正例，预测为负例</li>
</ul>
</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrffyhsdj30eu03mjrv.jpg" alt=""></p>
<ul>
<li>fnr+tpr=1, fpr+tnr=1</li>
<li>miss rate = FNR = 1 - true positive<ul>
<li>对于一个确定的阈值t，FPR和TPR是确定的，得到一个(fpr,tpr)元组。</li>
<li>当t增加， # FP也减小， # TN增加，则fpr减小；</li>
<li>当t增加， # TP减小， # FN增加，则tpr减小。</li>
<li>也就是说，当阈值t从0变化到1，fpr和tpr也单调减小，从(1，1)减小到(0,0)</li>
<li>miss rate = 1 - true positive rate，那么对应的YoX图像，也就是miss rate - false positive rate图像，就应当是单调下降的曲线。</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>定向梯度直方图（HOG）描述符的网格显著优于现有的人体检测特征集。</li>
<li>在重叠描述符块中的 <strong>精细尺度梯度</strong> ， <strong>精细定向分箱</strong> ， <strong>相对粗略的空间分箱</strong> 和 <strong>高质量局部对比度标准化</strong> 对于良好的结果都是重要的。</li>
<li>新的数据集</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>第一需求：robust feature set.</li>
<li>我们研究了人类监测的特征集问题，发现 <strong>本地归一化的定向梯度直方图（HOG）</strong> 描述符提供优异的性能相对于其他现有特征集包括小波。</li>
<li>提出的描述符让人联想到 <strong>边缘方向直方图</strong> [4,5]， <strong>SIFT描述符</strong> [12]和 <strong>形状上下文</strong> [1]，但与它们的不同点是：HOG描述器是在一个网格密集的大小统一的细胞单元（dense grid of uniformly spaced cells）上计算，而且为了提高性能，还采用了重叠的局部对比度归一化（overlapping local contrast normalization）技术。</li>
</ul>
<h2 id="Previous-Work"><a href="#Previous-Work" class="headerlink" title="Previous Work"></a>Previous Work</h2><h2 id="Overview-of-the-Method"><a href="#Overview-of-the-Method" class="headerlink" title="Overview of the Method"></a>Overview of the Method</h2><ul>
<li>The method is based on evaluating well-normalized local histograms of image gradient orientations in a dense grid.</li>
<li>基本思想是，在一副图像中，局部目标的 <strong>表象</strong> 和 <strong>形状</strong> （appearance and shape）能够被梯度或边缘的方向密度分布很好地描述，即使没有对应的梯度或边缘位置的精确知识。</li>
<li>具体的实现方法是：首先将图像分成小的连通区域，我们把它叫细胞单元（cell）。然后采集细胞单元中各像素点的梯度的或边缘的方向直方图。最后把这些直方图组合起来就可以构成特征描述器。</li>
<li>为了提高性能，我们还可以把这些局部直方图在图像的更大的范围内（我们把它叫区间或block）进行对比度归一化（contrast-normalized），所采用的方法是：先计算各直方图在这个区间（block）中的密度，然后根据这个密度对区间中的各个细胞单元做归一化。 <strong>通过这个归一化后，能对光照变化和阴影获得更好的效果。</strong></li>
<li>整体的物体检测链：</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfrfi9crxj30ww05udiy.jpg" alt=""></p>
<ul>
<li><p>这些基于稀疏特征的表示的成功有点遮蔽了HOG作为密集图像描述符的能力和简单性。</p>
</li>
<li><p>HOG/SIFT表示方法有几个优点。</p>
<ul>
<li>由于HOG方法是在图像的局部细胞单元上操作，所以它对图像几何的（geometric）和光学的（photometric）形变都能保持很好的不变性，这两种形变只会出现在更大的空间领域上。</li>
<li>他捕捉了局部形状非常具有特征性的边和梯度特征。</li>
<li>在局部表示中对局部的几何和光度变换的不变性更容易控制。</li>
<li>如果它们远小于局部空间或方向仓尺寸，则平移或旋转几乎没有差别。</li>
</ul>
</li>
<li><p>作者通过实验发现，在粗的空域抽样（coarse spatial sampling）、精细的方向抽样（fine orientation sampling）以及较强的局部光学归一化（strong local photometric normalization）等条件下，只要行人大体上能够保持直立的姿势，就容许行人有一些细微的肢体动作，这些细微的动作可以被忽略而不影响检测效果。综上所述，HOG方法是特别适合于做图像中的行人检测的。</p>
</li>
</ul>
<h2 id="Data-Sets-and-Methodology"><a href="#Data-Sets-and-Methodology" class="headerlink" title="Data Sets and Methodology"></a>Data Sets and Methodology</h2><ul>
<li>hard examples</li>
<li>Detection Error Tradeoff (DET) curves on a log-log scale. miss rate(1-Recall / FN/(TP+FN)) verses FPPW. 值越低越好。</li>
<li>DET图和ROC图提供的信息一眼，但是前者允许小概率更容易的去分布。</li>
<li>FPPW：NUMBER_OF_FALSE_POSITIVE/NUMBER_OF_WINDOWS</li>
<li>我们的DET曲线通常相当浅，所以即使非常小的缺失率的改善也等同于在不变缺失率下的情况下FPPW中的大增益。</li>
</ul>
<h2 id="Overview-of-Results"><a href="#Overview-of-Results" class="headerlink" title="Overview of Results"></a>Overview of Results</h2><ul>
<li>Generalized Haar Wavelets.</li>
<li>PCA-SIFT.</li>
<li>Shape Contexts.</li>
</ul>
<h2 id="Implementation-and-Performance-Study"><a href="#Implementation-and-Performance-Study" class="headerlink" title="Implementation and Performance Study"></a>Implementation and Performance Study</h2><ul>
<li><p>默认检测器：</p>
<ul>
<li>RGB colour space with no gamma correction</li>
<li>[−1, 0, 1] gradient filter with no smoothing</li>
<li>linear gradient voting into 9 orientation bins in 0◦ –180◦</li>
<li>16×16 pixel blocks of four 8×8 pixel cells</li>
<li>Gaussian spatial win- dow with σ = 8 pixel</li>
<li>L2-Hys (Lowe-style clipped L2 norm) block normalization</li>
<li>block spacing stride of 8 pixels (hence 4-fold coverage of each cell)</li>
<li>64×128 detection window;</li>
<li>linear SVM classifier.</li>
</ul>
</li>
<li><p>主要的结论是，为了良好的性能，应该使用细尺度导数（基本上没有平滑），许多定向仓,中等大小，强归一化，重叠的描述符块。</p>
</li>
</ul>
<h3 id="Gamma-Colour-Normalization"><a href="#Gamma-Colour-Normalization" class="headerlink" title="Gamma/Colour Normalization"></a>Gamma/Colour Normalization</h3><h3 id="Gradient-Computation"><a href="#Gradient-Computation" class="headerlink" title="Gradient Computation"></a>Gradient Computation</h3><ul>
<li>最通常用的方法就是简单的应用一个一维的离散的梯度模版分别应用在水平和垂直方向上去。可以使用如下的卷积核进行卷积：</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrfj1a8sj3066017q2t.jpg" alt=""></p>
<h3 id="Spatial-Orientation-Binning-方向单元划分"><a href="#Spatial-Orientation-Binning-方向单元划分" class="headerlink" title="Spatial / Orientation Binning(方向单元划分)"></a>Spatial / Orientation Binning(方向单元划分)</h3><ul>
<li>每个块内的每个像素对 <strong>方向直方图</strong> 进行投票</li>
<li>每个像素基于以其为中心的梯度元素的方向计算边缘取向直方图通道的 加权投票，并且投票被累积到在称为 <strong>单元</strong> 的局部空间区域上的 <strong>方向仓</strong> 中。</li>
<li>每个块的形状可以是矩形或圆形的</li>
<li>方向直方图的方向取值可以是0-180度或者0-360度，这取决于梯度是否有符号。无符号梯度（0-180º），有符号梯度（0-360º）</li>
<li>为了减少混叠，投票在相邻仓中心之间以取向和位置双向内插。</li>
<li>至于投票的权重，可以是梯度的幅度本身或者是它的函数。投票是像素处的梯度幅度的函数，或者是幅度本身、其平方、其平方根或者表示像素的边缘的软出现/缺失的幅度的限幅形式。在定向编码对于良好的性能是至关重要的。</li>
<li>梯度幅度本身通常产生最好的结果。其它可选的方案是采用幅度的平方或开方，或者幅度的裁剪版本。</li>
<li>Dalal和Triggs发现在人的检测实验中，把方向分为 <strong>9个通道(bin)</strong> 效果最好</li>
</ul>
<h3 id="Normalization-and-Descriptor-Blocks"><a href="#Normalization-and-Descriptor-Blocks" class="headerlink" title="Normalization and Descriptor Blocks"></a>Normalization and Descriptor Blocks</h3><ul>
<li>由于照明的局部变化和前景-背景对比度，梯度强度在可以在很宽范围内变化。所以梯度强度必须要局部地归一化，这需要把方格(cells)集结成更大、在空间上连结的区()</li>
<li><strong>有效的局部对比度归一化</strong> 对于良好的性能是必不可少的。</li>
<li>我们评估了多种不同的 <strong>归一化schemes(normalization schemes)</strong> ，他们大多数都是基于将单元格（cells）分组成更大的空间块（spatial blocks）<em>  </em>并且对比地单独对每个块进行归一化。</li>
<li><strong>最终描述符</strong> 是来自检测窗口中的所有块的归一化单元响应的所有分量的 <strong>向量</strong> 。</li>
<li><p>R-HOG：R-HOG块和SIFT描述符有许多相似之处，但是他们用途十分不同。</p>
<ul>
<li><p>R-HOG跟SIFT描述器看起来很相似，但他们的不同之处是：</p>
<ul>
<li>R-HOG是在单一尺度下、密集的网格内、没有对方向排序的情况下被计算出来；</li>
<li>而SIFT描述器是在多尺度下、稀疏的图像关键点上、对方向排序的情况下被计算出来。</li>
<li>R-HOG是各区间被组合起来用于对空域信息进行编码，而SIFT的各描述器是单独使用的。</li>
</ul>
</li>
</ul>
</li>
<li><p>它们在密集网格中以单个尺度计算而没有主要取向对准，并且用作隐式地去编码相对于检测窗口的空间位置的较大代码矢量的一部分，而SIFT在稀疏集合的 <strong>尺度不变关键点处</strong> 被计算，旋转以对准它们的主导方向，并单独使用。</p>
</li>
<li>SIFT被优化用于稀疏宽基线匹配，R-HOG用于空间形式的密集鲁棒编码。</li>
<li><p>R-HOG区块一般来说是多个方格子组成的，由三个参数表示：</p>
<ul>
<li>每个区块(block)有多少方格(cell)、</li>
<li>每个方格(cell)有几个像素(pixel)、</li>
<li>每个方格(cell)直方图有多少频道(bin)。</li>
</ul>
</li>
<li><p>对于人体检测，3x3的单元块，6x6的像素单元块儿表现最好，同时直方图是9通道。</p>
</li>
</ul>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrfhlpopj30ck095gmg.jpg" alt=""></p>
<ul>
<li><p>当其太小（1×1单元块，即，单独取向上的归一化）时，有价值的空间信息被抑制。</p>
</li>
<li><p>在对直方图做处理之前，给每个区间加一个高斯空域窗口是非常必要的，因为这样可以降低边缘的周围像素点的权重。</p>
</li>
<li><p>C-HOG</p>
<ul>
<li>每个空间单元包含梯度加权取向单元的堆叠而不是单个取向无关的边缘计数。</li>
<li>对数极坐标网格最初是由允许附近结构的精细编码与较宽上下文的粗略编码相结合的思想，以及从灵长类动物的 <strong>视野</strong> 到 <strong>V1皮层</strong> 的变换是 <strong>对数的</strong></li>
<li>然而，具有非常少的径向箱的小描述符反而能给出最好的性能，因此在实践中 <strong>几乎没有不均匀性或上下文</strong> 。</li>
<li>我们评估了C-HOG几何的两个变体，一个具有 <strong>单个圆形中心细胞</strong> （类似于[14]的GLOH特征），以及中心细胞被分成 <strong>角形扇区的形状上下文</strong> 。</li>
</ul>
</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrfkf2vgj302604a0sn.jpg" alt=""></p>
<ul>
<li><p>C-HOG的4个参数：</p>
<ul>
<li><p>the numbers of angular(角度盒子的个数）；</p>
</li>
<li><p>the numbers of radias(半径盒子个数)</p>
</li>
<li>the radius of the central bin in pixels（中心仓的半径（以像素为单位））</li>
<li>the expansion factor for subsequent (半径的伸展因子）</li>
</ul>
</li>
<li><p>为了良好的性能，最佳的参数设置为：4个角度盒子、2个半径盒子、中心盒子半径为4个像素、伸展因子为2</p>
</li>
<li><p>4像素是中央bin的最佳半径，但3和5给出类似的结果。</p>
</li>
<li><p>C-HOG看起来很像基于形状上下文（英语：Shape context）的方法，但不同之处是：C-HOG的区间中包含的细胞单元有多个方向通道，而基于形状上下文的方法仅仅只用到了一个单一的边缘存在数。[4]</p>
</li>
<li><p>Block Normalization schemes：引入v表示一个还没有被归一化的向量，它包含了给定区间（block）的所有直方图信息。vk 表示 v 的 k 阶范数，这里的 k={1,2}。用 e 表示一个很小的常数。一共4种不同的块规范化schemes</p>
<ul>
<li><p>L2-morm,<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrfgn4coj305l00tq2t.jpg" alt=""></p>
</li>
<li><p>L2-Hys, 它可以通过先进行L2-norm，对结果进行截短（clipping），然后再重新归一化得到。</p>
</li>
<li><p>L1-norm,<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrfltceqj304m00ndfp.jpg" alt=""></p>
</li>
<li><p>L1-sqrt,L1-norm followed by square root<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfrfl4bwqj305d00r0sm.jpg" alt=""></p>
</li>
<li><p>作者发现：采用L2-Hys, L2-norm, 和 L1-sqrt方式所取得的效果是一样的，L1-norm稍微表现出一点点不可靠性。</p>
</li>
</ul>
</li>
<li><p>Centre-surround normalization.</p>
</li>
</ul>
<h3 id="Detector-Window-and-Context"><a href="#Detector-Window-and-Context" class="headerlink" title="Detector Window and Context"></a>Detector Window and Context</h3><h3 id="Classifier"><a href="#Classifier" class="headerlink" title="Classifier"></a>Classifier</h3><p>最后一步就是把提取的HOG特征输入到SVM分类器中，寻找一个最优超平面作为决策函数。作者采用的方法是：使用免费的SVMLight软件包加上HOG分类器来寻找测试图像中的行人。</p>
<h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><ul>
<li>在甲酸梯度前进行任何程度的平滑处理都会毁掉HOG的结果，因为许多可供的图像信息都是从细尺度的突出边界形成的。</li>
<li>详单，梯度应该在当前金字塔层的最细可供尺度上被计算，修改或者用于方向投票并且只有在那之后在模糊空间。</li>
<li>其次，强的局部对比正常化对于良好的结果至关重要，传统的中心环绕样式方案不是最好的选择。</li>
<li>更好的结果可以通过相对于不同的局部支持对每个元素（边缘，单元）进行几次标准化，并将结果作为独立信号来实现。</li>
</ul>
<h2 id="Summary-and-Conclusions"><a href="#Summary-and-Conclusions" class="headerlink" title="Summary and Conclusions"></a>Summary and Conclusions</h2><ul>
<li><p>我们研究了各种描述符参数的影响，并得出结论，在重叠描述符块中的</p>
<ul>
<li>精细尺度梯度，</li>
<li>精细定向binning，</li>
<li>相对粗糙的空间binning</li>
<li>高质量局部对比度归一化 对于良好的性能都是重要的。</li>
</ul>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;相关知识点&quot;&gt;&lt;a href=&quot;#相关知识点&quot; class=&quot;headerlink&quot; title=&quot;相关知识点&quot;&gt;&lt;/a&gt;相关知识点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;从TP、FP、TN、FN到ROC曲线、miss rate&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TP：true positive，实际是正例，预测为正例&lt;/li&gt;
&lt;li&gt;FP：false positive，实际为负例，预测为正例&lt;/li&gt;
&lt;li&gt;TN：true negative，实际为负例，预测为负例&lt;/li&gt;
&lt;li&gt;FN：false negative，实际为正例，预测为负例&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrffyhsdj30eu03mjrv.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;fnr+tpr=1, fpr+tnr=1&lt;/li&gt;
&lt;li&gt;miss rate = FNR = 1 - true positive&lt;ul&gt;
&lt;li&gt;对于一个确定的阈值t，FPR和TPR是确定的，得到一个(fpr,tpr)元组。&lt;/li&gt;
&lt;li&gt;当t增加， # FP也减小， # TN增加，则fpr减小；&lt;/li&gt;
&lt;li&gt;当t增加， # TP减小， # FN增加，则tpr减小。&lt;/li&gt;
&lt;li&gt;也就是说，当阈值t从0变化到1，fpr和tpr也单调减小，从(1，1)减小到(0,0)&lt;/li&gt;
&lt;li&gt;miss rate = 1 - true positive rate，那么对应的YoX图像，也就是miss rate - false positive rate图像，就应当是单调下降的曲线。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Pedestrian Detection" scheme="http://yoursite.com/tags/Pedestrian-Detection/"/>
    
      <category term="行人检测" scheme="http://yoursite.com/tags/%E8%A1%8C%E4%BA%BA%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Object Detection" scheme="http://yoursite.com/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>行人检测论文笔记：Fast Feature Pyramids for Object Detection?</title>
    <link href="http://yoursite.com/posts/2735857030/"/>
    <id>http://yoursite.com/posts/2735857030/</id>
    <published>2016-11-23T22:32:24.000Z</published>
    <updated>2017-01-28T08:59:04.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="相关知识点"><a href="#相关知识点" class="headerlink" title="相关知识点"></a>相关知识点</h2><ul>
<li><p>Overcomplete Representations:</p>
<ul>
<li>Overcomplete：Such a complete system is overcomplete if removal of a \phi <em>{j} from the system results in a system (i.e., {\phi </em>{i}}_((i\in J\backslash {j))}) that is still complete.</li>
<li>In different research, such as signal processing and function approximation, overcompleteness can help researchers to achieve a more stable, more robust, or more compact decomposition than using a basis.[2]</li>
</ul>
</li>
<li><p>Image pyramid：影响金字塔</p>
<ul>
<li>影像金字塔由原始影像按一定规则生成的由细到粗不同分辨率的影像集。</li>
<li>指在同一的空间参照下，根据用户需要以不同分辨率进行存储与显示，形成分辨率由粗到细、数据量由小到大的金字塔结构。</li>
<li>图像编码和渐进式图像传输</li>
<li>从图中可以看出, 从金字塔的底层开始每四个相邻的像素经过重采样生成一个新的像素, 依此重复进行, 直到金字塔的顶层。重采样的方法一般有以下三种: 双线性插值、最临近像元法、三次卷积法。</li>
<li>金字塔是一种能对栅格影像按逐级降低分辨率的拷贝方式存储的方法。通过选择一个与显示区域相似的分辨率，只需进行少量的查询和少量的计算，从而减少显示时间。</li>
</ul>
</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfr9u09c3j305r064dfv.jpg" alt=""></p>
<ul>
<li>Gradient Histograms:</li>
</ul>
<a id="more"></a>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>The computational bottleneck of many modern detectors is the <strong>computation of features</strong> at every scale of a finely-sampled image pyramid.</li>
<li>The approach is general and is widely applicable to vision algorithms requiring fine-grained multi-scale analysis.</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>Multi-orientation decompostion: 多向分解，是图像处理中的基本技术。</li>
<li><p>在每个尺度和方向分别分析图像结构的想法起源于多个来源:</p>
<ul>
<li>哺乳动物视觉系统生理学的测量</li>
<li>有关视觉信息的统计和编码的原则性推理</li>
<li>谐波分析</li>
<li>谐波分析（多速率滤波)</li>
</ul>
</li>
<li><p>灵长类视觉系统显示：条纹皮层细胞（粗略的等价于图像的小波展开）在数量上超过视网膜神经节细胞（图像像素的近似表示）10^2到10^3.</p>
</li>
<li>这些表示相对于视点，照明和图像变形的变化的鲁棒性是Overcomplete Representations 优越性能的促成因素。</li>
<li>不幸的是，更高的检测正确率通常伴随着更高的计算开销。</li>
<li>在计算开销和为了提高检测和降低错误率而使用更复杂的表示之间是没有必要做权衡的。</li>
<li>自然图像具有分形统计，使得我们可以利用这一点来更可靠的预测图像跨尺度结构。</li>
<li><p>我们证明了我们提出的快速特征金字塔与三个不同的检测框架的有效性：</p>
<ul>
<li>积分通道特征（ICF）</li>
<li>聚集通道特征（积分通道特征的新颖变体）</li>
<li>可变形零件模型（DPM）</li>
</ul>
</li>
</ul>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul>
<li>Scale Space Theory: 尺度空间理论</li>
<li>Cascades, coarse-to-fine search, distance transforms, etc., 全部都关注于对提前计算好的图像特征来优化分类速度。</li>
<li>本方法专注于快速特征金字塔的构建，因此与上面的方法可以起到互补的效果。</li>
<li>行人检测的最佳执行方法[31]和PASCAL VOC [38]是基于多尺度特征金字塔的滑动窗[21]，[29]，[35]; 快速特征金字塔非常适合于这种滑动窗口检测器。</li>
</ul>
<h2 id="Multiscale-Gradient-Histograms"><a href="#Multiscale-Gradient-Histograms" class="headerlink" title="Multiscale Gradient Histograms"></a>Multiscale Gradient Histograms</h2>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;相关知识点&quot;&gt;&lt;a href=&quot;#相关知识点&quot; class=&quot;headerlink&quot; title=&quot;相关知识点&quot;&gt;&lt;/a&gt;相关知识点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Overcomplete Representations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Overcomplete：Such a complete system is overcomplete if removal of a \phi &lt;em&gt;{j} from the system results in a system (i.e., {\phi &lt;/em&gt;{i}}_((i\in J\backslash {j))}) that is still complete.&lt;/li&gt;
&lt;li&gt;In different research, such as signal processing and function approximation, overcompleteness can help researchers to achieve a more stable, more robust, or more compact decomposition than using a basis.[2]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Image pyramid：影响金字塔&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;影像金字塔由原始影像按一定规则生成的由细到粗不同分辨率的影像集。&lt;/li&gt;
&lt;li&gt;指在同一的空间参照下，根据用户需要以不同分辨率进行存储与显示，形成分辨率由粗到细、数据量由小到大的金字塔结构。&lt;/li&gt;
&lt;li&gt;图像编码和渐进式图像传输&lt;/li&gt;
&lt;li&gt;从图中可以看出, 从金字塔的底层开始每四个相邻的像素经过重采样生成一个新的像素, 依此重复进行, 直到金字塔的顶层。重采样的方法一般有以下三种: 双线性插值、最临近像元法、三次卷积法。&lt;/li&gt;
&lt;li&gt;金字塔是一种能对栅格影像按逐级降低分辨率的拷贝方式存储的方法。通过选择一个与显示区域相似的分辨率，只需进行少量的查询和少量的计算，从而减少显示时间。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://ww3.sinaimg.cn/large/006tKfTcgw1fbfr9u09c3j305r064dfv.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gradient Histograms:&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Pedestrian Detection" scheme="http://yoursite.com/tags/Pedestrian-Detection/"/>
    
      <category term="行人检测" scheme="http://yoursite.com/tags/%E8%A1%8C%E4%BA%BA%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Object Detection" scheme="http://yoursite.com/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>行人检测论文笔记：Pedestrian Detection - An Evaluation of the State of the Art</title>
    <link href="http://yoursite.com/posts/3091185356/"/>
    <id>http://yoursite.com/posts/3091185356/</id>
    <published>2016-11-22T22:32:24.000Z</published>
    <updated>2017-01-28T08:58:52.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><ul>
<li>对数正态分布（lognormally distributed）：对数为正态分布的任意随机变量的概率分布。<ul>
<li>如果 X 是正态分布的随机变量，则 exp(X)为对数正态分布.</li>
<li>如果 Y 是对数正态分布，则 ln(Y) 为正态分布。</li>
<li>如果一个变量可以看作是许多很小独立因子的乘积，则这个变量可以看作是对数正态分布。</li>
<li>对数正态分布的概率密度函数为：</li>
</ul>
</li>
</ul>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcjw1fbfr6mn9ccj308q01ldfr.jpg" alt=""></p>
<ul>
<li>对数平均：对数平均与几何平均相等，并且比算数平均，对于对数正态分布数据的典型值更具代表性<ul>
<li>二个数字的对数平均小于其算术平均，大于几何平均，若二个数字相等，对数平均会等于算数平均及几何平均。</li>
</ul>
</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcjw1fbfr6kv30dj30f701mmx5.jpg" alt=""></p>
<ul>
<li>Histogram of Oriented Gradients for Objection Detection.(HOG)步骤：<ul>
<li>Sampling positive images</li>
<li>Sampling negative images</li>
<li>Training a Linear SVM</li>
<li>Performing hard-negative mining</li>
<li>Re-training your Linear SVM using the hard-negative samples</li>
<li>Evaluating your classifier on your test dataset, utilizing non-maximum suppression to ignore redundant, overlapping bounding boxes</li>
</ul>
</li>
<li>NMS:Non-maximum Suppression(非极大值抑制):可看成一种局部极大值搜索，这里的局部极大值要比他的邻域值都要大。这里的邻域表示有两个参数：维度和n-邻域。</li>
<li>LBP: Local Binary Patterns</li>
</ul>
<a id="more"></a>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>单目图像的行人检测方法持续的在发展。</li>
<li>多种数据集+广泛变化的评估方法-&gt;导致方法之间直接的比较很困难。</li>
<li>三个贡献：<ul>
<li>数据集；</li>
<li>精炼的pre-image评估方法；</li>
<li>对现有state-of-art检测器进行比较评估。</li>
</ul>
</li>
<li>行人检测在 <strong>低像素</strong> 和 <strong>部分遮挡</strong> 行人情况下依旧表现失望。</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>对现有行人检测方法的一些疑问：<ul>
<li>Do current detectors work well?</li>
<li>What is the best ap- proach?” “What are the main failure modes?</li>
<li>What are the most productive research directions?</li>
</ul>
</li>
</ul>
<h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><ul>
<li>Data set.<ul>
<li>350,000行人标注框BB</li>
<li>250,000帧</li>
<li>遮挡和时间上相似的也被标注。</li>
<li>对行人等级、遮挡、位置进行了统计</li>
</ul>
</li>
<li>Evaluation methodology.</li>
<li>Evaluation.<ul>
<li>评估了16个有代表性的先进的行人检测器</li>
</ul>
</li>
<li>两个团队发布的surveys与作者的工作是互补的。<ul>
<li>Geronimo——先进的司机助手系统中的行人检测。</li>
<li>Enzweiler and Gavrila——发布Daimler检测数据集</li>
</ul>
</li>
</ul>
<h2 id="The-Caltech-Pedestrian-Data-Set"><a href="#The-Caltech-Pedestrian-Data-Set" class="headerlink" title="The Caltech Pedestrian Data Set"></a>The Caltech Pedestrian Data Set</h2><h3 id="Data-Collection-and-Ground-Truthing"><a href="#Data-Collection-and-Ground-Truthing" class="headerlink" title="Data Collection and Ground Truthing"></a>Data Collection and Ground Truthing</h3><ul>
<li>当一个标注器在至少两帧中在同一个行人上标注了边界框，则边界框利用3次插值在中间帧进行标注</li>
<li>BB-full</li>
<li>BB-vis: 被遮挡行人可见区域标注框</li>
<li>三种标注：<ul>
<li>Person</li>
<li>People</li>
<li>Person?</li>
</ul>
</li>
</ul>
<h3 id="Data-Set-Statistics"><a href="#Data-Set-Statistics" class="headerlink" title="Data Set Statistics"></a>Data Set Statistics</h3><ul>
<li>行人的高度、宽度都类似对数正态分布。</li>
<li>如果多变量中的每个变量符合对数正态分布，则这些变量的线性组合也符合正态分布。</li>
<li>BB 横纵比 w/ h, log(w /h) = log(w)-log(h).</li>
<li>由于行人姿势（手、肘）的原因，会导致行人宽度变化。</li>
<li>h=Hf/ d. H=1.8m, d=1800 /hm</li>
<li>大部分行人都被观察为medium scale，为了安全系统，检测也必须发生在这个scale。</li>
<li>通过对遮挡情况的统计，总体来说，遮挡情况远远没有统一， <strong>利用这个发现可以挡住提高行人检测器的性能。</strong></li>
<li>不仅遮挡是高度不一致的， <strong>遮挡的类型也是有明显额外的结构。</strong></li>
<li>通过对比ground truth和HOG检测器检测出的行人位置进行同样的统计对比，有如下的图，可以发现，利用行人位置这一约束条件可以合理的加快检测但是只能适当的减少假正例。</li>
</ul>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcjw1fbfr6mn9ccj308q01ldfr.jpg" alt=""></p>
<h3 id="Training-and-Testing-Data"><a href="#Training-and-Testing-Data" class="headerlink" title="Training and Testing Data"></a>Training and Testing Data</h3><ul>
<li>四种训练/测试情景<ul>
<li>Scenario ext0:</li>
<li>Scenario ext1:</li>
<li>Scenario cal0:</li>
<li>Scenario cal1:</li>
</ul>
</li>
<li>作者应该在检测器开发过程中使用 ext0/cal0，并且只在完成所有参数后再 ext1/cal1下进行评估。</li>
</ul>
<h3 id="Comparison-of-Pedestrian-Data-Sets"><a href="#Comparison-of-Pedestrian-Data-Sets" class="headerlink" title="Comparison of Pedestrian Data Sets"></a>Comparison of Pedestrian Data Sets</h3><ul>
<li>Imaging setup.</li>
<li>Data set size.</li>
<li>Data set type.</li>
<li>Pedestrian scale.</li>
<li>Data set properties.</li>
</ul>
<h2 id="Evaluation-Methodology"><a href="#Evaluation-Methodology" class="headerlink" title="Evaluation Methodology"></a>Evaluation Methodology</h2><h3 id="Full-Image-Evaluation"><a href="#Full-Image-Evaluation" class="headerlink" title="Full Image Evaluation"></a>Full Image Evaluation</h3><ul>
<li>对阈值小于0.6的评估就不用关心了。</li>
<li>具有最高置信度的检测首先被匹配；如果一个被检测到的BB匹配到多个ground truth边界框，则具有最高覆盖率的匹配将被使用。</li>
<li>没有被匹配到的BBdt算作假正例，没有被匹配道德BBgt被称为假负例。</li>
<li>通过改变检测置信度的阈值，画出miss rate-FPPI的曲线图来比较各个检测器。这种图比准确率-召回率的图更好，因为对于汽车应用，as typically there is an upper limit on the acceptable false positives per image rate independent of pedestrian density.</li>
<li>利用 <strong>对数平均 遗漏率</strong> 来总结检测器的性能。在（10^-2~10^0）范围的对数空间中，通过9个FPPI率平均计算miss rate来得到</li>
</ul>
<h3 id="Filtering-Ground-Truth"><a href="#Filtering-Ground-Truth" class="headerlink" title="Filtering Ground Truth"></a>Filtering Ground Truth</h3><ul>
<li>BBig: 被选择忽略的Ground truth. 被忽略的区域。</li>
<li>将BBgt设为被忽略和丢弃掉这个样本是不一样的；后者代表这个样本是一个假正例。</li>
</ul>
<h3 id="Filtering-Detections"><a href="#Filtering-Detections" class="headerlink" title="Filtering Detections"></a>Filtering Detections</h3><ul>
<li>考虑三种可能的过滤策略：<ul>
<li>strict filtering: 在匹配之前删除所选范围之外的所有检测。</li>
<li>postfiltering: 在所选评价范围外的检测允许与范围内的BBgt匹配。</li>
<li>expanded filtering: 类似于严格过滤，除了在评估之前去除扩展评估范围之外的所有检测</li>
</ul>
</li>
<li>Expanded filtering 在 strict filtering 和 postfiltering之间做了很好的妥协。</li>
<li>在整个评估工作中，我们使用expanded filtering(r=1.25)。</li>
</ul>
<h3 id="Standardizing-Aspect-Ratios"><a href="#Standardizing-Aspect-Ratios" class="headerlink" title="Standardizing Aspect Ratios"></a>Standardizing Aspect Ratios</h3><ul>
<li>标准化GT和DT的aspect ratio，这样做会从检测器设计中删除无关的任意选择，并有助于性能比较。</li>
<li>一般来说，探测器的长宽比取决于开发过程中使用的数据集，通常在训练后选择。</li>
<li>我们建议将所有BB标准化为0.41的长宽比（Caltech数据集中的对数 - 平均长宽比）。</li>
<li>我们保持BB高度和中心固定，同时调整宽度.</li>
<li>重要的是检测器和ground truth纵横比匹配。</li>
</ul>
<h3 id="Per-Window-versus-Full-Image-Evaluation"><a href="#Per-Window-versus-Full-Image-Evaluation" class="headerlink" title="Per-Window versus Full Image Evaluation"></a>Per-Window versus Full Image Evaluation</h3><ul>
<li>PM 评估方法通常用来比较分类器（检测器的返利）或者用来评估系统对于自动兴趣区域生成的性能。</li>
<li>PW结果是从其 <strong>原始出版物</strong> 中产生的。</li>
<li>全图像结果是通过评估同一行人但在其 <strong>原始图像上下文</strong> 中获得的。</li>
<li>将一个二分类转化为一个检测器所做的选择包括：<ul>
<li>包括空间和尺度跨度</li>
<li>非最大抑制的选择。会影响图像的性能。</li>
<li>在PW评估期间测试的窗口通常不同于在全图像检测期间测试的窗口，</li>
</ul>
</li>
<li>假阳性可能来自对身体部位或不正确的尺度或位置的检测</li>
<li>假阴性可能源于被测试的窗户和真实的行人位置或来自NMS之间的轻微不对准。</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcjw1fbfr6loh02j30as08vwey.jpg" alt=""></p>
<h2 id="检测算法"><a href="#检测算法" class="headerlink" title="检测算法"></a>检测算法</h2><h3 id="Survey-of-the-State-of-the-Art"><a href="#Survey-of-the-State-of-the-Art" class="headerlink" title="Survey of the State of the Art"></a>Survey of the State of the Art</h3><ul>
<li>Papageorgiou and Poggio [16]提出了第一个滑动窗口检测器。将支持向量机（SVM）应用于多尺度Haar小波的过度完备字典。</li>
<li>Viola和Jones( <strong>VJ</strong> )[44]基于这些想法，引入了用于快速特征计算的积分图像和用于有效检测的级联结构，以及利用AdaBoost进行自动特征选择。这些想法继续作为现代探测器的基础。</li>
<li>随着基于梯度的特征的采用带来了巨大的收益。</li>
<li>受SIFT [45]启发，Dalal和Triggs [HOG][7]通过显示相对于基于强度的特征的实质性增益，普及了用于检测的定向梯度特征的直方图（HOG）。</li>
<li>现在，HOG特征的变体的数量已经大大增加，几乎所有现代检测器以某种形式利用它们。</li>
<li>Shape features（形状特征）也是一个用于检测经常用到的线索.</li>
<li>Boosting用于学习头部，躯干，腿部和全身检测器.</li>
<li>Shapelets: 是从局部区块中的梯度辨别地学习的形状描述符.</li>
<li>Boosting用来将多个Shapelet结合成一个整体的检测器。</li>
<li>Motion是人类感知的另一个重要提示; 然而，成功地将运动特征结合到检测器中已证明对于移动的相机具有挑战性。</li>
<li>虽然没有迹象表明单个特征由于HOG，但附加特征可以提供一些互补信息。</li>
</ul>
<h3 id="Evaluated-Detectors"><a href="#Evaluated-Detectors" class="headerlink" title="Evaluated Detectors"></a>Evaluated Detectors</h3><ul>
<li>直接从作者处得到提前训练好的检测器。</li>
<li>这些检测器通常遵循滑动窗口范例，其需要对检测窗口进行特征提取，二分类和密集多尺度扫描，随后进行非极大值抑制。</li>
<li>Features：几乎所有的现代检测器都使用了都写形式的梯度直方图。</li>
<li>Learning：因为它们的理论保证，可扩展性和良好的性能，支持向量机[16]和boosting[44]是最受欢迎的选择。</li>
<li>Boosting自动执行特征选择。一些检测器（在“特征学习”列中用标记指示）在分类器训练之前或与分类器训练一起学习更小或中等大小的特征集合。</li>
<li>Detection details：两种主要的非最大抑制方法：<ul>
<li>Mean shift(MS)模型估计</li>
<li>Pairwise max(PM)抑制：根据充分的重叠丢弃可信度较低的每对检测</li>
<li>PM*：允许检测去匹配另一检测的任意子区域。</li>
</ul>
</li>
<li>Implementation notes</li>
</ul>
<h2 id="Performance-Evaluation"><a href="#Performance-Evaluation" class="headerlink" title="Performance Evaluation"></a>Performance Evaluation</h2><h3 id="Performance-on-the-Caltech-Data-Set"><a href="#Performance-on-the-Caltech-Data-Set" class="headerlink" title="Performance on the Caltech Data Set"></a>Performance on the Caltech Data Set</h3><ul>
<li>Overall：绝对性能很弱。</li>
<li>Scale</li>
<li>Occlusion</li>
<li>Reasonale：性能在中等规模或部分封闭的行人的检测很差，而对于远距离或在重度封闭的情况下，它的性能特别差。这促使我们评估超过50像素高的行人在没有或部分遮挡（这些在没有很多上下文的情况下清晰可见）的性能。 <strong>我们将这称为合理的评估设置。</strong></li>
<li>Localization</li>
</ul>
<h3 id="Evaluation-on-Multiple-Data-Sets"><a href="#Evaluation-on-Multiple-Data-Sets" class="headerlink" title="Evaluation on Multiple Data Sets"></a>Evaluation on Multiple Data Sets</h3><h3 id="Statistical-Significance（统计显著性）"><a href="#Statistical-Significance（统计显著性）" class="headerlink" title="Statistical Significance（统计显著性）"></a>Statistical Significance（统计显著性）</h3><ul>
<li>关键的洞察力是将每个数据集上的绝对性能转换为算法排名，从而消除不同数据集难度的影响。</li>
<li>我们使用非对数Friedman检验和posthoc分析来分析统计学显着性.</li>
<li>对于我们的分析，我们使用 <strong>非参数Friedman测试</strong> 以及 <strong>Shaffer posthoc test</strong></li>
<li>我们基于其对数平均丢失率（在合理的评估设置下测试）对每个数据折叠上的检测器进行排名。 该程序为14个检测器得到做总共28个排名。</li>
</ul>
<h3 id="Runtime-Analysis"><a href="#Runtime-Analysis" class="headerlink" title="Runtime Analysis"></a>Runtime Analysis</h3><ul>
<li>总的来说，运行时和精度之间似乎没有很强的相关性。</li>
</ul>
<h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><ul>
<li>应该注意，单帧性能是整个系统性能的下限，跟踪，上下文信息和额外传感器的使用可以帮助减少假警报并提高检测率（参见[2]）。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;知识点&quot;&gt;&lt;a href=&quot;#知识点&quot; class=&quot;headerlink&quot; title=&quot;知识点&quot;&gt;&lt;/a&gt;知识点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;对数正态分布（lognormally distributed）：对数为正态分布的任意随机变量的概率分布。&lt;ul&gt;
&lt;li&gt;如果 X 是正态分布的随机变量，则 exp(X)为对数正态分布.&lt;/li&gt;
&lt;li&gt;如果 Y 是对数正态分布，则 ln(Y) 为正态分布。&lt;/li&gt;
&lt;li&gt;如果一个变量可以看作是许多很小独立因子的乘积，则这个变量可以看作是对数正态分布。&lt;/li&gt;
&lt;li&gt;对数正态分布的概率密度函数为：&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://ww2.sinaimg.cn/large/006tKfTcjw1fbfr6mn9ccj308q01ldfr.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对数平均：对数平均与几何平均相等，并且比算数平均，对于对数正态分布数据的典型值更具代表性&lt;ul&gt;
&lt;li&gt;二个数字的对数平均小于其算术平均，大于几何平均，若二个数字相等，对数平均会等于算数平均及几何平均。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://ww4.sinaimg.cn/large/006tKfTcjw1fbfr6kv30dj30f701mmx5.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Histogram of Oriented Gradients for Objection Detection.(HOG)步骤：&lt;ul&gt;
&lt;li&gt;Sampling positive images&lt;/li&gt;
&lt;li&gt;Sampling negative images&lt;/li&gt;
&lt;li&gt;Training a Linear SVM&lt;/li&gt;
&lt;li&gt;Performing hard-negative mining&lt;/li&gt;
&lt;li&gt;Re-training your Linear SVM using the hard-negative samples&lt;/li&gt;
&lt;li&gt;Evaluating your classifier on your test dataset, utilizing non-maximum suppression to ignore redundant, overlapping bounding boxes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NMS:Non-maximum Suppression(非极大值抑制):可看成一种局部极大值搜索，这里的局部极大值要比他的邻域值都要大。这里的邻域表示有两个参数：维度和n-邻域。&lt;/li&gt;
&lt;li&gt;LBP: Local Binary Patterns&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Pedestrian Detection" scheme="http://yoursite.com/tags/Pedestrian-Detection/"/>
    
      <category term="行人检测" scheme="http://yoursite.com/tags/%E8%A1%8C%E4%BA%BA%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Review" scheme="http://yoursite.com/tags/Review/"/>
    
      <category term="综述" scheme="http://yoursite.com/tags/%E7%BB%BC%E8%BF%B0/"/>
    
      <category term="Caltech" scheme="http://yoursite.com/tags/Caltech/"/>
    
  </entry>
  
  <entry>
    <title>行人检测论文笔记：Pedestrian Detection - A Benchmark</title>
    <link href="http://yoursite.com/posts/4124170924/"/>
    <id>http://yoursite.com/posts/4124170924/</id>
    <published>2016-11-19T03:48:55.000Z</published>
    <updated>2017-01-28T08:58:33.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><ul>
<li>k折交叉验证</li>
<li>Non-Maximum Suppression：非极大值抑制算法，非极大值抑制（NMS）可以看做是抑制不是极大值的元素，搜索局部的极大值的搜索问题，NMS是许多计算机视觉算法的部分。<ul>
<li>这个局部代表的是一个邻域，邻域有两个参数可变，一是邻域的维数，二是邻域的大小。</li>
<li>在行人检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是行人的概率最大），并且抑制那些分数低的窗口。</li>
</ul>
</li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>引进了一个新的数据集——Caltech。</li>
<li>提出了了个更高的评估标准。</li>
<li>证明了平常用的逐个窗口检测的方法是有瑕疵的，在完整的图片上会预测失败。</li>
<li>衡量了现有的检测系统。</li>
<li>分析了一般的常见失败情况。</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>INRIA数据集。</li>
<li>现有数据集的缺陷。</li>
<li>贡献（4方面）。</li>
</ul>
<h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><ul>
<li>介绍了Caltech数据集的数据内容，标记等。</li>
<li><p>Scale(等级，范围)根据行人的图片大小，将行人分为3个范围：near（80或者更多像素）、medium（30-80像素之间）、far（30像素或更少）。</p>
<ul>
<li>大约68%的行人位于中等大小范围。</li>
<li>对于medium范围的加测对于汽车应用是十分重要的。</li>
<li>我们应当在整个工作中利用ner/ medium /far之间的区别。</li>
</ul>
</li>
<li><p>Occlusion(遮挡)</p>
<ul>
<li>遮挡的行人通过两个框来标注。</li>
<li>29%的行人从来没有被挡住</li>
<li>53%的呗挡在一部分帧</li>
<li>19%的在所有帧中都被挡</li>
</ul>
</li>
<li><p>Position(位置)：由于视点和地表形状的原因约束着行人值出现在图片的特定区域，经过分析，行人文职更加集中而不是突然出现的。</p>
</li>
<li>数据捕捉了超过11种场景:0-5用来作为训练，6-10用来作为测试</li>
<li><p>设置了三个具体的训练/测试场景</p>
<ul>
<li>Scenario-A：在所有外部数据上进行训练，在会话6-10上进行测试。这样允许在已经存在的方法上不进行重新训练就能进行广泛的调查。</li>
<li>Scenario-B：利用会话0-5进行6折交叉验证，每次使用5个session来进行训练，第6个进行测试，然后在验证集上融合结果，在政策训练集上汇报检测器的表现。</li>
<li>Scenario-C：用0-5会话来训练，用6-10会话来测试。（完整测试）</li>
</ul>
</li>
<li><p>与现有的数据集的比较：</p>
<ul>
<li>广泛使用的‘人’数据集：MIT LabelMe的子集和PASCAL VOC数据集。</li>
<li>现有数据集可以分为两类：一类是人数据集包含了人的各种姿势，另一类是行人数据集包含了垂直的人（站立或者行走），但主要是从一个较为限制的视点进行观察的。</li>
<li>从摄影师处收集的数据集都存在 <strong>选择偏差</strong> ，但是监控视频有着有限的背景，移动拍摄的数据会极大的排除了选择偏差。</li>
<li>INRIA偏向于打的，大部分未遮挡的行人</li>
<li>其他相关的数据集有：DC，ETH</li>
</ul>
</li>
<li>Caltech数据集最先进和重要的方面，而且这是目前第一个数据集与时间相对应的标注框和详细遮挡标签。</li>
</ul>
<h2 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h2><ul>
<li>现有的已建立的评估行人检测方法是有瑕疵的。</li>
<li>pre-window VS pre-image</li>
<li>pre-window：逐窗口检测器在图像上被密集扫描并且邻近的检测被合并，比如使用NMS。</li>
<li>一个典型的假设是：较好的pre-window分数会在一整个图片上带来更好的表现；然而在实际中pre-window表现在预测pre-image性能时失败。</li>
<li>不是所有检测系统都是基于华东窗口的，而且pre-window方法对这类系统的评估是不可能的。</li>
</ul>
<h3 id="Pre-image-evaluation"><a href="#Pre-image-evaluation" class="headerlink" title="Pre-image evaluation"></a>Pre-image evaluation</h3><ul>
<li>利用PASCAL物体检测挑战中的修改过的scheme版本进行单帧检测。</li>
<li>一个检测系统需要输入一个图像并且为每个检测返回一个边界框或者一个分数或者一个置信度。这个系统应该可以执行多等级检测以及必要的NMS或者其他后期处理。</li>
<li>评估应该在最后生成的被检测到的边界框中执行。</li>
<li>PASCAL估计：重叠区域必须超过50%：</li>
</ul>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfqwf7ettj309801oglo.jpg" alt=""></p>
<ul>
<li>为了比较方法，通过变化检测置信度的阈值，我们画出了纵坐标miss rate，横坐标每张图像假正例（FPPI）的图像。对于某些任务，更倾向于使用查准-召回曲线，比如汽车应用，典型的已经有一个可接受的FPPI上限，并且独立于行人行人密度。</li>
<li>引入ignore regions。这一区域不需要匹配，匹配上不算是TP，没有匹配上也不算FN。</li>
<li>只有完整的标注框才能用来匹配，不是可见的标注框，甚至对于部分遮挡的行人。</li>
</ul>
<h2 id="Evaluation-Results"><a href="#Evaluation-Results" class="headerlink" title="Evaluation Results"></a>Evaluation Results</h2><ul>
<li>Overall</li>
<li>Scale</li>
<li>Occlusion</li>
<li>Aspect ratio</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;知识点&quot;&gt;&lt;a href=&quot;#知识点&quot; class=&quot;headerlink&quot; title=&quot;知识点&quot;&gt;&lt;/a&gt;知识点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;k折交叉验证&lt;/li&gt;
&lt;li&gt;Non-Maximum Suppression：非极大值抑制算法，非极大值抑制（NMS）可以看做是抑制不是极大值的元素，搜索局部的极大值的搜索问题，NMS是许多计算机视觉算法的部分。&lt;ul&gt;
&lt;li&gt;这个局部代表的是一个邻域，邻域有两个参数可变，一是邻域的维数，二是邻域的大小。&lt;/li&gt;
&lt;li&gt;在行人检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是行人的概率最大），并且抑制那些分数低的窗口。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;引进了一个新的数据集——Caltech。&lt;/li&gt;
&lt;li&gt;提出了了个更高的评估标准。&lt;/li&gt;
&lt;li&gt;证明了平常用的逐个窗口检测的方法是有瑕疵的，在完整的图片上会预测失败。&lt;/li&gt;
&lt;li&gt;衡量了现有的检测系统。&lt;/li&gt;
&lt;li&gt;分析了一般的常见失败情况。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Pedestrian Detection" scheme="http://yoursite.com/tags/Pedestrian-Detection/"/>
    
      <category term="行人检测" scheme="http://yoursite.com/tags/%E8%A1%8C%E4%BA%BA%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Review" scheme="http://yoursite.com/tags/Review/"/>
    
      <category term="综述" scheme="http://yoursite.com/tags/%E7%BB%BC%E8%BF%B0/"/>
    
      <category term="Caltech" scheme="http://yoursite.com/tags/Caltech/"/>
    
  </entry>
  
  <entry>
    <title>南清北复交北航哈工大中科院华科保研记</title>
    <link href="http://yoursite.com/posts/3356325341/"/>
    <id>http://yoursite.com/posts/3356325341/</id>
    <published>2016-08-22T02:26:55.000Z</published>
    <updated>2017-02-01T08:37:03.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>7月23号从中科院软件所参加完夏令营回来，我的漫长的保研路也算是告一段落。</p>
<p>8月12号东软实训结束，8月13号坐上回家的火车，8月14号到家，然后就一直吃喝睡到今天，拿回来的几本书也没看几眼，本来打算着回来继续充实一下，去备战9，10月份的推免，现在看来时间又都荒废了……开学还是乖乖到学校吧，再这样下去一直待在家感觉要成废人一个了，我还是喜欢忙碌充实的感觉。</p>
<p>一直想着要把这次宝贵的保研经历记录一下，好给学弟学妹一个参考。学弟学妹们可以结合自身情况，大概了解一下保研流程，部分学校保研考核要求，从而少走一些弯路，去到自己理想中的学校。</p>
<a id="more"></a>
<h2 id="个人基本情况"><a href="#个人基本情况" class="headerlink" title="个人基本情况"></a>个人基本情况</h2><p>把我的个人情况大概说一下，学弟学妹可以对比自己情况，有个参考。</p>
<ul>
<li>绩点排名：5/262</li>
<li>四级：611分</li>
<li>六级：503分</li>
<li>科研竞赛情况：无科研，无论文，无国奖，无励志奖学金，无ACM，仅有一个蓝桥和美赛SP（其实没什么价值），总的来说也就是成绩好一点，但有时候高的成绩排名会是一个很好的敲门砖。</li>
</ul>
<h2 id="为什么我要保研"><a href="#为什么我要保研" class="headerlink" title="为什么我要保研"></a>为什么我要保研</h2><p>本来我是做iOS开发的，想要毕业后直接找工作，不准备读研的。但是直到大三上学期的时候通过和学长学姐交流，我才了解到保研夏令营这一回事，由于学长学姐都去了很好的高校读研，而且当时我的排名还不错，于是我思考了好长时间，权衡了读研和找工作的利弊，我决定毕业后继续读研。如果有同样疑惑的学弟学妹，我的建议是：能出国的话，那肯定出国；能保研的话，那肯定最好不要放弃这个来之不易的机会，当然这也不是绝对的。</p>
<h2 id="我的保研路"><a href="#我的保研路" class="headerlink" title="我的保研路"></a>我的保研路</h2><h3 id="报名参加过的夏令营"><a href="#报名参加过的夏令营" class="headerlink" title="报名参加过的夏令营"></a>报名参加过的夏令营</h3><p>这一部分是我报名相应学校夏令营后，对方给予我入营资格的学校，以下是按参加顺序记录：</p>
<h4 id="1-南大计算机"><a href="#1-南大计算机" class="headerlink" title="1. 南大计算机"></a>1. 南大计算机</h4><ul>
<li><p>网址：<a href="http://cs.nju.edu.cn/ee/57/c1654a126551/page.htm" target="_blank" rel="external">南京大学计算机系2016“本科生开放日”申请流程</a></p>
</li>
<li><p>时间：5月13日-5月15日</p>
</li>
<li><p>入营条件：985院校的话，绩点排名前5%基本可以入营</p>
</li>
<li><p>吃住补助：LAMDA实验室报销车票，住宿费，但是南大不报销车票，但管吃管住，住的很高级的宾馆，条件特别好。</p>
</li>
<li><p>参营记录：</p>
<p>南大的夏令营是开的最早的一个计算机夏令营，正因为开的早，很多同学都蠢蠢欲试，导致南大夏令营会有很多同学报名，这次有1000多个报名的，但是最后入营的只有300人，但只要绩点排名前5%基本可以入营。</p>
<p>如果你想要之后从事”机器学习与数据挖掘“相关领域的科研工作，那么南大的<a href="http://lamda.nju.edu.cn/CH.MainPage.ashx" target="_blank" rel="external">LAMDA实验室</a>是一个再好不过的选择了，由周志华教授作为带头人，南大的这个实验室在国内外上知名度很高，科研能力也很厉害。</p>
<p>但是LAMDA实验室是<strong>单独招生的</strong>，每年会有两批保研生的申请，所以要想申请LAMDA实验室，还需要在申请南大夏令营之外单独申请这个实验室的面试考核。申请的时候需要一份简历，一份研究动机说明，一份成绩单，这三份材料是让对方了解你的唯一途径，所以需要好好准备润色，虽然我是在截止日期前一天才急急忙忙的才提交的，但还是进入了初选，拿到了LAMDA实验室的面试资格。LAMDA实验室会在南大夏令营前一天组织面试，而且给报销往返车票及一晚的住宿费，还是很不错的。</p>
<p>下面说一下LAMDA大概的面试流程及问题。在申报LAMDA实验室之前会填三个导师志愿，我当时填的周志华、吴建鑫、俞扬，但到了现场才知道LAMDA会把你的材料分别送给所填报的三个老师，他们分别决定是否允许你参加他们每个人的面试，如果有老师允许你面试，那么你就拿到了面试资格，否则的话，说明没有老师看上你的简历，你也就没有面试资格。所以在填报导师的时候，除非自己简历特别光鲜，否则那种特别牛老师就不要填了，相当于浪费一个机会。在面试的时候，你会先去面试选中你的老师，面试完之后，你其实可以再去面试其他该实验室的老师，从而增加你进入该实验室的机会，即使你当时没有报他的研究生。我是5月12号参加的面试，当时只有吴建鑫老师给了我面试机会，面试完他之后我又去找了詹德川老师，最后我是被詹德川老师录取。面试吴建鑫老师的时候，问题如下：</p>
<ul>
<li>方差的计算方法，他会提前写好一个方差表达式问你对不对，如果不对的话请写出正确的表达式；</li>
<li>方差中的n-1含义；</li>
<li>如果写一个程序计算方差，那么计算一次内存访问几次；</li>
<li>本科做过的项目，项目内容；</li>
<li>机器学习了解多少，看过什么；</li>
<li>了不了解本人是做什么研究的。</li>
</ul>
<p>面试詹德川老师的时候，我不知道其他人是什么情况，但我是全程英文面试，问到的问题如下，<strong>注意：以下问题需要英文作答，实在英语回答不了，可以中文，不必有太大压力</strong>：</p>
<ul>
<li>自我介绍一下；</li>
<li>介绍一下做过的项目；</li>
<li>介绍一下梯度下降法是什么；</li>
<li>介绍一下牛顿迭代是什么；</li>
<li>什么是特征值，特征值的含义；</li>
<li>唠嗑。</li>
</ul>
<p>面试完当天晚上就会有邮件通知是否面试通过，最后我是被詹德川老师录取，但是通过LAMDA面试并不代表你就能100%能进入LAMDA读研了，最重要的一关是拿到南大夏令营的优秀营员，只有这样，通过两轮考核才能进入LAMDA。</p>
<p>下面说一下南大夏令营的流程，主要是三部分：各个实验室介绍、机试、面试，其中最最重要的是机试，只要机试通过，90%就能拿到优秀营员。</p>
<p>机试这次一共4道题，以前听说6道题，只要AC出其中的两道题就肯定没问题了，多做无用，有罚时，考核方式：OJ，题型：算法+数据结构，难度：ACM一般难度的题。这次的具体题目如下：</p>
<ul>
<li>最大子串和；</li>
<li>无向图最长路径</li>
<li>表达式求值；</li>
<li>给一棵树求最长的路径。</li>
</ul>
<p>面试问题如下（由于分组面试，每组没人问题不一样）：</p>
<ul>
<li>解释一下什么是时间复杂度；</li>
<li>快排的时间复杂度；</li>
<li>快排最坏时间复杂度为什么是$O(n^2)$，如何优化快排最坏时间复杂度；</li>
<li>看成绩单，问如何进行文献检索</li>
<li>英文回答：如何利用文献检索知识去检索一个机器学习的问题</li>
</ul>
<p>15号上午面试完毕，提交完材料，填完导师志愿，下午就可以走了，导师志愿基本只有第一志愿有用，而且有些导师有一个点招权（反正我目前貌似只知道黄宜华老师有点招权，点招了我身边好多同学，但他们都拒绝了。。。他是搞大数据很厉害的一个老师，如果对这方面感兴趣，成绩排名又靠前的话，千万记住找这个老师报名，多一个机会！）。</p>
<p>南大之旅也就这样结束了，接下来会在官网上公布优秀营员的名单。</p>
</li>
</ul>
<h4 id="2-复旦计算机"><a href="#2-复旦计算机" class="headerlink" title="2. 复旦计算机"></a>2. 复旦计算机</h4><ul>
<li><p>网址： <a href="http://www.cs.fudan.edu.cn/?p=19403" target="_blank" rel="external">2016年复旦大学计算机科学技术学院和软件学院优秀大学生夏令营活动报名通知</a></p>
</li>
<li><p>时间：7月4日~7月8日</p>
</li>
<li><p>入营条件：对于东北大学计算机软件专业的同学，每个专业只会给一个入营资格，所以排名最高的那个人才能入营</p>
</li>
<li><p>吃住补助：报销单程的路费（无论什么以硬卧为准），管吃住，条件都还不错。</p>
</li>
<li><p>参营记录：</p>
<p>复旦大学计算机虽然排名没有东大计算机高，但是毕竟复旦，还在上海，所以还是很值得去试一试的，但上海的学校貌似都比较”傲娇“，给的名额真的特别少，考核也是很严格的，这次去了复旦才知道一共有600个人报名夏令营，最后只有50个人入营，而且最后在这50个人中只会发放14个拟录取，所以这个从这个录取比例可以看出竞争很激烈，还是需要认真的准备。</p>
<p>夏令营是在复旦大学张江校区举行，为期5天，主要活动包括：</p>
<ul>
<li>学术报告。将邀请在学术研究方面有建树的教师进行学术报告，介绍计算机科学技术学院和软件学院最新的研究方向和研究成果，为期2天；</li>
<li>课题组的学术讨论。进入感兴趣的课题组和老师进行进一步的交流，参加课题组的科研活动； 可以提前联系导师，趁早去找老师唠一唠，让他认识你，了解你，而最好在面试之前就定下导师，否则面试会减分；</li>
<li>考核。上午机试，下午面试，面试的时候分两组，每组5个老师，老师手中会有你的机试成绩，所以机试好的话，给老师印象会很好，也就是说只要机试分数高，进复旦就容易很多了。</li>
</ul>
<p>复旦搞计算机视觉和媒体（视频、图片）大数据分析的居多，基本很多老师都在围绕这个来展开科研工作的。</p>
<p>下面说一下机试。机试这次和以往完全不一样，虽然也是OJ，但这次是给你3个大题，每个大题中有3个小题，每个小题之间的区别就是约束条件和数据量级不同，对应的题目难易程度也是不一样的，所以和ACM的题型还不太一样。具体题目记得不太清楚，大概如下：</p>
<ul>
<li>第一题类似迷宫问题，利用BFS求解，一个n*n个方格组成的方阵，里面可能有若干个门，每个门对应着一把钥匙，钥匙会出现在某个方格中，所以要想开门就必须先把钥匙拿到，你需要给出从起点到终点的可能路径之和，迷宫会有多种多样。三个小问分别是：<ul>
<li>对于1*n的迷宫，求出问题的解。</li>
<li>对于n*n的迷宫，没有门，求出问题的解。</li>
<li>对于n*n的迷宫，有门，求出问题解。</li>
</ul>
</li>
<li>第二题貌似是车过桥问题，由于桥有限高，所以车需要有不同的装载方案来过不同的桥，貌似需要求解出装载方案，这个题没有仔细看。</li>
<li>第三题很常规的一道ACM字符串题目，具体题目记不太清，但是主要考察你在特别大的数量级下能否在规定时间内求出解。</li>
</ul>
<p>下面说一下面试。面试分两组，而且有专业面试和英语面试两个环节，每组同学到相应的组面试，面试问题大概如下：</p>
</li>
</ul>
<ul>
<li>专业面试：<ul>
<li>自我介绍；</li>
<li>介绍项目，做过什么，项目具体内容是什么；</li>
<li>机器学习了解多少，如何学习的；</li>
<li>说一下神经网络的优点缺点；</li>
<li>自我感觉机试做的怎么样；</li>
<li>学院院长是谁；</li>
<li>选好导师没有。</li>
</ul>
</li>
<li><p>英语面试：</p>
<ul>
<li>自我介绍；</li>
<li>说一下媒体大数据是什么。</li>
</ul>
<p>面试问题大概如上，面试还是很快地，面试完基本就可以走了，回去等着邮件通知是否通过即可，没有通过的还可以继续申报9月份的推免面试，这次是和本校学生一起竞争，所以竞争会更加激烈。还是一句话：”得机试者得天下“，虽然竞争激烈，但是只要机试分数高，胜算还是很大的。</p>
</li>
</ul>
<h4 id="3-北航计算机"><a href="#3-北航计算机" class="headerlink" title="3. 北航计算机"></a>3. 北航计算机</h4><ul>
<li><p>网址：<a href="http://scse.buaa.edu.cn/buaa-css-web/toTemplateAction.action?baseTemplateId=5849c5bd-9d91-4b4a-ad6b-1f4acbb781e7&amp;templateType=ARTICLE_TEMPLATE&amp;curSelNavId=NOTICE_PUBLICITY&amp;language=0" target="_blank" rel="external">报考2017年北航计算机学院硕士研究生 7月11~12日暑期宣传活动通知</a></p>
</li>
<li><p>时间：7月11日~7月12日</p>
</li>
<li><p>入营条件：对于985学校的学生，绩点排名前5%基本会有入营资格</p>
</li>
<li><p>吃住补助：什么都不报销！！！！不论吃的、住的！！！！</p>
</li>
<li><p>参营记录：</p>
<p>从复旦回来一天后，我就到了北航参加北航计算机夏令营，北航听说这次入营的有500多人，所以不包吃住很正常，因为根本管不过来。</p>
<p>北航夏令营只有两天，第一天机试，第二天面试，空余时间可以提前找一找联系的老师。北航有一个免机试政策，就是有CCF（计算机职业资格认证考试）成绩的同学，只要成绩在200分以上，带着成绩单去就可以免机试，还是不错的，可以省去机试好好准备面试了。</p>
<p>下面说一下机试，北航机试一共就两个题，分两场，两场题目不一样，做完只会显示是否编译通过，不会有任何错误提示信息，所以做完你也不知道是不是能把所有样例都通过，比较坑。我所参加的那场题目如下：</p>
<ul>
<li>找出一串数字中，连续递增子串的最大个数</li>
<li>哈弗曼树构造，编码</li>
</ul>
<p>北航不同的是机试不通过的话，是无法参加面试，而且面试是需要交100块钱的。面试的话，你需要学会去引导老师，让他去问你知道的东西，这样你才能把自己的优势展现出来。</p>
<p>面试结束后，千万不要走，因为晚上会贴出拟录取名单，第二天还会给你发拟录取证明，这个是不能代领的，所以面试结束最好先别急着走，等你拿到拟录取名单，就可以安安心心回家了。</p>
</li>
</ul>
<h4 id="4-哈工大计算机"><a href="#4-哈工大计算机" class="headerlink" title="4. 哈工大计算机"></a>4. 哈工大计算机</h4><ul>
<li><p>网址：没有通知，我是当时加了一个哈工大保研群才知道哈工大计算机的保研面试安排，群号：212632913</p>
</li>
<li><p>时间：7月17号</p>
</li>
<li><p>入营条件：感兴趣都可以去面试</p>
</li>
<li><p>参营记录：</p>
<p>哈工大计算机推免面试是分面试点的，当时在东大有一个面试点，而且一两天后就会出结果和你签拟录取合同，从而省下你专门跑到哈尔滨面试，还是很人性化的。哈工大面试分三个老师分别面试，分别面试三个方面：逻辑思维、专业知识、动手能力，老师都很和蔼的，根本不用紧张。下面是面试问到的问题：</p>
<ul>
<li>逻辑思维<ul>
<li>给你一道逻辑题，让你选出正确答案；</li>
<li>家乡是哪儿，毕竟是在哈尔滨，怕有些同学适应不了环境；</li>
<li>高考成绩等唠嗑性问题。</li>
</ul>
</li>
<li>专业知识<ul>
<li>大学什么科目学的比较好</li>
<li>B树是什么，主要作用是什么；</li>
<li>B树在数据库中如何应用；</li>
<li>给你很多学生的成绩，如何利用B树来进行检索；</li>
<li>反正基本围绕B树，因为是我引导的老师到这个问题上的；</li>
<li>机器学习了解多少。</li>
</ul>
</li>
<li>动手能力<ul>
<li>做过的项目；</li>
<li>涉及到的算法有什么。</li>
</ul>
</li>
</ul>
<p>面试完一两天基本就会出结果，我在签协议的时候，老师是这样和我说的：虽然我们不想招软件学院的学生，但是学校给的要求是：只要是985院校的学生，但凡不是特别差的，就都招了吧。。。所以，想要报哈工大或者想找一个保底的学校，最好不要放弃这个机会。</p>
</li>
</ul>
<h4 id="5-中科院软件所"><a href="#5-中科院软件所" class="headerlink" title="5. 中科院软件所"></a>5. 中科院软件所</h4><ul>
<li><p>网址：<a href="http://www.is.cas.cn/yjsjy/zsxx/201605/t20160524_4608413.html" target="_blank" rel="external">中国科学院大学2016年全国大学生“软件与网络”夏令营通知</a></p>
</li>
<li><p>时间：7月18日-7月23日</p>
</li>
<li><p>入营条件：对于东北大学学生，绩点排名前20%基本都会有入营资格</p>
</li>
<li><p>吃住补助：报销去程车票，提供住宿（学生公寓、两人间、环境感觉不好），给100元的饭卡，可在食堂和超市消费。</p>
</li>
<li><p>参营记录：</p>
<p>原本我并没有报名软件所的夏令营，只是报名了计算所的夏令营，因为我以为中科院只能报名一个研究院。但是计算所好像不是特别欢迎软件专业的学生，所以对于计算所，软件学院入营的同学屈指可数，而计算机学院入营的同学有十几个吧，最后我也没有入营计算所。</p>
<p>没有入营计算所的我以为我的夏令营就这样结束了，看着身边很多同学去参加软件所的夏令营，我当时真的是特别后悔为什么脑子短路不报软件所的夏令营。但是在软件所开营的前一天，我和另一个同样没有报名的同学得到消息说没有报名可以去现场报名，于是我们当天晚上头脑一热，就买了去北京的<strong>硬座</strong>，连夜坐到北京，准备霸面。在前一天去北京的车上，我们提前联系了几个老师，说明了一下自己情况。</p>
<p>第二天早晨到北京后，我们直奔中科院。由于没有任何计划，也不知道去了联系哪个老师，如何临时报名，就一直在软件所里面呆坐着。幸运的是直到下午，在同学和一个软件所学姐的帮助下，我们找到了软件所研究生办事处主任李彩丽老师，提交了部分材料，办了手续，领了公寓钥匙，才算报了名。（顺便说一句，李彩丽老师人特别好，有什么问题她都会尽量帮忙的！）由于软件所夏令营持续到23号，而我和另一个同学当时22号还要去参加华科夏令营，我们经过了长时间的心理斗争，决定放弃华科的夏令营。至此，我的软件所夏令营才幸运地开始，所以，保研过程中的许多机会都需要去争取的，运气也是很重要的，即使有时候觉得不可能，也要试一试，说不定运气好就得到了这个机会！</p>
<p>下面正式介绍一下软件所夏令营。软件所夏令营为期6天，来来回回基本就一个星期了。这6天里，第一天报道，然后接下来两天一样的听报告，但是在第二天听报告的下午需要填报两个实验室的志愿，这个志愿其实只有第一志愿有用，填完志愿后会当场统计人数，看有没有扎堆，如果有的话，可以当场改志愿，让每个实验室人数尽量均衡。关于各个实验室的好坏，这里有一篇挺公正的介绍——<a href="http://www.cskaoyan.com/thread-161418-1-1.html" target="_blank" rel="external">中科院软件所各实验室情况简要介绍</a>，总的来说：软工中心最好，人机最不受欢迎吧，学弟学妹填志愿的时候要注意，当然最好的实验室报的人也最多，录取比例当然更低。我当时报名的是天基和国重。</p>
<p>报完志愿后，第二天各个实验室就都开始各自的考核了，有的有机试、笔试、面试，有的只有笔试和面试，由于我报的天基，那么我只能说一说天基的考核方式了。天基只有笔试和面试，笔试的话其实考的都是很基础的东西，包括OS、计网、机组（考了很多选择）、数学、数据结构、算法等，平常认真考试的话，基本没什么问题。</p>
<p>下面说一下面试，面试最主要的就是3分钟的个人介绍PPT，所有老师提问都是通过你的PPT来提问，所以这个PPT需要废话少说，把你最精彩的部分讲出来，但是所有部分需要尽量真实，最好不要给自己挖坑跳就可以了，自我介绍完会有一个英文问答题目，然后就是老师提问时间了，我被问到的问题有：</p>
<ul>
<li>机器学习了解哪些算法；</li>
<li>逻辑回归和线性回归的区别是什么；</li>
<li>如果想要进王浩老师组（因为我提前联系的这个老师），想做什么？</li>
</ul>
<p>软件所面试大概就是这样了，接下来就可以回去等官网公示优秀营员了。这次的录取比例没有说的那么高，除国重实验室很多老师单独招生比较特殊外，所有实验室基本是50%的录取率。</p>
<p>需要提一下，天基里面只有王浩老师的研究组还不错，其他的话就千万不要考虑了！！！！</p>
</li>
</ul>
<h3 id="报名未能参加的夏令营"><a href="#报名未能参加的夏令营" class="headerlink" title="报名未能参加的夏令营"></a>报名未能参加的夏令营</h3><p>这一部分是我报名相应学校夏令营后，对方没有给予我入营资格或者由于某些原因我没有去的学校。</p>
<h4 id="1-清华计算机"><a href="#1-清华计算机" class="headerlink" title="1. 清华计算机"></a>1. 清华计算机</h4><ul>
<li>网址：<a href="http://www.cs.tsinghua.edu.cn/publish/cs/4723/2016/20160505085141209166544/20160505085141209166544_.html" target="_blank" rel="external">关于举办2016年暑期全国优秀大学生夏令营的预通知</a></li>
<li>时间：7月16日－7月18日</li>
<li>入营条件：我觉得基本只有专业第一可以去吧，除非你在科研竞赛特别优秀，否则基本没戏。注意这个只能是直博。</li>
<li>具体介绍：参考信息安全章博亨大神的日志：<a href="http://www.jianshu.com/p/89e6a9352cbc?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=reader_share&amp;utm_source=weixin&amp;from=timeline&amp;isappinstalled=0" target="_blank" rel="external">南大、清华、北大、上交、中科院、北航等高校夏令营保送经历</a></li>
</ul>
<h4 id="2-北大信科"><a href="#2-北大信科" class="headerlink" title="2. 北大信科"></a>2. 北大信科</h4><ul>
<li>网址：<a href="http://eecs.pku.edu.cn/index.aspx?menuid=4&amp;type=articleinfo&amp;lanmuid=116&amp;infoid=4242&amp;language=cn" target="_blank" rel="external">北京大学信息科学技术学院关于举办2016年信息学科优秀大学生夏令营的通知（第一轮）-北京大学信息科学技术学院</a></li>
<li>时间：7月13日-7月15日</li>
<li>入营条件：北大信科除了对绩点排名要求前5%外，还需要有很高的综合素质，所以成绩不是唯一因素，这次入营的软件工程专业的同学有两个，分别排名第2名和第11名。</li>
<li>具体介绍：参考章博亨大神的日志：<a href="http://www.jianshu.com/p/89e6a9352cbc?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=reader_share&amp;utm_source=weixin&amp;from=timeline&amp;isappinstalled=0" target="_blank" rel="external">南大、清华、北大、上交、中科院、北航等高校夏令营保送经历</a>，还有胡少晗大神的保研经历（目前还没有链接，有了的话我会更新）</li>
</ul>
<h4 id="3-上交计算机"><a href="#3-上交计算机" class="headerlink" title="3. 上交计算机"></a>3. 上交计算机</h4><ul>
<li>网址：上交计算机没有夏令营，只有一个直硕面试，这个是没有网址的，但是这个和上交软件夏令营是同时开始报名的，可以关注一下：<a href="http://yjwb.seiee.sjtu.edu.cn/yjwb/info/11369.htm" target="_blank" rel="external">关于上海交通大学“2017软件工程优才夏令营”的通知</a></li>
<li>时间：貌似是7月2号，挤不太清楚了，反正只有一天时间，可以当天去当天回。</li>
<li>入营条件：前面有提到，上交计算机只给一个专业一个直硕名额、一个直博名额，所以谁名次高，谁就有机会去（也不一定，还是报名试试吧）。</li>
<li>具体介绍：<br>对于我个人来说，我当初以为我能获得面试资格，就提前联系了几个老师，其中和<a href="http://www.cs.sjtu.edu.cn/PeopleDetail.aspx?id=82" target="_blank" rel="external">申瑞民</a>老师和<a href="http://www.cs.sjtu.edu.cn/PeopleDetail.aspx?id=63" target="_blank" rel="external">朱其立</a>老师分别进行了视频面试，他们都同意只要我获得上交直硕面试资格，参加考核，就收我作为他们的研究生，可惜我并没有获得直硕面试资格，所以比较遗憾。（朱其立老师英文名是Kenny Zhu，这个老师可能从国外回来的老师，所以他和我Skype的是时候全程英文交流，还是需要准备一下，具体考核方式可以参照章博亨大神的日志上交那部分：<a href="http://www.jianshu.com/p/89e6a9352cbc?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=reader_share&amp;utm_source=weixin&amp;from=timeline&amp;isappinstalled=0" target="_blank" rel="external">南大、清华、北大、上交、中科院、北航等高校夏令营保送经历</a>）</li>
</ul>
<h4 id="4-中科院计算所"><a href="#4-中科院计算所" class="headerlink" title="4. 中科院计算所"></a>4. 中科院计算所</h4><ul>
<li>网址：<a href="http://www.ict.cas.cn/shye/tzgg/201605/t20160513_4601562.html" target="_blank" rel="external">“计算未来”全国大学生计算技术暑期研修班招生简章</a></li>
<li>时间：7月17-7月23日</li>
<li>入营条件：之前提到，计算所不太喜欢收软件学院的学生，所以每个专业只给一两个名额，谁名次高谁就可以去</li>
<li>具体介绍：参考章博亨大神的日志：<a href="http://www.jianshu.com/p/89e6a9352cbc?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=reader_share&amp;utm_source=weixin&amp;from=timeline&amp;isappinstalled=0" target="_blank" rel="external">南大、清华、北大、上交、中科院、北航等高校夏令营保送经历</a></li>
</ul>
<h4 id="5-华科"><a href="#5-华科" class="headerlink" title="5. 华科"></a>5. 华科</h4><ul>
<li><p>网址：<a href="http://cs.hust.edu.cn/tongzhi/xq?id=10581&amp;type=50" target="_blank" rel="external">关于举办“2016年华中科技大学计算机学院优秀大学生夏令营”活动的通知</a></p>
</li>
<li><p>时间：7月21-7月22日</p>
</li>
<li><p>入营条件：985排名前15%，或者你在竞赛方面有突出表现+可以获得保研资格</p>
</li>
<li><p>具体介绍：</p>
<p>华科由于和软件所时间重了，所以我没有去，据说华科只有机试，3道题，一个半小时，人工判题，比较简单，985的同学去了基本可以拿到优秀营员，也是个不错的选择。</p>
</li>
</ul>
<h2 id="一些体会"><a href="#一些体会" class="headerlink" title="一些体会"></a>一些体会</h2><p>通过这次夏令营，有以下几个体会，也当做给学弟学妹的一些建议吧。</p>
<ul>
<li>保研不像想象中的那么容易。很多985院校的同学以为只要成绩好，保研到好学校很容易，其实并不是，因为有很多你不认识的人，他们比你学校好，成绩优异，科研竞赛经历丰富。所以成绩并不能代表一切，它最多只能是一块儿敲门砖，把你带到你想进的夏令营，但当你进入夏令营后，决定你水平的不单单是成绩，更重要的是综合素质，比如基础知识、编程能力、语言表达能力等种种因素。如何利用你三年学到的知识拿到优秀营员才是关键。不要总想着拿成绩说事，try to prove it!</li>
<li>抓住一切可能的机会。保研的路上，你可能觉得身心俱疲，可能觉得这个机会没什么价值，可能觉得这个机会哪有那么容易获得，如果是那么也不会是留给我的，我还是一遍歇着吧。但我想说的是，千万不要因为你的懒惰，你的想当然，让一个又一个机会从你手中溜走，因为任何事情只有尝试后，你才有资格评价，而且很多时候，这个机会就是为你准备的，你不去争取，你就一无所获。就像阿姆的《Lose yourself》歌词所说：Look, if you had one shot, or one opportunity to seize everything you ever wanted. In one moment, would you capture it, or just let it slip? 我想选择前者总是没有错的。</li>
<li>机会留是给有准备的人。通过这次保研，我感受最深的就是机试，真的是”得机试者得天下“。机试一直是我的薄弱之处，虽然在努力刷题提高机试水平，但是由于我没有参加过ACM，意识到的时间晚，没有时间去准备机试，所以短短的时间是无法有质的飞跃，导致我在机试上摔了一次又一次，”成功地“与很多到手的机会失之交臂，痛悔不已。所以我希望学弟学妹们一定要好好准备机试，没事多刷题（C/C++，千万不要用Java），到时候才会有临危不乱，秒杀众生的感觉，只要夏令营机试过了，你也就基本没问题了。当然，面试也是要好好准备的，经常复习四大专业课，高数、高代、概率论等数学课，这样到时候大概过一遍就行了。还有一年时间，争取多参加一些科研和竞赛，这是很加分的，如果你能通过这些”套到“一个好导师，何乐而不为呢！最后要说的是，尽早确定下来将来读研的研究方向，早一点去看一些相关专业书，争取利用剩下的一年时间跟老师做一做相关方向的科研，这会对你的简历增加不少光彩！So go for it!</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;7月23号从中科院软件所参加完夏令营回来，我的漫长的保研路也算是告一段落。&lt;/p&gt;
&lt;p&gt;8月12号东软实训结束，8月13号坐上回家的火车，8月14号到家，然后就一直吃喝睡到今天，拿回来的几本书也没看几眼，本来打算着回来继续充实一下，去备战9，10月份的推免，现在看来时间又都荒废了……开学还是乖乖到学校吧，再这样下去一直待在家感觉要成废人一个了，我还是喜欢忙碌充实的感觉。&lt;/p&gt;
&lt;p&gt;一直想着要把这次宝贵的保研经历记录一下，好给学弟学妹一个参考。学弟学妹们可以结合自身情况，大概了解一下保研流程，部分学校保研考核要求，从而少走一些弯路，去到自己理想中的学校。&lt;/p&gt;
    
    </summary>
    
      <category term="经验" scheme="http://yoursite.com/categories/%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="保研" scheme="http://yoursite.com/tags/%E4%BF%9D%E7%A0%94/"/>
    
  </entry>
  
</feed>
