<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Weijie Kong&#39;s Homepage</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://jacobkong.github.io/"/>
  <updated>2019-05-18T10:20:42.038Z</updated>
  <id>http://jacobkong.github.io/</id>
  
  <author>
    <name>Weijie Kong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文笔记：A Comprehensive Survey on Graph Neural Networks</title>
    <link href="http://jacobkong.github.io/blog/4197138744/"/>
    <id>http://jacobkong.github.io/blog/4197138744/</id>
    <published>2019-05-18T14:51:24.000Z</published>
    <updated>2019-05-18T10:20:42.038Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li><h3 id="GNN的发展"><a href="#GNN的发展" class="headerlink" title="GNN的发展"></a>GNN的发展</h3><ul>
<li>Spectral graph theory: The ﬁrst prominent research on GCNs is presented in Bruna et al. (2013), which develops a variant of graph convolution based on spectral graph theory<ul>
<li>Since that time, there have been increasing improvements, extensions, and approximations on spectral-based graph convolutional networks</li>
</ul>
</li>
<li>Spatial-based graph convolutional networks: As spectral methods usually handle the whole graph simultaneously and are difﬁcult to parallel or scale to large graphs, spatial-based graph convolutional networks have rapidly developed recently<ul>
<li>Together with sampling strategies, the computation can be performed in a batch of nodes instead of the whole graph [24], [27], which has the potential to improve the efﬁciency.</li>
</ul>
</li>
<li>Others: In addition to graph convolutional networks, many alternative graph neural networks have been developed in the past few years.<ul>
<li>These approaches include graph attention networks, graph autoencoders, graph generative networks, and graph spatial-temporal networks.</li>
</ul>
</li>
</ul>
</li>
</ul>
<a id="more"></a>
<ul>
<li><h3 id="GNN-vs-Network-embedding"><a href="#GNN-vs-Network-embedding" class="headerlink" title="GNN vs Network embedding"></a>GNN vs Network embedding</h3><ul>
<li>两者属于相交的关系，交集是Deep learning</li>
<li>Network embedding aims to represent network vertices into a low-dimensional vector space, by preserving both network topology structure and node content information, so that any subsequent graph analytics tasks such as classiﬁcation, clustering, and recommendation can be easily performed by using simple off-the-shelf learning machine algorithm</li>
<li>Many network embedding algorithms are typically unsupervised algorithms and they can be broadly classiﬁed into three groups [32]<ul>
<li>matrix factorization [38], [39]</li>
<li>random walks [40]</li>
<li>deep learning approaches</li>
</ul>
</li>
<li>The deep learning approaches for network embedding at the same time belong to graph neural networks, which include graph autoencoder-based algorithms (e.g., DNGR [41] and SDNE [42]) and graph convolution neural networks with unsupervised training(e.g., GraphSage [24]).</li>
<li><img src="http://ww3.sinaimg.cn/large/006tNc79gy1g35m24yir6j30dd08gdh4.jpg" alt="image-20190518104429873"></li>
</ul>
</li>
<li><p>GNN分类，5类</p>
<ul>
<li><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g35m2qa2bwj30c90av408.jpg" alt="image-20190518104510256"></li>
</ul>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;h3 id=&quot;GNN的发展&quot;&gt;&lt;a href=&quot;#GNN的发展&quot; class=&quot;headerlink&quot; title=&quot;GNN的发展&quot;&gt;&lt;/a&gt;GNN的发展&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Spectral graph theory: The ﬁrst prominent research on GCNs is presented in Bruna et al. (2013), which develops a variant of graph convolution based on spectral graph theory&lt;ul&gt;
&lt;li&gt;Since that time, there have been increasing improvements, extensions, and approximations on spectral-based graph convolutional networks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Spatial-based graph convolutional networks: As spectral methods usually handle the whole graph simultaneously and are difﬁcult to parallel or scale to large graphs, spatial-based graph convolutional networks have rapidly developed recently&lt;ul&gt;
&lt;li&gt;Together with sampling strategies, the computation can be performed in a batch of nodes instead of the whole graph [24], [27], which has the potential to improve the efﬁciency.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Others: In addition to graph convolutional networks, many alternative graph neural networks have been developed in the past few years.&lt;ul&gt;
&lt;li&gt;These approaches include graph attention networks, graph autoencoders, graph generative networks, and graph spatial-temporal networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Graph Neural Networks" scheme="http://jacobkong.github.io/blog/tags/Graph-Neural-Networks/"/>
    
      <category term="图神经网络" scheme="http://jacobkong.github.io/blog/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：行为预测(Action Prediction / Anticipation)相关论文略读笔记</title>
    <link href="http://jacobkong.github.io/blog/1352025799/"/>
    <id>http://jacobkong.github.io/blog/1352025799/</id>
    <published>2018-07-20T07:51:24.000Z</published>
    <updated>2019-02-02T13:27:12.890Z</updated>
    
    <content type="html"><![CDATA[<h2 id="论文一：Part-Activated-Deep-Reinforcement-Learning-for-Action-Prediction"><a href="#论文一：Part-Activated-Deep-Reinforcement-Learning-for-Action-Prediction" class="headerlink" title="论文一：Part-Activated Deep Reinforcement Learning for Action Prediction"></a>论文一：Part-Activated Deep Reinforcement Learning for Action Prediction</h2><p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fxntax0ywlj30tn0beqa8.jpg" alt=""></p>
<p>现有的许多行为预测的方法会用到整个帧的演化来对动作建模，这不能避免当前动作所带来的噪声，特别是在早期预测中。为了解决这个问题，我们设计了PA-DRL，通过在深层强化学习框架下提取骨架proposal来开发人体结构。具体而言，我们从人体的不同part单独提取特征，并激活特征中与动作相关的部分以增强表征。 我们的方法不仅利用了人体的结构信息，而且还考虑了表达动作的显着部分。 我们在三个流行的动作预测数据集上评估我们的方法：UT-Interaction，BIT-Interaction和UCF101。 我们的实验结果表明，我们的方法通过最先进的技术实现了性能。</p>
<a id="more"></a>
<h2 id="Temporal-Recurrent-Networks-for-Online-Action-Detection"><a href="#Temporal-Recurrent-Networks-for-Online-Action-Detection" class="headerlink" title="Temporal Recurrent Networks for Online Action Detection"></a>Temporal Recurrent Networks for Online Action Detection</h2><p>以前的预测方法仅根据历史信息来进行预判，而不利用未来信息</p>
<p>动机：1）与仅仅关注过去相比，联合建模当前行动识别和未来在训练中的行动预期将迫使网络学习更具辨别力的表示；2）明确预测将来会发生什么作为额外时间背景的来源，将有助于在测试时对当前行动进行分类。实验也有验证这一点，方法架构图如下：</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fxopqyz1r3j30tg0aojwh.jpg" alt=""></p>
<p>主要是重新设计了一个RNNcell，这样在原本RNN的基础上额外加入了未来信息来达到目的。</p>
<h2 id="Action-Prediction-from-Videos-via-Memorizing-Hard-to-Predict-Samples"><a href="#Action-Prediction-from-Videos-via-Memorizing-Hard-to-Predict-Samples" class="headerlink" title="Action Prediction from Videos via Memorizing Hard-to-Predict Samples"></a>Action Prediction from Videos via Memorizing Hard-to-Predict Samples</h2><p>这是AAAI-2018的一篇关于行为预测的文章。文章有提出行为预测的一个困难点在于：在某些动作中，由于视觉相似性，开始的几帧特征是不具有足够的辨别力以进行准确分类。如何解决这个问题对于行为预测的性能也是至关重要的，这样可以帮助分类器尽早的对行为进行分类，虽然最近的研究表明，当观察到一半长度的视频时，预测性能通常就会变得稳定，但还是有必要越早发现具有判别性的特征越好。因此本文提出了加入Memory机制，从而在训练阶段记住难以预测的训练样本。</p>
<p>为了帮助更好的预测，本工作使用了未来的信息。在实现上是采用具有前向连接和后向连接的双层双向LSTM来表征时间动作演变并捕获用于预测的未来信息。这一点和Bidirectional Attentive Fusion With Context Gating for Dense Video Captioning这篇论文的想法有点像。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fxpx4wuva0j30ki08djuf.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;论文一：Part-Activated-Deep-Reinforcement-Learning-for-Action-Prediction&quot;&gt;&lt;a href=&quot;#论文一：Part-Activated-Deep-Reinforcement-Learning-for-Action-Prediction&quot; class=&quot;headerlink&quot; title=&quot;论文一：Part-Activated Deep Reinforcement Learning for Action Prediction&quot;&gt;&lt;/a&gt;论文一：Part-Activated Deep Reinforcement Learning for Action Prediction&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNbRwgy1fxntax0ywlj30tn0beqa8.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;现有的许多行为预测的方法会用到整个帧的演化来对动作建模，这不能避免当前动作所带来的噪声，特别是在早期预测中。为了解决这个问题，我们设计了PA-DRL，通过在深层强化学习框架下提取骨架proposal来开发人体结构。具体而言，我们从人体的不同part单独提取特征，并激活特征中与动作相关的部分以增强表征。 我们的方法不仅利用了人体的结构信息，而且还考虑了表达动作的显着部分。 我们在三个流行的动作预测数据集上评估我们的方法：UT-Interaction，BIT-Interaction和UCF101。 我们的实验结果表明，我们的方法通过最先进的技术实现了性能。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="行为识别" scheme="http://jacobkong.github.io/blog/tags/%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB/"/>
    
      <category term="ECCV 2018" scheme="http://jacobkong.github.io/blog/tags/ECCV-2018/"/>
    
      <category term="Action Prediction" scheme="http://jacobkong.github.io/blog/tags/Action-Prediction/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：CVPR 2018 关于行为识别论文略读笔记（二）</title>
    <link href="http://jacobkong.github.io/blog/2879360315/"/>
    <id>http://jacobkong.github.io/blog/2879360315/</id>
    <published>2018-06-15T07:51:24.000Z</published>
    <updated>2018-07-21T08:10:29.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="论文五：PoTion-Pose-MoTion-Representation-for-Action-Recognition"><a href="#论文五：PoTion-Pose-MoTion-Representation-for-Action-Recognition" class="headerlink" title="论文五：PoTion: Pose MoTion Representation for Action Recognition"></a>论文五：PoTion: Pose MoTion Representation for Action Recognition</h2><p>和上面两篇论文类似，这篇文章主要是利用人体关键点（Keypoint）来做行为识别。目前的许多方法主要是双流网络来分别处理外观（appearance）和动态（motion）。在本篇文章中，作者引入了一种新颖的表示方式，可以优雅地编码某些语义关键点的移动。我们使用人体关节作为这些关键点，编码后的维度固定的特征称为：PoTion，将该特征图输送到简单的CNN中即可用用来行为识别分类。方法框架图如下：</p>
<a id="more"></a>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fsfn0dgjr7j30qr0oin4e.jpg" alt=""></p>
<p>方法大致流程为：首先在每个帧中运行目前最先进的人体姿态估计器，并为每个人体关节获取热图。<strong>这些热图对每个像素的概率进行编码以包含特定的关节</strong>。<strong>我们使用取决于视频片段帧的相对时间的颜色对这些热度图进行着色</strong>。如下图所示的为不同通道下的随时间的上色机制：</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fsfn7ji98pj30hf0ah0v5.jpg" alt=""></p>
<p>对于每个关节，我们对所有帧上的彩色热图进行求和，以获得整个视频片段的PoTion表示。如下图所示为某一关节点聚合之后的色彩图，使用了不同的聚合方式：</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fsfn6vdgtfj310k08rwjw.jpg" alt=""></p>
<p>给定这种表示形式，我们训练一个浅层CNN架构，包含6个卷积层和一个完全连接的层来执行动作分类，CNN结构如下：</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fsfn9ov2p1j30hj08cgnn.jpg" alt="image-20180618211827050"></p>
<p>整个这个网络可以从头开始训练，并胜过其他姿势表示商法。而且，由于网络很浅并且以整个视频clip的紧凑表示为输入，因此训练例如非常快速。在一台用于HMDB的GPU上只需要4个小时，而标准的双流方法则需要几天的培训和仔细的初始化[5,43]。另外，PoTion可以看做是标准外观和运动流的补充。与RGB和光学流程的I3D [5]结合使用时，我们在JHMDB，HMDB，UCF101上获得了最先进的性能。</p>
<h2 id="论文六：Im2Flow-Motion-Hallucination-from-Static-Images-for-Action-Recognition"><a href="#论文六：Im2Flow-Motion-Hallucination-from-Static-Images-for-Action-Recognition" class="headerlink" title="论文六：Im2Flow: Motion Hallucination from Static Images for Action Recognition"></a>论文六：Im2Flow: Motion Hallucination from Static Images for Action Recognition</h2><p>这是今年CVPR 2018中在<strong>静态图像</strong>中做行为识别的一篇文章。静态图像动作识别需要系统识别发生在单张照片中的行为。 该问题对于基于人类行为和事件组织照片集合（例如，在网络，社交媒体上的照片）具有实际意义。现有的一些方法都仅仅依据图像的表象特征——物体、场景和肢体姿势来区分单张静态图像中的动作，但是这样的方法会忽略图像中所包含的丰富的动态结构和动作。为了挖掘单张图片所包含的motion信息，本文提出了一种为单张图片产生类似光流的图像，表示图像中所蕴含的未来的可能的motion。 其中一个<strong>关键的想法是</strong>：从数千个未标记的视频中学习一个先验的短期动态，一次来在新的静态图像上推断的预期光流，然后训练利用RGB流和光流来进行的动作的识别。框架图如下：</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fs8h7m3emnj30nb0o7gzt.jpg" alt=""></p>
<p><strong>方法大致流程为：</strong>首先通过观察上千个包含各种动作的未标注的视频中学习一个动作的先验知识（motion prior）；然后利用下图中的Image-to-image translation模型，去将一个RGB图像转化为上图右边类似的光流图，改光流图是3通道的，前两个通道是motion angle $\theta\in[0,2\pi]$，第三个通道是幅度M。最后通过RGB图像和得到的光流图像去共同进行行为识别。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fs8hm9zxzsj313e09gwjz.jpg" alt=""></p>
<p>本方法很重要的一个创新点：结合了GAN中的Image-to-image translation模型来仅仅为静态图像即可生成相应的光流图，所预测出的光流是十分准确的，同时也提升了行为识别的准确性，思想值得借鉴。</p>
<h2 id="论文七：Compressed-Video-Action-Recognition"><a href="#论文七：Compressed-Video-Action-Recognition" class="headerlink" title="论文七：Compressed Video Action Recognition"></a>论文七：Compressed Video Action Recognition</h2><p>这是今年CVPR 2018中做行为识别的另一篇文章。本文很重要的一个创新点是：利用压缩视频作为输入来进行视频中的行为识别。原始的视频帧数据往往具有巨大的尺寸而且高时间冗余，关于动作的有用信息很容易淹没在许多不相关的背景数据中。利用压缩后的视频具有许多好处：首先由于视频压缩（使用H.264，HEVC等）可将原始视频信息量降低两个数量级；其次视频中压缩中的motion vector提供了额外的motion信息，这是RGB图像所不具备的；而且压缩视频排除了空间可变性，这样提高了模型的泛化性；最后压缩视频模型会更快更简单更准确；因此本文建议直接在压缩视频上训练深层网络。</p>
<p>大多数现代编解码器将视频分成I帧（内编码帧），P帧（预测帧）和零个或多个B帧（双向帧）。I帧是常规图像并且被压缩。P帧引用前面的帧并仅编码相对前一帧所需要的‘变化’。B帧可以被视为特殊的P帧，其中运动向量是双向计算的，并且只要在参考中没有循环就可以引用未来帧。</p>
<p>本文工作将视频压缩后得到的用于描述两帧之间变化的motion vector、residual帧图像（如下图所示）作为各种SOTA网络中输入进行行为识别，都得到了很明显的提升，而且更快速、更简单、更准确。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fs8ps5lpmpj30nk0cv0xn.jpg" alt=""></p>
<h2 id="论文八：What-have-we-learned-from-deep-representations-for-action-recognition"><a href="#论文八：What-have-we-learned-from-deep-representations-for-action-recognition" class="headerlink" title="论文八：What have we learned from deep representations for action recognition?"></a>论文八：What have we learned from deep representations for action recognition?</h2><p>这是今年CVPR 2018中对行为识别任务时空特征研究的一篇探究性文章。这篇文章通过可视化双流网络具体所学习到的时空特征来探究视频中行为识别这一任务。我们展示了用于外观和运动物体的局部检测器，以形成用于识别人类行为的分布式表示。</p>
<p>目前的可视化方法有三类：</p>
<ul>
<li>Visualization for given inputs.</li>
<li>Activation maximization.</li>
<li>Generative Adversarial Networks (GANs).</li>
</ul>
<p>本文的可视化方法是基于activation maximization并将它们扩展到时空域，以便在双流融合模型中找到各个单元的首选时空输入，我们将问题表述为在输入空间中搜索的（正则化的）<strong>基于梯度的优化问题</strong>，方法框架如下图所示。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1ftgf47w7i3j30te0d8diu.jpg" alt=""></p>
<p>方法流程为：随机初始化的输入呈现给我们模型的光流和外观路径。 我们计算特征图一直到我们想要可视化的特定图层。 选择单个目标特征通道c，并且执行激活最大化（activation maximization）以分两步产生优选输入（preferred input）。 首先，影响c的输入上的导数是通过将目标损耗（在所有位置上求和）反向计算到输入层来计算的。 其次，通过学习速率缩放传播的梯度并将其添加到当前输入。 这些操作由图2中的虚线红线说明。基于梯度的优化以自适应降低的学习速率迭代地执行这些步骤，直到输入收敛为止。 重要的是，在此优化过程中，网络权重不会改变，<strong>只有输入接收更改</strong>。 </p>
<p>主要的结论有如下四点：</p>
<ul>
<li>第一：相比于分开单独学习外观以及运动特征，cross-stream可以学习到真正的时空特征</li>
<li>第二：网络可以学习高度类别已知的本地表示，也可以学习适用于一系列类的通用表示。</li>
<li>第三：通过网络结构，特征变得更加抽象并且对于对获得期望的分布不重要的数据（比如不同速度间的motion模式）显示出逐渐增加的不变性。</li>
<li>第四：可视化不仅可用于揭示学习的表示，还可用于揭示训练数据的特性并解释系统的失败情况。</li>
</ul>
<h2 id="论文八：Non-local-Neural-Networks"><a href="#论文八：Non-local-Neural-Networks" class="headerlink" title="论文八：Non-local Neural Networks"></a>论文八：Non-local Neural Networks</h2><p>这是今年CVPR 2018中做利用Non-local方法去做行为识别的一篇文章。long-range的依赖捕捉是深度学习的核心问题，对于序列数据，recurrent操作是主要的方法，对于图像数据，long-range依赖通过多层的卷积形成的感受野来捕捉。但卷积和recurrent操作都是local的操作，在本文中，受启发于计算机视觉的经典non-local均值法，作者提出了进行non-local操作的通用构建块系列，来捕获long-range依赖性的。本文中的non-local操作将所有位置处的特征的加权和作为某一位置的响应。直观一点，如下图所示：</p>
<p><img src="../../../../../../var/folders/2c/k786p2fd5mb4jp5l3cmd7m3w0000gp/T/abnerworks.Typora/image-20180721152747380.png" alt="image-20180721152747380"></p>
<p>使用非本地操作有几个优点：</p>
<ul>
<li>（a）与循环和卷积操作的渐进行为相反，非本地操作通过计算任意两个位置之间的相互作用直接捕获长程依赖性，而不管它们的位置距离如何; </li>
<li>（b）正如我们在实验中所表明的那样，非局部操作是有效的，即使只有几层（例如5）也能达到最佳效果; </li>
<li>（c）最后，我们的非本地操作保持可变输入大小，并且可以很容易地与其他操作组合（例如，我们将使用的卷积）。</li>
</ul>
<p>该non-local构建块可以集成到目前许多计算机视觉框架中。通过在视频分类任务上进行验证，我们的non-local模型也可以在Kinetics和Charades数据集上取得很有竞争力的结果；在静态图像识别中，我们的non-local模型提高了在COCO数据集上的对象检测/分割和姿态估计任务，证明了模型的有效性。</p>
<p>​       </p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;论文五：PoTion-Pose-MoTion-Representation-for-Action-Recognition&quot;&gt;&lt;a href=&quot;#论文五：PoTion-Pose-MoTion-Representation-for-Action-Recognition&quot; class=&quot;headerlink&quot; title=&quot;论文五：PoTion: Pose MoTion Representation for Action Recognition&quot;&gt;&lt;/a&gt;论文五：PoTion: Pose MoTion Representation for Action Recognition&lt;/h2&gt;&lt;p&gt;和上面两篇论文类似，这篇文章主要是利用人体关键点（Keypoint）来做行为识别。目前的许多方法主要是双流网络来分别处理外观（appearance）和动态（motion）。在本篇文章中，作者引入了一种新颖的表示方式，可以优雅地编码某些语义关键点的移动。我们使用人体关节作为这些关键点，编码后的维度固定的特征称为：PoTion，将该特征图输送到简单的CNN中即可用用来行为识别分类。方法框架图如下：&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Action Recognition" scheme="http://jacobkong.github.io/blog/tags/Action-Recognition/"/>
    
      <category term="行为识别" scheme="http://jacobkong.github.io/blog/tags/%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB/"/>
    
      <category term="CVPR 2018" scheme="http://jacobkong.github.io/blog/tags/CVPR-2018/"/>
    
  </entry>
  
  <entry>
    <title>行为识别论文笔记：Something about Temporal Reasoning</title>
    <link href="http://jacobkong.github.io/blog/3309988052/"/>
    <id>http://jacobkong.github.io/blog/3309988052/</id>
    <published>2018-06-10T07:51:24.000Z</published>
    <updated>2018-07-20T07:55:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>在视频的行为识别中，影响性能很重要的一点：就是模型能否提取出强有力的时间信息。虽然有的行为光从单张图像的空间特征就能大概判断出其中所包含的动作是什么，但是还是有很多动作需要从其随时间的变化才能准确判断出来。最近看了几篇关于视频中时间推理（Temproal Reasioning）的文章，这里顺便整理一下。</p>
<a id="more"></a>
<h2 id="论文一：Temporal-Relational-Reasoning-in-Videos"><a href="#论文一：Temporal-Relational-Reasoning-in-Videos" class="headerlink" title="论文一：Temporal Relational Reasoning in Videos"></a>论文一：Temporal Relational Reasoning in Videos</h2><p><strong>时间关系推理</strong>是指连接物体或实体随时间有意义变化的能力。受启发于Relation Network，本文提出了一个时间关系网络（Temporal Relation Network ，TRN），用来在多时间尺度上学习并推理视频帧之间的时间依赖关系，该网络可以思想很简单，可以很容易的集成到现有的卷积神经网络中。</p>
<p>首先本文定义了时间关系（Temporal Relations）。给定一段视频V，选取n个有序的视频帧序列${f_1,f_2,…,f_n}$，则第$f_i$和$f_j$帧的关系定义如下：</p>
<script type="math/tex; mode=display">
T_2(V)=h_\delta(\sum_{i<j}g_\theta(f_i,f_j))</script><p>其中函数$h<em>\delta$和$g</em>\theta$用于融合不同帧的特征，这里使用了简单的多层感知机（MLP）来实现。这里i和j的选取是随机的。将该2-frame的关系概念可以拓展到3-frame或者更多。</p>
<p>通过将不同时间维度的关系特征融合到一起，即可躲到多时间维度的时间关系，如下：</p>
<script type="math/tex; mode=display">
MT_N(V)=T_2(V)+T_3(V)+...+T_N(V)</script><p>最后利用如下网络结构图进行训练和测试：</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fsbnbv7b24j30yi0f1qfx.jpg" alt="image-20180615101817238"></p>
<p>输入一段视频，随机选2帧一组、3帧一组、4帧一组分别提取CNN特征，然后进行最后简单的相加融合，得到最终的检测结果</p>
<p>目前可以用来做时间关系推理的数据集有Something-Something, Jester, 和Charades。其中Something-Something是最近提出的数据集，主要是用于人-物体交互动作识别，里面有174类，比如：“Tearing Something into two pieces”、‘Tearing Something just a little bit’、‘Turn something upside down’以及 ‘Pretending to turn something upside down’。对于这些类别的识别需要很好的时间推理能力，单独通过空间特征时很难准确识别的。本文提出的方法在三个数据集上都能很好的发现视频中的时间关系，超过了许多其他原本做行为识别很好的方法，说明了本文方法的有效性。</p>
<h2 id="论文二：A-Closer-Look-at-Spatiotemporal-Convolutions-for-Action-Recognition"><a href="#论文二：A-Closer-Look-at-Spatiotemporal-Convolutions-for-Action-Recognition" class="headerlink" title="论文二：A Closer Look at Spatiotemporal Convolutions for Action Recognition"></a>论文二：A Closer Look at Spatiotemporal Convolutions for Action Recognition</h2><p>这篇文章是CVPR 2018中关于行为识别的一篇文章。主要是利用2D和3D卷积层设计了不同的组合结构来进行视频中时间推理的研究。具体作者对比并设计了如下卷积残差块，所有如下结构都是残差网络结构，因此忽略掉了残差：</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fsbnaih331j31030gigqj.jpg" alt="image-20180615101658111"></p>
<ul>
<li><p>R2D：在整个clip上运用2D卷积</p>
<p>2D卷积忽略了视频中的时间顺序，将一个clip中的L帧类比于通道数。R2D网络中的第一个卷积层就已经将视频的整个时间维度信息折叠，这阻止了后续层中的任何时间推理。</p>
</li>
<li><p>f-R2D：在frames上运用2D卷积</p>
<p>该网络结构将每个frame单独处理，对每个frame都运用同样的2D卷积核，而非将L个frame当成输入的通道数。在这种情况下，在卷积层中不进行时间建模，并且顶部的全局时空池化层简单地融合了L帧单独提取的信息。 因此将此架构变体称为f-R2D（基于帧的R2D）。</p>
</li>
<li><p>R3D：3D卷积</p>
<p>3D CNN 保留了时间信息，并通过网络层传播。 张量$z<em>i$在这种情况下是4D的，并且具有尺寸$N_i×L×H_i×W_i$，其中$N i$是在第i个块中使用的滤波器的数量。 每个滤波器都是四维的，它的大小为$N</em>{i-1}×t×d×d$，其中t表示滤波器的时间尺度的范围（在本文中，使用t = 3）。 因为t &lt;L，所以过滤器现在可以在3D中进行卷积，即在时间和空间维度上进行卷积。 图1(d)说明了这种CNN体系结构。</p>
</li>
<li><p>MCx和rMCx：混合3D-2D卷积</p>
<p>本文的一个假设是运动(motion)或时间（temporal）建模（即3D卷积）可能对早期层特别有用，而在更高水平的语义抽象（后期层）中，运动或时间建模则不是必需的。<strong>因此，合理的体系结构可以从3D卷积开始并切换到在顶层使用2D卷积。</strong></p>
<p>由于在这项工作中本文考虑了具有5组卷积的3D ResNets（R3D）结构，我们的第一个变体是用2D卷积代替第5组中的所有3D卷积。我们用MC5（混合卷积）表示这个变体。我们设计了第二个变体，替换第4组和第5组中的3D卷积为2D卷积，并将该模型命名为MC4（意思是来自第4组，更深层的所有卷积都是2D）。遵循这种模式，我们还创建了MC3和MC2变体。我们忽略考虑MC1，因为它相当于fR2D。图1(b)说明了这种CNN体系结构。</p>
<p>另一种假设是，时间建模可能在深层更有利，因为可以通过2D卷积提前捕获外观信息。为了说明这种可能性，我们还尝试了“反转”混合卷积。按照MC模型的命名约定，我们将这些模型表示为rMC2，rMC3，rMC4和rMC5。因此，rMC3将包括块1和块2中的2D卷积，以及组3和更深组中的3D卷积。图1(c)说明了这种CNN体系结构。</p>
</li>
<li><p>R(2+1)D：(2+1)D卷积</p>
<p>另一种可能的理论是全3D卷积可以通过2D卷积+1D卷积来更方便地近似，将空间和时间建模分解成两个单独的步骤。 因此，本文设计了一个名为R(2 + 1)D的网络体系结构，其中我们用一个由$M<em>i$个尺度为$N</em>{i-1}×1×d×d$ 的2D卷积核和$N<em>i$个尺度为$M_i×t×1×1$的时间卷积滤波器组成的(2 + 1)D块代替了$N_i$个大小为$N</em>{i-1}×t×d×d$的三维卷积滤波器。超参数$M_i$确定信号在空间中投影的中间子空间的维数和时间卷积，具体如下。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fsbvlm0snzj30hn0imjv3.jpg" alt=""></p>
<p>相比于3D卷积，(2+1)D分解有两个好处：</p>
<ul>
<li>第一：尽管没有改变参数的数量，但由于每个块中的2D和3D卷积之间的额外的ReLU，所以它使网络中非线性映射的数量增加了一倍。<strong>增加非线性的数量增加了可以表示的函数的复杂性，也如VGG网络[27]中指出的那样，通过应用多个更小的滤波器以及其间的附加非线性来逼近大滤波器的影响</strong>。</li>
<li>第二：将3D卷积强制划分为单独的空间和时间分量使得优化更容易。与具有相同容量的3D卷积网络相比，<strong>这表现在较低的训练误差上。</strong>对于相同数量的层（和参数），与R3D相比，R(2+1)D不仅产生更低的测试误差，而且还产生更低的训练误差。这表明当时空滤波器因式分解时优化变得更容易。<strong>这表明优化的便利性随着深度变大而增加。</strong></li>
</ul>
</li>
</ul>
<p>本文我们提出的体系结构R(2+1)D实现了在Sports-1M，Kinetics，UCF101和HMDB51数据集上技术水平相当或更好的结果。希望能够激发新的网络设计，以利用时空卷积潜在的效能和建模的灵活性。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在视频的行为识别中，影响性能很重要的一点：就是模型能否提取出强有力的时间信息。虽然有的行为光从单张图像的空间特征就能大概判断出其中所包含的动作是什么，但是还是有很多动作需要从其随时间的变化才能准确判断出来。最近看了几篇关于视频中时间推理（Temproal Reasioning）的文章，这里顺便整理一下。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Action Recognition" scheme="http://jacobkong.github.io/blog/tags/Action-Recognition/"/>
    
      <category term="行为识别" scheme="http://jacobkong.github.io/blog/tags/%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：CVPR 2018 关于行为识别论文略读笔记（一）</title>
    <link href="http://jacobkong.github.io/blog/3799204522/"/>
    <id>http://jacobkong.github.io/blog/3799204522/</id>
    <published>2018-06-02T07:51:24.000Z</published>
    <updated>2018-07-21T07:45:17.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="论文一：Optical-Flow-Guided-Feature-A-Fast-and-Robust-Motion-Representation-for-Video-Action-Recognition"><a href="#论文一：Optical-Flow-Guided-Feature-A-Fast-and-Robust-Motion-Representation-for-Video-Action-Recognition" class="headerlink" title="论文一：Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition"></a>论文一：Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition</h2><p>这是今年CVPR 2018中做行为识别的一篇文章，提出了一个叫做光流引导的特征（Optical Flow guided Feature，OFF）。时间信息是视频行为识别的关键，二光流可以很好的表征时间信息，其在视频分析领域已经被很多工作证明是一个很有用的特征。但是目前的双流网络Two-Stream在训练时其实还是比较麻烦的，因为需要单独对视频提取光流图，然后送到网络的另一至进行训练；而且如果数据集很大的话，光流图和RGB图像合起来得有原视频数据大小的好几倍，也十分消耗硬盘空间。因此思考如何利用单流网络同时利用RGB特征以及类似光流的特征去进行训练是一个值得思考的问题。本文从光流本身的定义出发，给了我们一个关于该问题很好的启发。该方法也在UCF-101逮到了96%的分类准确率，超过了不用Kinetics数据集预训练的I3D模型，可见该方法的有效性。</p>
<a id="more"></a>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fseagxexxdj30vx0pyn82.jpg" alt=""></p>
<p>本文提出的光流引导特征（OFF），它使网络能够通过快速和稳健的方法提取时间信息。 OFF由光流的定义导出，并与光流正交。该特征由水平和垂直方向上的特征图的空间梯度以及从不同帧的特征图之间的差异获得的时间梯度组成，OFF操作是CNN特征上的像素级运算，而且所有操作都是可导的，因此整个过程是可以端到端训练的，而且可以应用到仅有RGB输入的网络中去同时有效提取空间和时间特征。</p>
<h2 id="论文二：Recognize-Actions-by-Disentangling-Components-of-Dynamics"><a href="#论文二：Recognize-Actions-by-Disentangling-Components-of-Dynamics" class="headerlink" title="论文二：Recognize Actions by Disentangling Components of Dynamics"></a>论文二：Recognize Actions by Disentangling Components of Dynamics</h2><p>这是今年CVPR 2018中做行为识别的另一篇文章。<strong>本文和第一篇论文的中心思想相似：都是想通过原始的RGB图像直接在网络中间接获得类似光流的特征，从而减少目前双流网络中计算光流模块导致的额外开销。</strong>因此本文提出了一个新的用于视频表征学习的ConvNet框架，其可以完全从原始视频帧中推导出动态信息，而不需进行额外的光流估计。具体网络框架如下：</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fsea15nqunj315o0m3ah8.jpg" alt=""></p>
<p>大致流程为：给定一个连续的帧序列，该模型首先产生一些低级特征映射，然后将其馈入三个分支，分别是静态外观（Static Appearance，上），外观动态（Apparent Motion，中）和外观变化（Appearance Change，下）。 这些分支分别计算其对应的高级特征并进行预测。 最后，这些预测被合并为最终的预测。最后，3个组件预测出的结果将通过求平均的方式融合到一起生成最终的预测。</p>
<p>其中在静态外观分支，通过迭代地应用2D卷积，空间2D池化和时间1D池化来逐渐提取外观特征；在外观动态分支，主要提取视频帧中特征点的空间位移，主要第一次引入了Cost Volume来进行外观动态的估计；在外观变化分支中，由于不是所有的变化都能够通过外观动态表解释，诸如物体外观的固有变化或照明变化的其他因素也可能导致视频帧的变化，不同于以前使用RGB-diff的方法，本文提出了一个叫做warped differences的方法来表征外观变化。</p>
<p>通过在UCF101和Kinetics两个数据集上进行验证，本文的方法在仅使用RGB图像帧的前提下也能取得很有竞争力的结果，而且具有很高的效率，证明了方法的优越性和有效性。</p>
<h2 id="论文三：2D-3D-Pose-Estimation-and-Action-Recognition-using-Multitask-Deep-Learning"><a href="#论文三：2D-3D-Pose-Estimation-and-Action-Recognition-using-Multitask-Deep-Learning" class="headerlink" title="论文三：2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning"></a>论文三：2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning</h2><p>这是今年CVPR 2018中利用姿态做行为识别的一篇文章，主要突出了一个多任务网络来同时做2D和3D的姿态估计以及2D和3D的行为识别，同时利用姿态估计的结果来促进行为识别任务的性能。<strong>这也是解决问题的一个很好的出发点，就是利用两个任务来互相促进</strong>。</p>
<p>下图是网络的整体框架图，输入静态的RGB图像，同时进行姿态估计和行为识别。其中的姿态估计模型是利用基于回归的方法，其中利用了一个可微分的Softargmax来联合2D和3D的姿态估计。其中的动作识别方法分为两部分，一部分基于身体关节坐标序列，我们称之为<strong>基于姿态的识别</strong>，另一部分基于一系列视觉特征，我们称其为<strong>基于外观的识别</strong>。 将每个部分的结果组合起来估计最终的动作标签。</p>
<p>作者在MPII, Human3.6M, Penn Action 和 NTU四个数据集上进行了实验，验证了模型在两个任务上的有效性。</p>
<p>本文值得借鉴的一个思想就是：利用多任务之间的互相促进，来提升各自任务的有效性。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fs44dyxu0aj30kl0migsy.jpg" alt=""></p>
<h2 id="论文四：Deep-Progressive-Reinforcement-Learning-for-Skeleton-based-Action-Recognition"><a href="#论文四：Deep-Progressive-Reinforcement-Learning-for-Skeleton-based-Action-Recognition" class="headerlink" title="论文四：Deep Progressive Reinforcement Learning for Skeleton-based Action Recognition"></a>论文四：Deep Progressive Reinforcement Learning for Skeleton-based Action Recognition</h2><p>这是今年CVPR 2018中基于骨架（Skeleton-based）来做行为识别的一篇文章，但是一个重要的创新点是利用增强学习首先找到一段视频帧中最具动作代表性的帧，丢弃掉序列中的不明确帧，然后利用基于图的神经网络来捕捉关节连接点之间的依赖关系，从而达到行为识别的目的。框架图如下：</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fs8h8br7cvj30l10qitfh.jpg" alt=""></p>
<p><strong>方法大致流程为：</strong>给定一个人体关节的视频，我们首先选择框架提取网络（FDNet）来提取视频中的关键帧，这是由提出的深度渐进式强化学习方法进行训练所得到。 我们根据两个重要因素逐步调整每个状态下的选定帧。 一个是所选帧用于动作识别的所具备的判别能力。 另一个是所选帧与整个动作序列的关系。然后，我们采用<strong>基于图的卷积神经网络（GCNN）</strong>，它保留了人体关节之间的依赖关系，以处理所选关键帧以进行动作识别。 本文的方法在三个广泛使用的数据集上实现了非常有竞争力的性能。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;论文一：Optical-Flow-Guided-Feature-A-Fast-and-Robust-Motion-Representation-for-Video-Action-Recognition&quot;&gt;&lt;a href=&quot;#论文一：Optical-Flow-Guided-Feature-A-Fast-and-Robust-Motion-Representation-for-Video-Action-Recognition&quot; class=&quot;headerlink&quot; title=&quot;论文一：Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition&quot;&gt;&lt;/a&gt;论文一：Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition&lt;/h2&gt;&lt;p&gt;这是今年CVPR 2018中做行为识别的一篇文章，提出了一个叫做光流引导的特征（Optical Flow guided Feature，OFF）。时间信息是视频行为识别的关键，二光流可以很好的表征时间信息，其在视频分析领域已经被很多工作证明是一个很有用的特征。但是目前的双流网络Two-Stream在训练时其实还是比较麻烦的，因为需要单独对视频提取光流图，然后送到网络的另一至进行训练；而且如果数据集很大的话，光流图和RGB图像合起来得有原视频数据大小的好几倍，也十分消耗硬盘空间。因此思考如何利用单流网络同时利用RGB特征以及类似光流的特征去进行训练是一个值得思考的问题。本文从光流本身的定义出发，给了我们一个关于该问题很好的启发。该方法也在UCF-101逮到了96%的分类准确率，超过了不用Kinetics数据集预训练的I3D模型，可见该方法的有效性。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Action Recognition" scheme="http://jacobkong.github.io/blog/tags/Action-Recognition/"/>
    
      <category term="行为识别" scheme="http://jacobkong.github.io/blog/tags/%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB/"/>
    
      <category term="CVPR 2018" scheme="http://jacobkong.github.io/blog/tags/CVPR-2018/"/>
    
  </entry>
  
  <entry>
    <title>行为检测论文笔记：One-shot Action Localization by Learning Sequence Matching Network</title>
    <link href="http://jacobkong.github.io/blog/3531717168/"/>
    <id>http://jacobkong.github.io/blog/3531717168/</id>
    <published>2018-05-30T07:51:24.000Z</published>
    <updated>2018-05-31T08:49:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>这是今年CVPR 2018中接受为数不多的动作时间轴定位论文中的另一篇，基于学习的时间轴动作定位方法需要大量的训练数据。 然而，这样的大规模视频数据集不仅非常难以获得而且可能因为存在无数的动作类别而不实用。 当训练样本少且罕见时，当前方法的弊端就暴露出来了。为了解决这个挑战，本文的解决方案是采用匹配网络的One-shot学习技术，并利用相关性来挖掘和定位以前没有看过类别的行为。 本文在THUMOS14和ActivityNet数据集上评估了本文的one-shot动作定位方法。</p>
<a id="more"></a>
<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li>现在显存的基于深度学习的行为定位的方法都采用很强的监督学习策略，需要大量的标注数据，非常耗时去收集。</li>
<li>虽然转移学习或模型预训练可能在一定程度上缓解了这个问题，但处理新的动作类别和将学习的网络模型以高数据效率适应到新场景中仍然具有挑战性。</li>
<li>在本文中，考虑one(few)-shot的动作定位学习场景：给出一个（或几个）新动作类的例子，通常每个类一个例子，我们的目标是检测未修剪视频中所有出现的每个类。</li>
<li>目前很少有工作将one-shot learning应用到检测时空特征目标中。</li>
</ul>
<h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><p>为缓解目前用来训练的动作视频数据量稀少且难以获取的问题，利用one(few)-shot learning的方法，通过少量训练样本即可达到时间轴定位以及未见过的动作的预测，提高模型的泛化性能。</p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>论文框架如下：</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1frto52bt5ej30rj0e0gpp.jpg" alt=""></p>
<ul>
<li><p>本文开发了一种新颖的<strong>元学习（meta-learning）策略</strong>，将<strong>视频序列匹配</strong>的任务级先验知识集成到学习动作定位中。 我们的一次学习策略的关键思想是动作视频的直观，结构化表示适合于用来匹配（部分）序列（matching  sequences），以及一种相似性度量，它允许我们将动作示例的标签转换为未修剪视频中的<strong>动作提议</strong>。</p>
</li>
<li><p>本文提出了一个新的Matching Network结构，首先生成许多proposal，然后这些proposal送入到三个网络组件中进行动作标签预测：</p>
<ul>
<li><p><strong>Video Encoder Network：</strong>它为每个行动建议和参考行动计算一个segment-based的动作表示，它维护行动的时间结构并用于准确定位。</p>
<ul>
<li>该网络的性能依赖于候选proposal和参考视频之间的能否良好对应，为了实现准确对齐，我们打算在行动表示中保留动作视频的时间结构。 为此，我们开发了一个利用ranking LSTM来获得segment-based的视频表示，以便将每个动作实例编码为固定长度序列的视频片段特征。</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1frua9d9df8j30fs0b8q5p.jpg" alt=""></p>
</li>
<li><p><strong>Similarity Network:</strong> 将行动建议利用相似性网络与每个参考行动（reference action）进行比较，该网络在每个时间步骤生成一组相关分数。</p>
<ul>
<li><p>类似于Matching Network，网络首先计算每个单独示例$x_i$相对于整个支持集（support set）的完整上下文嵌入</p>
</li>
<li><p>然后给定一个行动建议$\hat{x}$及其编码向量$g(x_i)$，相似性网络计算提议表示与所有示例之间的<strong>余弦距离</strong>：</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fruajf2q51j30b701wwej.jpg" alt="mage-20180531100105"></p>
</li>
<li><p>接着基于上述距离，原始匹配网络使用关注机制和投票策略将测试数据分类到支持集中的一个类中</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1frub8m9vjtj309t029weg.jpg" alt="mage-20180531102522"></p>
</li>
<li><p>我们注意到，由于支持集只由前景类组成，这种分类方法不适用于定位任务，我们还必须区分前景和背景。 因此，在本文的 one-shot action localization 架构中，我们使用相似性网络计算相关分数，并设计一个单独的标签网络来推断每个提案的类别标签（包括背景）。</p>
</li>
</ul>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1frua9w27e6j30fs0b2di1.jpg" alt=""></p>
</li>
<li><p><strong>Labeling Network:</strong> 设置不同长度的时间窗口，根据时间窗口内的encoding vector和correlation scores，第三个网络会在每个时间步骤中预测提案的动作类别标签（作为前景类别或背景之一）。</p>
<ul>
<li>标记网络直接应用于相关矩阵。 类似于通过比较余弦距离进行分类，我们通过在短暂的时间跨度上在相关矩阵上应用全连接层来比较相关矩阵的相同列的不同行，并且输出在前景和通过sigmoid激活的背景。</li>
<li>全连接网络沿时间维度滑过相关矩阵，为每个提案确定前景/背景。 在标签网络确定为前景的提议中，公式6应用于预测动作标签，如框架图的上半部分所示。</li>
<li>相关矩阵包含有关特定特定类的信息以及提案是属于背景还是属于前景。 例如，如果与一个示例的关联比其他示例的关联高得多，那么该提议很可能属于前景并且与示例具有相同的动作标签; 如果与所有例子的相关性都比较低，那么它可能属于背景。 <strong>标签网络通过训练学习这些标准。</strong></li>
<li>还要注意标签网络在某种意义上<strong>独立于动作类别</strong>，因为它应用于相关矩阵而不是每个视频的特征表示。 这意味着它学到的标准应该适用于输入来自不同类别的视频，<strong>这使得它适用于一次性预测。</strong></li>
<li>后处理阶段：我们结合多尺度 proposal-level 预测来获得frame-level的单帧级预测，并将相同标号的相邻帧分组以获得动作实例。</li>
</ul>
</li>
</ul>
</li>
<li><p>本文的定位系统是针对 one-shot action localization 而设计的，相应的 Meta Learning Formulation 可用于模型的训练。 系统的每个组成部分都是可导的，并且可以对系统进行端到端的培训。 然而，为了获得更好的初始化和性能，我们对<strong>Video Encoder Network</strong>和<strong>Similarity Network</strong>进行了预训练。 </p>
<ul>
<li><p>Meta Learning Formulation</p>
<ul>
<li>在元学习中，模型在一组训练任务的元阶段进行训练，并对另一组测试任务进行评估</li>
<li>元学习的目标是在元测试任务分布中找到最小化损失的模型</li>
</ul>
</li>
<li><p>Optimization for the Localization System</p>
<ul>
<li><p>损失函数为：</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1frujibjlk1j30bw01c748.jpg" alt=""></p>
<p>其中，两个损失函数分别如下：</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1frujixrzcpj30dk03h74i.jpg" alt="mage-20180531151204"></p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1frujj5jzvsj30cy02udfy.jpg" alt="mage-20180531151217"></p>
</li>
</ul>
</li>
<li><p>Pretraining for Video Encoder &amp; Similarity Net</p>
<ul>
<li><p>预训练两者的损失函数：</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1frujmmshe5j30df01wq2y.jpg" alt="mage-20180531151538"></p>
</li>
<li><p>利用到了一个Ranking Loss，其直观的想法是：当给予越来越多的视频内容，分类器会生成越来越置信度高的预测</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><ul>
<li><p>实验在<strong>Thumos14</strong>和<strong>ActivityNet 1.2</strong>上进行评估</p>
</li>
<li><p><strong>One-shot问题设置要求测试期间的类别不得存在在训练期间</strong>，因此在训练和测试过程中需要对两个数据集进行划分，从而达到这一要求</p>
</li>
<li><p>训练和测试阶段有很多细节，建议看论文，这里不再赘述</p>
</li>
<li><p>和强监督方法的比较，我们从表1和表2可以看出，虽然one-shot和全监督行为检测之间仍然存在性能差距，但我们的方法在以one-shot设置进行测试时，显着优于目前最先进的方法。 我们在Thumos14数据集上使用更多的训练数据，每类15个样本进一步测试我们的方法。 我们的结果表明，性能出现明显提升，而CDC只有很小的提升，这表明我们的方法相对于样本数量<strong>具有良好的可扩展性</strong>。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1frujw8cs54j30fz0a1q59.jpg" alt="mage-20180531152451"></p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1frujwobo2qj30fw0ct40u.jpg" alt="mage-20180531152512"></p>
</li>
<li><p>还有一些消融探究实验，这里也不再赘述</p>
</li>
</ul>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li>针对目前视频数据集标注费时、难以获得等痛点，作者首次将one(few)-shot的方法引入了时间轴定位，很有创新点，和弱监督学习有异曲同工之妙</li>
<li>提出了一种基于匹配网络框架的动作定位问题的<strong>元学习方法</strong>，它能够捕获<strong>任务级别（task-level）的先验知识</strong></li>
<li>后处理中的grouping策略可以要用在我之前的工作中</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>性能目前看来并不是特别理想，和全监督的方法还有差距，还有很多改进的空间，这可能需要多去了解一些<strong>one-shot learning或transfer learning</strong>等方向的论文。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是今年CVPR 2018中接受为数不多的动作时间轴定位论文中的另一篇，基于学习的时间轴动作定位方法需要大量的训练数据。 然而，这样的大规模视频数据集不仅非常难以获得而且可能因为存在无数的动作类别而不实用。 当训练样本少且罕见时，当前方法的弊端就暴露出来了。为了解决这个挑战，本文的解决方案是采用匹配网络的One-shot学习技术，并利用相关性来挖掘和定位以前没有看过类别的行为。 本文在THUMOS14和ActivityNet数据集上评估了本文的one-shot动作定位方法。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Action Detection" scheme="http://jacobkong.github.io/blog/tags/Action-Detection/"/>
    
      <category term="行为检测" scheme="http://jacobkong.github.io/blog/tags/%E8%A1%8C%E4%B8%BA%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>行为检测论文笔记：Rethinking the Faster R-CNN Architecture for Temporal Action Localization</title>
    <link href="http://jacobkong.github.io/blog/3697434189/"/>
    <id>http://jacobkong.github.io/blog/3697434189/</id>
    <published>2018-05-29T07:51:24.000Z</published>
    <updated>2018-05-30T09:13:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>这是今年CVPR 2018中接受为数不多的动作时间轴定位论文中的一篇，解决了目前现存方法中的3个问题：（1）Multi-scale的动作片段；（2）Temproal context的利用；（3）Multi-stream 特征融合。方法在THUMOS’ 14数据集上的提议和检测任务上达到目前最好的效果（mAP@tIoU=0.5达到42.8%），在ActivityNet数据及上取得了具有挑战性的效果。</p>
<a id="more"></a>
<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li><p>时间轴行为检测其实和目标检测相类似，因此目前许多行为检测的方法都受启发于目标检测的一些先进方法，比如R-CNN系列，先从整个视频中生成segments proposal，然后用分类器去对这些proposal进行分类。</p>
</li>
<li><p>目前有一些方法将Faster R-CNN迁移到时间轴行为检测中，然而直接迁移过来引入一些挑战，如下：</p>
<ul>
<li><p>如何处理行动持续时间的巨大变化？</p>
<blockquote>
<p>因为行为会有许多时间长短不一的持续时间，从几秒到几分钟的行为片段都有，而Faster R-CNN利用anchor提proposal会在特征的temporal scope和anchor的span之间产生misalignment现象。<strong>我们提出了一个multi-tower网络和利用扩张时间卷积（dilated temporal convolutions）来解决alignment的问题。</strong></p>
</blockquote>
</li>
<li><p>如何利用时间上下文信息？</p>
<blockquote>
<p>动作实例之前和之后的时刻包含关于定位和分类的关键信息（可以说比对象的空间上下文更重要）。Faster R-CNN没有利用时间上下文信息。<strong>我们建议通过扩展提案生成和动作分类中的感受野来明确地编码时间上下文。</strong></p>
</blockquote>
</li>
<li><p>如何最好的去融合multi-stream的特征？</p>
<blockquote>
<p>对于Faster R-CNN探索这种RGB和Flow特征融合方面的工作有限。 我们提出了一个后期融合方案，并且经验性地证明了它在一般的早期融合方案上的优势。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><p>解决Faster R-CNN直接引入到时间轴行为检测中的上述3个挑战,并以此来提升Faster R-CNN在行为检测中的性能.</p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>论文框架如下：</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1frr390sqxsj30ua0a4adx.jpg" alt="mage-20180528153210"></p>
<p>本文提出了TAL-Net，有三个创新的结构改变：</p>
<ul>
<li><p>Receptive Field Alignment</p>
<ul>
<li><p>传统的anchor机制有一个缺点：每个时间点的锚点分类都有相同的单一的感受野。</p>
</li>
<li><p>为了解决这个问题，我们建议将每个锚点的感受野与它的时间跨度对齐。 这是通过两个关键因素实现的：multi-tower网络和扩张时间卷积（dilated temporal convolutions）。</p>
</li>
<li><p>给定一个一维feature map，我们的Segment Proposal Network 由K个temproal ConvNets 组成，每个K网络负责对特定比例的锚段进行分类.最重要的是，每个时间ConvNet都经过精心设计，使得其接受的字段大小与相关的锚点尺度一致。 在每个ConvNet结束时，我们分别应用两个核心大小为1的平行卷积层进行锚定分类和边界回归。</p>
</li>
<li><p>另一问题：如何设计具有可控感受野s的时间卷积？</p>
<ul>
<li><p>方法一：如果s=2L+1，则叠加L层卷积层得到相应的感受野。缺点是层数L随着s线性增加，很容易增加参数数量使网络过拟合。</p>
</li>
<li><p>方法二：在每一层卷积层后添加一个kernel size为2的pooling层，则感受野$s=2^{(L+1)}-1$，此时层数随着s成log变化，但是添加pooling层会减小输出feature map的分辨率，会影响定位准确率。</p>
</li>
<li><p>方法三：使用扩充时间卷积，这种卷积可以在扩充感受野的同时不损失分辨率。在我们的Segment Proposal Network中，每一个temporal ConvNet都只由2个dilated convolutional layers组成。为了获得一个目标感受野s，则第一层的dilated convolutional layers的dilation rate $r_1=s/6, r_2=(s/6)\times2$. </p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1frpy9obbp6j30g70bsjss.jpg" alt=""></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Context Feature Extraction</p>
<ul>
<li><p>时间轴上下文信息十分重要</p>
</li>
<li><p>为了确保上下文特征用于锚定分类和边界回归，感受野必须覆盖时间轴上下文信息区域，可以通过将dilation rate加倍，即$r_1=s/6\times2, r_2=(s/6)\times2\times2$，如下：</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1frpysrksxjj30fq0b53zx.jpg" alt=""></p>
</li>
<li><p>在动作分类阶段，我们要利用SoI pooling来为每个proposal提取一个固定尺寸的feature map</p>
</li>
</ul>
</li>
<li><p>Late Feature Fusion</p>
<ul>
<li>目前许多方法都在使用RGB和光流特征</li>
<li>本文为双流特征提出了一个后融合的机制</li>
<li>们首先使用两个不同的网络分别从RGB帧和叠加的光流中提取两个一维特征映射。 我们通过一个不同的Segment Proposal Network来处理每个feature ma，该网络并行地生成锚定分类和边界回归的逻辑。 我们使用来自两个网络的logits的元素平均值作为最终的逻辑来生成提议。 对于每个提案，我们在两个特征映射上并行执行SoI池，并在每个输出上应用不同的DNN分类器。</li>
</ul>
</li>
</ul>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><ul>
<li><p>基于TensorFlow目标检测API</p>
</li>
<li><p>9个anchor，scales为{1, 2, 3, 4, 5, 6, 8, 11, 16}</p>
</li>
<li><p>NMS阈值为0.7去筛选proposal，保留前300个proposal用于分类</p>
</li>
<li><p>THUMOS’ 14检测结果</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1frq0h25kllj30g20goadr.jpg" alt=""></p>
</li>
<li><p>ActivityNet v1.3在验证集的检测结果</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1frq0m3der6j30ft07d0u2.jpg" alt=""></p>
</li>
</ul>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>相比于R-C3D，本文的方法解决了Multi-scale的问题，利用了上下文信息以及额外的光流信息，解决了目前许多方法中存在的大大小小的缺陷，组合成了一个较为完整的框架，因此在THUMOS’ 14数据集上检测效果达到最好，在ActivityNet数据集上也取得了很有竞争力的结果，但是还是不如SSN的结果。文中分析：THUMOS’ 14是一个更好的用来评估行为定位的数据集，因为其每段视频中包含有更多的行为实例，并且每段视频包含大量的背景活动。</p>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>我认为除了第一点创新：利用dilated temporal convlutional组成感受野可控的multi-tower网络来解决multi-scale问题比较有创新外，另外两点创新其实不算特别有新意。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是今年CVPR 2018中接受为数不多的动作时间轴定位论文中的一篇，解决了目前现存方法中的3个问题：（1）Multi-scale的动作片段；（2）Temproal context的利用；（3）Multi-stream 特征融合。方法在THUMOS’ 14数据集上的提议和检测任务上达到目前最好的效果（mAP@tIoU=0.5达到42.8%），在ActivityNet数据及上取得了具有挑战性的效果。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Action Detection" scheme="http://jacobkong.github.io/blog/tags/Action-Detection/"/>
    
      <category term="行为检测" scheme="http://jacobkong.github.io/blog/tags/%E8%A1%8C%E4%B8%BA%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>论文调研：ICCV 2017论文调研</title>
    <link href="http://jacobkong.github.io/blog/679115822/"/>
    <id>http://jacobkong.github.io/blog/679115822/</id>
    <published>2017-11-25T07:51:24.000Z</published>
    <updated>2018-05-28T07:34:35.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Visual-object-tracking"><a href="#Visual-object-tracking" class="headerlink" title="Visual object tracking"></a>Visual object tracking</h2><ul>
<li><h4 id="Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades"><a href="#Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades" class="headerlink" title="Learning Policies for Adaptive Tracking with Deep Feature Cascades"></a>Learning Policies for Adaptive Tracking with Deep Feature Cascades</h4><ul>
<li>Our fundamental insight is to take an adaptive approach, where easy frames are processed with cheap features (such as pixel values), while challenging frames are processed with invariant but expensive deep features.</li>
<li>Formulate the adaptive tracking problem as a decision-making process.</li>
<li>Learn an agent to decide whether to locate objects with high conﬁdence on an early layer, or continue processing subsequent layers of a network.</li>
</ul>
</li>
</ul>
<ul>
<li>Signiﬁcantly reduces the feedforward cost.</li>
<li>Train the agent ofﬂine in a reinforcement learning fashion.</li>
<li>Obviously, the major computational burden comes from the forward pass through the entire network, and can be larger with deeper architectures.</li>
<li>However, when the object is visually distinct or barely moves, early layers are in most scenarios sufﬁcient for precise localization - offering the potential for substantial computational savings.</li>
<li>The agent learns to ﬁnd the target at each layer, and decides if it is conﬁdent enough to output and stop there.</li>
</ul>
<a id="more"></a>
<ul>
<li><h4 id="Tracking-The-Untrackable-Learning-to-Track-Multiple-Cues-with-Long-Term-Dependencies"><a href="#Tracking-The-Untrackable-Learning-to-Track-Multiple-Cues-with-Long-Term-Dependencies" class="headerlink" title="Tracking The Untrackable: Learning to Track Multiple Cues with Long-Term Dependencies"></a>Tracking The Untrackable: Learning to Track Multiple Cues with <strong>Long-Term Dependencies</strong></h4><ul>
<li>Combine cues in a coherent end-to-end fashion over a <strong>long period of time.</strong></li>
<li>Present a structure of Recurrent Neural Networks (RNN) that jointly reasons on multiple cues over a temporal window.</li>
<li>We are able to <strong>correct many data association errors</strong> and recover observations from an occluded state.</li>
</ul>
</li>
<li><h4 id="Tracking-as-Online-Decision-Making-Learning-a-Policy-from-Streaming-Videos-with-Reinforcement-Learning"><a href="#Tracking-as-Online-Decision-Making-Learning-a-Policy-from-Streaming-Videos-with-Reinforcement-Learning" class="headerlink" title="Tracking as Online Decision-Making: Learning a Policy from Streaming Videos with Reinforcement Learning"></a>Tracking as Online Decision-Making: Learning a Policy from Streaming Videos with Reinforcement Learning</h4><ul>
<li>A tracking agent must follow an object despite ambiguous image frames and a limited computational budget.</li>
<li>The agent must decide<ul>
<li>where to look in the upcoming frames</li>
<li>when to reinitialize because it believes the target has been lost</li>
<li>when to update its appearance model for the tracked object</li>
</ul>
</li>
<li>Formulating tracking as a partially observable decision-making process (POMDP).</li>
<li><strong>Sparse rewards</strong> allow us to quickly train on massive datasets.</li>
<li>Challenges:<ul>
<li>First, the limited quantity of annotated video data impedes both training and evaluation.</li>
<li>Second, as vision (re)integrates with robotics, video processing must be done in an online, streaming fashion.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Face-detection"><a href="#Face-detection" class="headerlink" title="Face detection"></a>Face detection</h3><ul>
<li><h4 id="S3FD-Single-Shot-Scale-invariant-Face-Detector"><a href="#S3FD-Single-Shot-Scale-invariant-Face-Detector" class="headerlink" title="S3FD - Single Shot Scale-invariant Face Detector."></a>S3FD - Single Shot Scale-invariant Face Detector.</h4><ul>
<li>Use a single deep neural network, especially for small faces.</li>
<li>Contribution<ul>
<li>提出一个尺度公平的人脸检测框架来处理不同尺度的人脸。我们在各种各样的图层上拼贴anchor，以确保所有人脸的比例尺都具有足够的特征用于检测。基于有效接收域<strong>（effective receptive ﬁeld）</strong>和等比例区间原则<strong>（equal proportion interval principle）</strong>设计anchor</li>
<li>用尺度补偿anchor匹配策略<strong>（ a scale compensation anchor matching strategy）</strong>提高小脸的召回率; </li>
<li>通过最大化背景标签<strong>（ max-out background label）</strong>减少小脸的误报率。</li>
<li>effective receptive ﬁeld: Understanding the effective receptive ﬁeld in deep convolutional neural networks.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Salient-Object-Detection"><a href="#Salient-Object-Detection" class="headerlink" title="Salient Object Detection"></a>Salient Object Detection</h3><ul>
<li><h4 id="Amulet-Aggregating-Multi-level-Convolutional-Features-for-Salient-Object-Detection"><a href="#Amulet-Aggregating-Multi-level-Convolutional-Features-for-Salient-Object-Detection" class="headerlink" title="Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection"></a>Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection</h4><ul>
<li>How to better aggregate multi-level convolutional feature maps for salient object detection is <strong>underexplored</strong>.</li>
<li>Our framework:<ul>
<li>First integrates multi-level feature maps into multiple resolutions, which simultaneously incorporate coarse semantics and ﬁne details. </li>
<li>Then it adaptively learns to combine these feature maps at each resolution and <strong>predict saliency maps with the combined features.</strong> </li>
<li>Finally, the predicted results are efﬁciently fused to generate the <strong>ﬁnal saliency map</strong>.</li>
</ul>
</li>
<li>In addition, edge-aware maps and high-level predictions are embedded into the framework.</li>
</ul>
</li>
<li><h4 id="Learning-Uncertain-Convolutional-Features-for-Accurate-Saliency-Detection"><a href="#Learning-Uncertain-Convolutional-Features-for-Accurate-Saliency-Detection" class="headerlink" title="Learning Uncertain Convolutional Features for Accurate Saliency Detection"></a><a href="http://openaccess.thecvf.com/content_iccv_2017/html/Zhang_Learning_Uncertain_Convolutional_ICCV_2017_paper.html" target="_blank" rel="external">Learning Uncertain Convolutional Features for Accurate Saliency Detection</a></h4><ul>
<li>The key contribution of this work is to learn deep uncertain convolutional features (UCF), which encourage the robustness and accuracy of saliency detection.</li>
<li>我们提出了一种有效的混合上采样方法来减少我们的解码器网络中去卷积算子的棋盘伪影。</li>
<li>We ﬁnd that the actual cause of these artifacts is the upsampling mechanism, which generally utilizes the deconvolution operation.</li>
</ul>
</li>
</ul>
<h3 id="Action-Related"><a href="#Action-Related" class="headerlink" title="Action Related"></a>Action Related</h3><ul>
<li><h4 id="Encouraging-LSTMs-to-Anticipate-Actions-Very-Early"><a href="#Encouraging-LSTMs-to-Anticipate-Actions-Very-Early" class="headerlink" title="Encouraging LSTMs to Anticipate Actions Very Early"></a>Encouraging LSTMs to Anticipate Actions Very Early</h4><ul>
<li>Action anticipation - identify the action from only partially available videos.</li>
<li>To this end, we develop a <strong>multi-stage LSTM architecture</strong> that leverages <strong>context-aware</strong> and <strong>action-aware</strong> features, and introduce <strong>a novel loss function</strong> that encourages the model to predict the correct class as early as possible.</li>
<li>Intuitive: our loss models the intuition that some actions, such as running and high jump, are highly ambiguous after seeing only the ﬁrst few frames, and <strong>false positives</strong> should therefore not be penalized too strongly in the early stages.</li>
<li>We would like to predict a high probability for the correct class <strong>as early as possible</strong>, and thus penalize <strong>false negatives</strong> from the beginning of the sequence.</li>
<li>Contribute a novel multi-stage Long Short Term Memory (LSTM) architecture for action anticipation. This model effectively extracts and jointly exploits context- and action-aware features.</li>
<li>Existing method drawbacks:<ul>
<li>This is in contrast to existing methods that typically extract either <strong>global representations</strong> for the entire image or video sequence thus <strong>not focusing on the action itself, or localize the feature extraction process to the action itself</strong> via dense trajectories  optical ﬂow or actionness, thus <strong>failing to exploit contextual information.</strong></li>
<li>利用光流不允许这些方法在localization过程中明确地利用外观。</li>
<li>Computing optical ﬂow is typically expensive.</li>
</ul>
</li>
<li>In the future, we intend to study new ways to incorporate additional sources of information, such as <strong>dense trajectories</strong> and <strong>human skeletons</strong> in our framework.</li>
</ul>
</li>
<li><h4 id="Unsupervised-Action-Discovery-and-Localization-in-Videos"><a href="#Unsupervised-Action-Discovery-and-Localization-in-Videos" class="headerlink" title="Unsupervised Action Discovery and Localization in Videos"></a>Unsupervised Action Discovery and Localization in Videos</h4><ul>
<li>创新：无监督的action localization。</li>
<li>First to address the problem of unsupervised action localization in videos.</li>
<li><p>We propose a novel approach that:</p>
<ul>
<li>Discovers action class labels</li>
<li>Spatio-temporally localizes actions in videos.</li>
</ul>
</li>
<li><p>Method:</p>
<ul>
<li>It begins by computing local video features to apply <strong>spectral clustering</strong> on a set of unlabeled training videos.<ul>
<li>For each cluster of videos, an <strong>undirected graph</strong> is constructed to extract a dominant set, which are known for <strong>high internal homogeneity</strong> and <strong>in-homogeneity</strong> between vertices outside it.</li>
</ul>
</li>
<li>Next, a <strong>discriminative clustering approach</strong> is applied, by training a classiﬁer for each cluster, to iteratively select videos from the non-dominant set and obtain complete video action classes.<ul>
<li>Once classes are discovered, training videos within each cluster are selected to perform <strong>automatic spatio-temporal annotations</strong>, by ﬁrst over-segmenting videos in each discovered class into <strong>supervoxels</strong>（超体素） and constructing a <strong>directed graph</strong> （有向图）to apply a variant of knapsack problem with temporal constraints. （并构建有向图以应用具有时间约束的背包问题的变体。）</li>
</ul>
</li>
<li>背包优化联合收集超体素的一个子集，通过强制注释的动作进行时空连接，其体积是一个actor的大小。These annotations are used to train SVM action classiﬁers.</li>
</ul>
</li>
<li>在测试过程中，操作使用类似的背包方法来进行localize，在这种方法中将超体素分组在一起，并且使用来自发现的动作类的视频学习的SVM被用于识别这些动作。</li>
<li>However, supervised algorithms have some disadvantages compared to unsupervised approaches, due to the difﬁculty of video annotation.</li>
<li>Contributions：<ul>
<li>Automatic discovery of action class labels using <strong>a new discriminative clustering approach</strong> with dominant sets (Sec. 3).</li>
<li>We propose a novel <strong>Knapsack approach</strong> with <strong>graph-based temporal constraints</strong> to <strong>annotate actions</strong> in training videos</li>
<li>The annotations within each cluster of videos are jointly selected by <strong>Binary Integer Quadratic Programming (BIQP)</strong> optimization to train action classiﬁers.</li>
<li><strong>Structural SVM</strong> is used to learn the pairwise relations of supervoxels within foreground action and foreground-background, which enforces that the supervoxels belonging to the action to be simultaneously selected.</li>
<li>Lastly, we address a new problem of <strong>Unsupervised Action Localization</strong> (Sec. 5.2).</li>
</ul>
</li>
</ul>
</li>
<li><h4 id="Dense-Captioning-Events-in-Videos"><a href="#Dense-Captioning-Events-in-Videos" class="headerlink" title="Dense-Captioning Events in Videos"></a>Dense-Captioning Events in Videos</h4><ul>
<li>We introduce the task of dense-captioning events, which involves both detecting and describing events in a video.</li>
<li>Our model introduces a variant of an existing proposal module that is designed to capture both short as well as long events that span minutes.</li>
<li>To <strong>capture the dependencies between the events in a video</strong>, our model introduces a <strong>new captioning module</strong> that uses <strong>contextual information</strong> from past and future events to jointly describe all events.</li>
<li>While the success of these methods is encouraging, they all share one key limitation: <strong>detail.</strong></li>
<li>We introduce the task of <strong>dense-captioning events</strong>, which requires a model to generate a set of descriptions for multiple events occurring in the video and localize them in time.</li>
<li>However, we observe that densecaptioning events comes with its own set of challenges distinct from the image case.<ul>
<li>One observation is that events in videos can range across multiple time scales and can even overlap.<ul>
<li>Past captioning works have circumvented this problem by encoding the entire video sequence by <strong>mean-pooling</strong> [50] or by using a <strong>recurrent neural network (RNN)</strong> [49].</li>
<li>To overcome this limitation, we extend recent work on generating action proposals [10] to <strong>multi-scale detection of events.</strong></li>
</ul>
</li>
<li>Another key observation is that the events in a given video <strong>are usually related to one another.</strong> <ul>
<li>We introduce a <strong>captioning module</strong> that utilizes the context from all the events from our proposal module to generate each sentence.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><h4 id="Learning-long-term-dependencies-for-action-recognition-with-a-biologically-inspired-deep-network"><a href="#Learning-long-term-dependencies-for-action-recognition-with-a-biologically-inspired-deep-network" class="headerlink" title="Learning long-term dependencies for action recognition with a biologically-inspired deep network"></a>Learning long-term dependencies for action recognition with a biologically-inspired deep network</h4><ul>
<li>How to efﬁciently learn long-term dependencies from sequences still remains a pretty challenging task.</li>
<li>As one of the key models for sequence learning, <strong>recurrent neural network (RNN)</strong> and its variants such as <strong>long short term memory (LSTM)</strong> and <strong>gated recurrent unit (GRU)</strong> are still not powerful enough in practice.<ul>
<li>One possible reason is that they have only feedforward connections, which is different from the biological neural system that is typically composed of <strong>both feedforward and feedback connections.</strong>(既有前传，也有反馈)</li>
</ul>
</li>
<li>Propose <strong>shuttleNet technologically.</strong></li>
<li>The shuttleNet <strong>consists of several processors</strong>, each of which is a GRU while <strong>associated with multiple groups of cells and states.</strong></li>
<li><strong>Attention mechanism</strong> is then employed to select the best information ﬂow pathway.</li>
</ul>
</li>
<li><h4 id="Adaptive-RNN-Tree-for-Large-Scale-Human-Action-Recognition"><a href="#Adaptive-RNN-Tree-for-Large-Scale-Human-Action-Recognition" class="headerlink" title="Adaptive RNN Tree for Large-Scale Human Action Recognition"></a>Adaptive RNN Tree for Large-Scale Human Action Recognition</h4><ul>
<li>We present the RNN Tree (RNN-T), an adaptive learning framework for <strong>skeleton based human action recognition.</strong></li>
<li>Our method categorizes action classes and <strong>uses multiple Recurrent Neural Networks (RNNs)</strong> in a <strong>treelike hierarchy.</strong></li>
<li>在骨架表示中的行为是通过<strong>分层推理</strong>过程来识别的，在这个过程中，单独的RNN将细化的行为类别与增加的置信度</li>
<li>RNN-T effectively addresses two main challenges of large-scale action recognition:<ul>
<li>able to distinguish ﬁne-grained action classes that are intractable using a single network</li>
<li>adaptive to new action classes by augmenting an existing model.</li>
</ul>
</li>
</ul>
</li>
<li><h4 id="Ensemble-Deep-Learning-for-Skeleton-based-Action-Recognition-using-Temporal-Sliding-LSTM-networks"><a href="#Ensemble-Deep-Learning-for-Skeleton-based-Action-Recognition-using-Temporal-Sliding-LSTM-networks" class="headerlink" title="Ensemble Deep Learning for Skeleton-based Action Recognition using Temporal Sliding LSTM networks"></a>Ensemble Deep Learning for Skeleton-based Action Recognition using Temporal Sliding LSTM networks</h4><ul>
<li>Traditional methods generally use relative coordinate systems dependent on some joints, and model <strong>only the long-term dependency</strong>, while excluding <strong>short-term and medium term dependencies.</strong></li>
<li>We transform the skeletons into <strong>another coordinate system</strong> to obtain the robustness to scale, rotation and translation and then extract salient motion features from them.</li>
<li>We propose novel ensemble <strong>Temporal Sliding LSTM (TS-LSTM) networks</strong> for skeleton-based action recognition. The proposed network is composed of multiple parts containing <strong>short-term, medium-term and long-term TS-LSTM networks</strong>.</li>
<li>With a rapid development of 3D data acquisition over the past few decades, lots of researches on <strong>human activity recognition from 3D data</strong> can have been actively performed.</li>
<li>For the modeling of human actions, recent researches show tha<strong>t Long Short-Term Memory (LSTM) networks</strong> are <strong>superior to</strong> temporal pyramids and hidden markov models.</li>
<li>Overall Method：<ul>
<li>Firstly, we transform the coordinates of input skeleton sequences so that the data can be <strong>robust to scale, rotation and translation.</strong> </li>
<li>Secondly, instead of using the simple joint positions, we employ the <strong>motion features</strong> in terms of temporal differences, which help our networks to be focused on the actual skeleton movements. </li>
<li>Thirdly, the motion features are processed with <strong>multi-term LSTMs containing short-term, medium-term and long-term LSTMs</strong>, which allow robustness to variable temporal dynamics. </li>
<li>Finally, the multi-term LSTMs <strong>capture a variety of action dynamics through ensemble.</strong></li>
</ul>
</li>
</ul>
</li>
<li><h4 id="What-Actions-are-Needed-for-Understanding-Human-Actions-in-Videos"><a href="#What-Actions-are-Needed-for-Understanding-Human-Actions-in-Videos" class="headerlink" title="What Actions are Needed for Understanding Human Actions in Videos?"></a>What Actions are Needed for Understanding Human Actions in Videos?</h4><ul>
<li>We analyze the current state of human activity understanding in videos.</li>
<li>The goal of this paper is to examine datasets, evaluation metrics, algorithms, and potential future directions.</li>
<li>The results demonstrate that while there is inherent ambiguity in the temporal extent of activities, current datasets still permit effective benchmarking.</li>
<li>我们发现，当与时间推理相结合时，对物体和姿态的细粒度理解很可能在算法精度上产生实质性的改善。</li>
<li>Some questions:<ul>
<li>What is an activity and how should we represent it? </li>
<li>Do activities have well-deﬁned spatial and temporal extent? </li>
<li>What role do goals and intentions play in deﬁning and understanding activities?</li>
<li>What does the data show about the right categories for recognition in case of activities? </li>
<li>Do existing approaches scale with increasing complexity of activities categories, video data, or temporal relationships between activities? </li>
<li>Are the hypothesized new avenues of studying context, objects, or intentions worthwhile: Do these really help in understanding videos?</li>
</ul>
</li>
<li>This paper provides an in-depth analysis of the <strong>new generation of video datasets</strong>, <strong>human annotators</strong>, <strong>activity categories</strong>, <strong>recognition approaches</strong>, and above all possible new cues for video understanding.</li>
<li>We found that people considered verbs to be relatively more ambiguous.</li>
<li>This suggests that despite boundary ambiguity, current datasets allow us to understand, learn from, and evaluate the temporal extents of activities.</li>
<li>That is, a perfect classiﬁer would automatically do 5 times better than current state-of-the-art [30] on activity localization.</li>
<li>This suggests that focusing our attention on gaining more insight into activity classiﬁcation would naturally yield signiﬁcant improvements in localization accuracy as well.</li>
<li>Having concluded that:<ul>
<li>(1) we should be reasoning about activities as (verb,object) pairs rather than just verb,</li>
<li>(2) temporal boundaries of activities are ambiguous but nevertheless meaningful, and</li>
<li>(3) classiﬁcation of short videos is a reasonable proxy for temporal localization</li>
</ul>
</li>
<li>This suggests that moving forward ﬁne-grained discrimination between activities with similar objects and verbs is needed.</li>
</ul>
</li>
<li><h4 id="Lattice-Long-Short-Term-Memory-for-Human-Action-Recognition"><a href="#Lattice-Long-Short-Term-Memory-for-Human-Action-Recognition" class="headerlink" title="Lattice Long Short-Term Memory for Human Action Recognition"></a>Lattice Long Short-Term Memory for Human Action Recognition</h4><ul>
<li>However, naively applying RNNs to video sequences in a convolutional manner implicitly assumes that motions in videos are <strong>stationary</strong> across different spatial locations. <strong>This assumption is valid for short-term motions but invalid when the duration of the motion is long.</strong></li>
<li>In this work, we propose Lattice-LSTM (L^2STM), which extends LSTM by learning <strong>independent hidden state transitions</strong> of memory cells for individual spatial locations.</li>
<li>Additionally, we introduce a novel multi-modal training procedure for training our network.</li>
<li>An accurate action recognition should:<ul>
<li>(1) have a high capacity for learning and capturing as many motion dynamics as possible</li>
<li>(2) when an action appears in sequential images, the neurons should properly decide what kind of spatio-temporal dynamics should be encoded into the memory for distinguishing actions.</li>
</ul>
</li>
</ul>
</li>
<li><h4 id="Common-Action-Discovery-and-Localization-in-Unconstrained-Videos"><a href="#Common-Action-Discovery-and-Localization-in-Unconstrained-Videos" class="headerlink" title="Common Action Discovery and Localization in Unconstrained Videos"></a>Common Action Discovery and Localization in Unconstrained Videos</h4><ul>
<li>In this work, we tackle the problem of <strong>common action discovery</strong> and <strong>localization</strong> in unconstrained videos, where we do not assume to know the types, numbers or locations of the common actions in the videos.</li>
<li>To perform automatic discovery and localization in such challenging scenarios, we ﬁrst generate action proposals using human prior.</li>
<li>By building an afﬁnity graph among all action proposals, we formulate the common action discovery as <strong>a subgraph density maximization problem</strong> to select the proposals containing common actions.</li>
<li>为了避免在指数级大的解空间中枚举，我们提出了一个有效的多项式时间优化算法。</li>
<li>It solves the problem up to a user speciﬁed error bound with respect to the global optimal solution.</li>
<li>Action discovery的困难：<ul>
<li>首先，由于我们事先不知道在给定的数据集中常见的动作类型或位置，我们必须同时进行发现和定位。 给定一组未标记的视频，我们需要自动识别一组捕获常见操作的时空边界框。</li>
<li>其次，类似的行为也可能由于视点变化，尺度变化或相机运动而出现不同。 自动关联这些常见操作并不是一项简单的任务。</li>
<li>最后，除了常见的动作之外，视频还可能包含动态背景或不常见的动作，因此将这种“noisy motions”与常见动作区分开来是至关重要的。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Pedestrian-Related"><a href="#Pedestrian-Related" class="headerlink" title="Pedestrian Related"></a>Pedestrian Related</h3><ul>
<li><h4 id="HydraPlus-Net-Attentive-Deep-Features-for-Pedestrian-Analysis"><a href="#HydraPlus-Net-Attentive-Deep-Features-for-Pedestrian-Analysis" class="headerlink" title="HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis"></a>HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis</h4><ul>
<li>Learning of comprehensive features of pedestrians for ﬁne-grained tasks remains an open problem.</li>
<li>HydraPlus-Net: multi-directionally feeds the multi-level attention maps to different feature layers.</li>
<li>Advantages: <ul>
<li>(1) the model is capable of capturing <strong>multiple attentions</strong> from low-level to semantic-level</li>
<li>(2) it explores the <strong>multi-scale selectiveness of attentive features</strong> to enrich the ﬁnal feature representations for a pedestrian image.</li>
</ul>
</li>
<li>We demonstrate the effectiveness and generality of the proposed HP-net for pedestrian analysis on two tasks, i.e. <strong>pedestrian attribute recognition</strong> and <strong>person reidentiﬁcation.</strong></li>
<li>However, the learning of feature representation for pedestrian images, as the backbone for all those applications, still confronts critical challenges and needs profound studies.</li>
<li>However, existing arts merely extract global features [13, 24, 30] and are hardly effective to location-aware semantic pattern extraction.</li>
<li><strong>Multidirectional attention (MDA) modules</strong></li>
<li>Reliable 3D <strong>skeleton-based action recognition (SAR)</strong> is now feasible [1].</li>
<li>Although much progress has been achieved, these methods are still facing two challenges.<ul>
<li>We term the ﬁrst one as the <strong>discriminative challenge.</strong></li>
<li>We term the second challenge as <strong>adaptability</strong>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Pose-Estimation"><a href="#Pose-Estimation" class="headerlink" title="Pose Estimation"></a>Pose Estimation</h3><ul>
<li><h4 id="Towards-3D-Human-Pose-Estimation-in-the-Wild-A-Weakly-Supervised-Approach"><a href="#Towards-3D-Human-Pose-Estimation-in-the-Wild-A-Weakly-Supervised-Approach" class="headerlink" title="Towards 3D Human Pose Estimation in the Wild: A Weakly-Supervised Approach"></a><a href="http://openaccess.thecvf.com/content_iccv_2017/html/Zhou_Towards_3D_Human_ICCV_2017_paper.html" target="_blank" rel="external">Towards 3D Human Pose Estimation in the Wild: A Weakly-Supervised Approach</a></h4><ul>
<li>We propose a <strong>weakly-supervised transfer learning</strong> method that uses mixed 2D and 3D labels in a uniﬁed deep neutral network that presents two-stage cascaded structure.</li>
</ul>
</li>
</ul>
<h3 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h3><ul>
<li><h4 id="Flow-Guided-Feature-Aggregation-for-Video-Object-Detection"><a href="#Flow-Guided-Feature-Aggregation-for-Video-Object-Detection" class="headerlink" title="Flow-Guided Feature Aggregation for Video Object Detection"></a>Flow-Guided Feature Aggregation for Video Object Detection</h4><ul>
<li>Video object detection</li>
<li>The accuracy of detection suffers from degenerated object appearances in videos, e.g., motion blur, video defocus, rare poses, etc.</li>
<li>We present ﬂow-guided feature aggregation, an accurate and <strong>end-to-end</strong> learning framework for <strong>video object detection.</strong></li>
<li>It leverages <strong>temporal coherence</strong> on feature level instead.</li>
<li>它通过沿着运动路径聚集附近的特征来改进每帧特征，从而提高了视频识别的准确性。</li>
<li>Fast moving objects.</li>
</ul>
</li>
<li><h4 id="DeNet-Scalable-Real-time-Object-Detection-with-Directed-Sparse-Sampling"><a href="#DeNet-Scalable-Real-time-Object-Detection-with-Directed-Sparse-Sampling" class="headerlink" title="DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling"></a>DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling</h4><ul>
<li>We deﬁne the object detection from imagery problem as estimating a very large but <strong>extremely sparse bounding box</strong> dependent probability distribution. （我们将图像问题中的目标检测定义为估计非常大但极其稀疏的边界框相关概率分布。）</li>
<li>Two novelties:<ul>
<li>a <strong>corner based</strong> region-of-interest estimator</li>
<li>a <strong>deconvolution based</strong> CNN model</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Image-Recognition"><a href="#Image-Recognition" class="headerlink" title="Image Recognition"></a>Image Recognition</h3><ul>
<li><h4 id="Multi-label-Image-Recognition-by-Recurrently-Discovering-Attentional-Regions"><a href="#Multi-label-Image-Recognition-by-Recurrently-Discovering-Attentional-Regions" class="headerlink" title="Multi-label Image Recognition by Recurrently Discovering Attentional Regions"></a>Multi-label Image Recognition by Recurrently Discovering Attentional Regions</h4><ul>
<li>Current solutions for this task usually rely on an extra step of extracting hypothesis regions (i.e., region proposals), resulting in <strong>redundant computation</strong> and <strong>sub-optimal performance.</strong></li>
<li>Developing a recurrent memorized-attention module.</li>
<li>This module consists of two alternately performed components:<ul>
<li>a spatial transformer layer to locate attentional regions from the convolutional feature maps in a region-proposal-free way</li>
<li>an LSTM (Long-Short Term Memory) sub-network to sequentially predict semantic labeling scores on the located regions while capturing the global dependencies of these regions.</li>
</ul>
</li>
<li>Despite acknowledged successes, these methods take the redundant computational cost of extracting region proposals and usually over-simplify the contextual dependencies among foreground objects, leading to a sub-optimal performance in complex scenarios.</li>
</ul>
</li>
</ul>
<p>  ​<br>  ​<br>  ​<br>  ​    </p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Visual-object-tracking&quot;&gt;&lt;a href=&quot;#Visual-object-tracking&quot; class=&quot;headerlink&quot; title=&quot;Visual object tracking&quot;&gt;&lt;/a&gt;Visual object tracking&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;h4 id=&quot;Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades&quot;&gt;&lt;a href=&quot;#Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades&quot; class=&quot;headerlink&quot; title=&quot;Learning Policies for Adaptive Tracking with Deep Feature Cascades&quot;&gt;&lt;/a&gt;Learning Policies for Adaptive Tracking with Deep Feature Cascades&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Our fundamental insight is to take an adaptive approach, where easy frames are processed with cheap features (such as pixel values), while challenging frames are processed with invariant but expensive deep features.&lt;/li&gt;
&lt;li&gt;Formulate the adaptive tracking problem as a decision-making process.&lt;/li&gt;
&lt;li&gt;Learn an agent to decide whether to locate objects with high conﬁdence on an early layer, or continue processing subsequent layers of a network.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;Signiﬁcantly reduces the feedforward cost.&lt;/li&gt;
&lt;li&gt;Train the agent ofﬂine in a reinforcement learning fashion.&lt;/li&gt;
&lt;li&gt;Obviously, the major computational burden comes from the forward pass through the entire network, and can be larger with deeper architectures.&lt;/li&gt;
&lt;li&gt;However, when the object is visually distinct or barely moves, early layers are in most scenarios sufﬁcient for precise localization - offering the potential for substantial computational savings.&lt;/li&gt;
&lt;li&gt;The agent learns to ﬁnd the target at each layer, and decides if it is conﬁdent enough to output and stop there.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/blog/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文调研" scheme="http://jacobkong.github.io/blog/tags/%E8%AE%BA%E6%96%87%E8%B0%83%E7%A0%94/"/>
    
  </entry>
  
  <entry>
    <title>行为识别论文笔记：行为分类深度模型的总结.md</title>
    <link href="http://jacobkong.github.io/blog/679115822/"/>
    <id>http://jacobkong.github.io/blog/679115822/</id>
    <published>2017-11-24T07:51:24.000Z</published>
    <updated>2017-11-29T08:42:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次主要总结了目前常见一些经典的基于深度学习的行为分类模型。其中的主要内容来自于论文《Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset》中的Related Work部分的总结。</p>
<a id="more"></a>
<p>虽然近年来图像表示体系结构的发展已经迅速成熟，但视频的前端运行架构仍然不够清晰。当前视频体系结构中的一些主要差异在于convolutional and layers operators是使用2D（基于图像的）还是3D（基于视频的）kernels; 无论网络输入是RGB视频或者还是包含预先计算的光流，在2D ConvNets的情况下，对于信息如何跨帧传播，这可以通过使用诸如LSTM之类的temporally-recurrent layers或者随着时间的推移进行特征聚合来完成。</p>
<p>图1显示了我们评估的五种体系结构的图形概述。<br><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fly3z4y2x1j316f0cxaf1.jpg" alt="图 1. 各种视频架构。其中K表示一个视频中全部的帧数，N表示一段视频相邻帧子集"></p>
<h3 id="模型1：ConvNet-LSTM"><a href="#模型1：ConvNet-LSTM" class="headerlink" title="模型1：ConvNet+LSTM"></a>模型1：ConvNet+LSTM</h3><p>图像分类网络的高性能使得我们尝试只需很少的改变就能将其重用于视频中。 通过使用它们针对每个独立帧提取特征，然后集中在整个视频中来提取预测结果来实现这一目标。 这是bag of words图像建模方法的精神; 但是在实践中使用方便的同时，还存在完全忽略时间结构的问题（例如，模型不能很好的区分开门和关门）。</p>
<p>理论上，更令人满意的方法是向模型添加一个recurrent layer，例如可以对状态进行编码的LSTM，并捕获<strong>时间顺序</strong>和<strong>长程依赖性</strong>。 本文在有512个隐藏单元的Inception-V1的最后的平均池化层之后防止了一个有着BN的LSTM层， 一个全连接层被添加到分类器的顶部。</p>
<p>该模型对所有时间步骤的输出上使用交叉熵损失进行训练。 在测试过程中，我们只考虑最后一帧的输出。</p>
<h3 id="模型2：3D-ConvNets"><a href="#模型2：3D-ConvNets" class="headerlink" title="模型2：3D ConvNets"></a>模型2：3D ConvNets</h3><p>3D ConvNets似乎是一种更自然的视频建模方法，其就像标准的卷积网络一样，但是具有时空卷积核。<strong>它们有一个非常重要的特征</strong>：它们直接创建时空数据的分层表示。这些模型的一个问题是，由于附加的内核维度，它们比2D ConvNets<strong>有更多的参数</strong>，这使得它们<strong>更难以训练</strong>。另外，它们似乎排除了ImageNet预训练的好处，因此以前的工作定义了相对较浅的架构，并且都是train from scratch。基准测试的结果具有提升的前景，但是与最新的技术水平相比还不具有竞争性，使得这种类型的模型成为评估我们大型数据集的好选择。</p>
<p>在本文中，我们实现了一个C3D模型的小变体，它在顶部有8个卷积层，5个池化层和2个完全连接的层。模型的输入与原始实现相同，使用112×112像素共16帧。与原始C3D不同的是，我们在所有卷积和全连接层之后使用了batch normalization。另一个区别在于对于第一个池化层，我们使用stride=2而不是stride=1，这减少了内存占用，并允许更大批量 - 这对于批量标准化非常重要。使用这一步，我们能够使用标准的K40 GPU在每个GPU上每批处理15个视频。</p>
<h3 id="模型3：Two-Stream-Networks"><a href="#模型3：Two-Stream-Networks" class="headerlink" title="模型3：Two-Stream Networks"></a>模型3：Two-Stream Networks</h3><p>来自ConvNets最后层的特征，LSTM可以模拟高层次的变化，但是可能无法捕获在许多情况下非常关键的精细的low-level动作。训练也是昂贵的，因为它需要通过多帧来展开网络以便反向传播。</p>
<p>Simonyan和Zisserman介绍了一种不同的非常实用的方法，在通过两个副本ImageNet预先训练的ConvNet后，通过对来自单个RGB帧的预测和10个外部计算的光流帧的预测进行平均，来对视频的短时间快照进行建模。光流 stream有一个自适应的输入卷积层，输入通道的数量是光流帧的两倍（因为流量有两个通道，水平和垂直），在测试时，多个快照从视频中采样，并对动作预测进行平均。这被证明在现有的基准测试中得到了非常高的性能，同时非常有效地进行训练和测试。</p>
<p>最近的一个扩展[8]将最后一个网络卷积层之后的空间流和光流融合起来，显示出对HMDB的一些改进，同时需要较少的测试时间增量（快照采样）。我们的实现大致使用了Inception-V1。网络的输入是5个连续的RGB帧，相隔10帧，以及相应的光流片段。 Inception-V1（5×7×7特征网格，对应于时间x和y维度）的最后一个平均汇聚层之前的空间和运动特征通过具有512个输出通道的3×3×3的3D卷积层，然后是3×3×3的3D最大池层，并通过最终的完全连接层。这些新层的权重用高斯噪声初始化。</p>
<p>两种模型（原始双流和3D融合版本）都是端对端训练（包括原始模型中的双流平均流程）。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次主要总结了目前常见一些经典的基于深度学习的行为分类模型。其中的主要内容来自于论文《Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset》中的Related Work部分的总结。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/blog/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Action Recognition" scheme="http://jacobkong.github.io/blog/tags/Action-Recognition/"/>
    
      <category term="行为识别" scheme="http://jacobkong.github.io/blog/tags/%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：DSSD</title>
    <link href="http://jacobkong.github.io/blog/2938514597/"/>
    <id>http://jacobkong.github.io/blog/2938514597/</id>
    <published>2017-03-31T07:51:24.000Z</published>
    <updated>2017-04-01T02:16:57.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>本文的主要贡献在于在当前最好的通用目标检测器中加入了额外的上下文信息。</li>
<li>为实现这一目的：我们通过将<strong>ResNet-101</strong>与<strong>SSD</strong>结合。然后，我们用<strong>deconvolution layers</strong>来丰富了SSD + Residual-101，以便在物体检测中引入额外的large-scale的上下文，并提高准确性，<strong>特别是对于小物体</strong>，从而称之为<strong>DSSD</strong>。</li>
<li>我们通过仔细的加入额外的<strong>learned transformations阶段</strong>，具体来说是一个用于在deconvolution中前向传递连接的模块，以及一个新的输出模型，使得这个新的方法变得可行，并为之后的研究提供一个潜在的道路。</li>
<li>我们的DSSD具有513×513的输入，在VOC2007测试中达到81.5％de的mAP，VOC2012测试为80.0％de的mAP，COCO为33.2％的mAP，<strong>在每个数据集上优于最先进的R-FCN</strong> 。</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>最近的一些目标检测方法回归到了<strong>滑动窗口技术</strong>，这种技术随着更强大的整合了深度学习的机器学习框架而回归。</li>
<li>Faster RCNN -&gt; YOLO -&gt; SSD.</li>
<li>回顾最近的这些优秀的目标检测框架，要想提高检测准确率，一个很明显的目标就是：利用更好的特征网络并且添加更多的上下文，特别是对于小物体，另外还要提高边界框预测过程的空间分辨率。</li>
<li>在目标检测之外，最近有一个集成上下文的工作，利用所谓的<strong>“encoder-decoder”网络</strong>。该网络中间的bottleneck layer用于编码关于输入图像的信息，然后逐渐地更大的层将其解码到整个图像的map中。所形成的wide，narrow，wide的网络结构通常被称为沙漏。</li>
<li>但是有必要仔细构建用于集成反卷积的组合模块和输出模块，以在训练期间隔绝ResNet-101层，从而允许有效的学习。</li>
</ul>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul>
<li>SPPnet, Fast R-CNN, Faster R-CNN, R-FCN, YOLO：使用卷积网络的最上面的层来进行不同尺度的物体检测。</li>
<li>通过在ConvNet中开发多层来提高检测精度的方法有多重。<ul>
<li>第一种方法：组合了ConvNet不同层的特征图，并使用组合特征图进行预测。</li>
<li>ION利用L2 normalization来结合多个VGGNet和池化层的特征来进行目标检测。<ul>
<li>HyperNet也是使用类似于ION的方法。</li>
<li>但是这种结合多层特征的方法不仅增加内存，而且降低了模型的速度。</li>
</ul>
</li>
<li>第二种方法：使用ConvNet中的不同层来预测不同尺度的对象。<ul>
<li>因为不同层中的节点具有不同的接收域，所以自然会<strong>从具有大型接收场的层预测大对象，并使用具有小接收场的层来预测小物体。</strong></li>
<li>SSD将不同尺度的默认框扩展到ConvNet中的多个层，并强制执行每一层专注于预测一定规模的对象。</li>
<li>S-CNN [2]在ConvNet的多层应用去卷积，以在使用层去学习region proposal和pool feature之前<strong>增加feature maps的分辨率。</strong></li>
<li>然而，为了很好地检测小物体，这些方法需要从具有小的接收场和密集特征图的浅层中使用一些信息，这可能导致在检测小对象性能较低，因为浅层具有较少的关于对象的语义信息。</li>
<li>通过使用deconvolution layers和skip connections,，我们可以在密集（去卷积）特征图中注入更多的信息，从而有助于预测小物体。</li>
</ul>
</li>
<li>另外还有一个工作方法，尽量去包括预测的上下文信息。<ul>
<li>Multi-Region CNN</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Deconvolutional-DSSD-model-Single-Shot-Detection"><a href="#Deconvolutional-DSSD-model-Single-Shot-Detection" class="headerlink" title="Deconvolutional (DSSD) model Single Shot Detection"></a>Deconvolutional (DSSD) model Single Shot Detection</h3><h3 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h3><p><img src="https://ww1.sinaimg.cn/large/006tNc79gy1fe6cf9kja4j30wd0beta0.jpg" alt=""></p>
<ul>
<li>SSD构建于base network之上，添加了一些逐渐见效的卷积层，如上图蓝色部分。</li>
<li>每个添加的层和一些较早的基本网络层用于预测某些预定义的边界框的分数和偏移。 </li>
<li>这些预测由3x3x＃个通道维数的滤波器执行，一个滤波器用于产生每个类别分数，一个用于回归边界框的每个维度。 </li>
<li>它使用非最大抑制（NMS）对预测进行后处理，以获得最终检测结果。 </li>
</ul>
<h3 id="Using-Residual-101-in-place-of-VGG"><a href="#Using-Residual-101-in-place-of-VGG" class="headerlink" title="Using Residual-101 in place of VGG"></a>Using Residual-101 in place of VGG</h3><p>将Base Network从VGG16换为ResNet-101并未提升结果，但是添加额外的<strong>prediction module</strong>会显著地提成性能。</p>
<h3 id="prediction-module"><a href="#prediction-module" class="headerlink" title="prediction module"></a>prediction module</h3><p><img src="https://ww1.sinaimg.cn/large/006tNc79gy1fe6cpqgt1rj311z0gxtbt.jpg" alt=""></p>
<ul>
<li>在原始SSD中，目标函数直接应用于所选择的特征图，并且由于梯度的大幅度，使用L2标准化层用于conv4 3层。</li>
<li>MS-CNN指出，改进每个任务的子网可以提高准确性，按照这个原则，我们为每个预测层添加一个残差块，如图2变体（c）所示。</li>
<li>我们还尝试了原始SSD方法（a）和具有跳过连接（b）的残余块的版本以及两个顺序的残余块（d）。 我们注意到，<strong>ResNet-101和预测模块似乎显著优于对于较高分辨率输入图像没有预测模块的VGG。</strong></li>
</ul>
<h3 id="Deconvolutional-SSD"><a href="#Deconvolutional-SSD" class="headerlink" title="Deconvolutional SSD"></a>Deconvolutional SSD</h3><p><img src="https://ww2.sinaimg.cn/large/006tNc79gy1fe6cyu3qi7j311j0f9aca.jpg" alt=""></p>
<ul>
<li>为了在检测中包含更多的高层次上下文，我们将prediction module转移到在原始SSD设置之后放置的一系列去卷积层中，有效地制作了非对称沙漏网络结构。</li>
<li>添加额外的去卷积层，以连续增加feature maps layers的分辨率。为了加强特征，我们采用了沙漏模型中“<strong>跳跃连接</strong>”的想法。</li>
<li>尽管沙漏模型在编码器和解码器阶段均包含对称层，但由于两个原因，我们使解码器阶段非常浅。</li>
</ul>
<h3 id="Deconvolution-Module"><a href="#Deconvolution-Module" class="headerlink" title="Deconvolution Module"></a>Deconvolution Module</h3><p><img src="https://ww3.sinaimg.cn/large/006tNc79gy1fe6d85jld2j30ht0g9wfq.jpg" alt=""></p>
<ul>
<li>为了帮助整合早期特征图和去卷积层的信息，我们引入了一个去卷积模块，如图3所示。</li>
<li>首先，在每个卷积层之后添加BN层。</li>
<li>第二，我们使用学习的去卷积层代替双线性上采样。</li>
<li>最后，我们测试不同的组合方法：element-wise sum and element-wise product。</li>
</ul>
<h3 id="Traning"><a href="#Traning" class="headerlink" title="Traning"></a>Traning</h3><ul>
<li>我们遵循与SSD相同的训练政策。</li>
<li>在原始SSD模型中，长宽比为2和3的boxes从实验中证明是有用的。为了了解训练数据（PASCAL VOC 2007和2012年 trainval）中边界框的纵横比，我们以training box运行K-means聚类，以方格平方根为特征。我们从两个集群开始，如果错误可以提高20％以上，就会增加集群的数量。经过试验因此，我们决定在每个预测层添加一个宽高比1.6，并使用（1.6, 2.0, 3.0）。</li>
</ul>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ul>
<li>PASCAL VOC2007 test detection results.</li>
</ul>
<p><img src="https://ww2.sinaimg.cn/large/006tNc79gy1fe6ykpyluhj30s30bgwlo.jpg" alt=""></p>
<ul>
<li>PASCAL 2012 test detection results.</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tNc79gy1fe6yl98p6tj30rp080dka.jpg" alt=""></p>
<ul>
<li>COCO test-dev2015 detection results.</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tNc79gy1fe6ylysfswj30s809ldjq.jpg" alt=""></p>
<ul>
<li>Comparison of Speed &amp; Accuracy on PASCAL VOC2007 test.</li>
</ul>
<p><img src="https://ww1.sinaimg.cn/large/006tNc79gy1fe6ymnt5vsj30rp0cnn1k.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;本文的主要贡献在于在当前最好的通用目标检测器中加入了额外的上下文信息。&lt;/li&gt;
&lt;li&gt;为实现这一目的：我们通过将&lt;strong&gt;ResNet-101&lt;/strong&gt;与&lt;strong&gt;SSD&lt;/strong&gt;结合。然后，我们用&lt;strong&gt;deconvolution layers&lt;/strong&gt;来丰富了SSD + Residual-101，以便在物体检测中引入额外的large-scale的上下文，并提高准确性，&lt;strong&gt;特别是对于小物体&lt;/strong&gt;，从而称之为&lt;strong&gt;DSSD&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;我们通过仔细的加入额外的&lt;strong&gt;learned transformations阶段&lt;/strong&gt;，具体来说是一个用于在deconvolution中前向传递连接的模块，以及一个新的输出模型，使得这个新的方法变得可行，并为之后的研究提供一个潜在的道路。&lt;/li&gt;
&lt;li&gt;我们的DSSD具有513×513的输入，在VOC2007测试中达到81.5％de的mAP，VOC2012测试为80.0％de的mAP，COCO为33.2％的mAP，&lt;strong&gt;在每个数据集上优于最先进的R-FCN&lt;/strong&gt; 。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/blog/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/blog/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/blog/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：Deep Residual Learning for Image Recognition</title>
    <link href="http://jacobkong.github.io/blog/3085218970/"/>
    <id>http://jacobkong.github.io/blog/3085218970/</id>
    <published>2017-03-23T07:51:24.000Z</published>
    <updated>2017-03-28T08:02:39.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>本文是何凯明大神的又一篇CVPR最佳论文。</li>
<li>网络越深越难训练，所以我们提出一个residual learning framework从而减轻网络的训练，该网络比以前使用的网络要深得多。</li>
<li>我们明确地将参考层的输入来作为学习残差函数，而不是学习无参考的函数（unreferenced functions）。</li>
<li>我们提供全面的经验证据，表明这些残留网络更容易优化，并可以从显着增加的深度中获得准确性。</li>
<li>这些残留网络的集合在ImageNet测试集上达到3.57％的误差。 该结果在ILSVRC 2015分类任务中荣获第一名。</li>
<li><strong>深度对于许多CV领域的任务都十分重要的。</strong>由于我们网络很深，我们在COCO对象检测数据集上获得了28％的相对改进。我们还荣获了ImageNet检测，ImageNet定位，COCO检测和COCO分割任务的第一名。</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><p>深层网络自然地将低/中/高层特征和分类器以端到端多层方式进行集成，并且特征的“级别”可以通过堆叠层数（深度）来丰富。<strong>网络的深度有着十分重要的作用。</strong></p>
</li>
<li><p>随着网络深度的增加，带来一个问题：<strong>学习更好的网络是否和堆叠更多的层一样简单？</strong>回答这个问题的障碍是：<strong>逐渐消失的梯度问题</strong>。</p>
</li>
<li><p>当较深的网络能够开始收敛时，暴露了一个退化问题：<strong>随着网络深度的增加，精度饱和，然后迅速下降。</strong>这种下降不是由于过拟合，添加多层会导致更高的训练错误。</p>
</li>
<li><p>从浅到深的一个解决方案：</p>
<ul>
<li>附加层：设置为“恒等”（identity）</li>
<li>原始层：由一个已经学会的较浅模型复制得来。</li>
<li>这种解决方案的存在表明，较深的模型不应该产生比较浅的模型更高的训练误差。至少具有相同的训练误差。</li>
</ul>
</li>
<li><p>优化难题：<strong>随着网络层数不断加深，求解器不能找到解决途径。</strong></p>
</li>
<li><p>为了解决这个问题，本文提出了<strong>深度残差学习框架</strong>。</p>
</li>
<li><p><strong>平原网络</strong>：</p>
<p><img src="https://ww3.sinaimg.cn/large/006tNc79gy1fdyd28cydnj30k706wt99.jpg" alt=""></p>
<p>H(x)是任意一种理想的映射</p>
<p>平原网络希望第2层权重层能够<strong>与H(x)拟合</strong>。</p>
</li>
<li><p><strong>残差网络</strong>：</p>
<p><img src="https://ww3.sinaimg.cn/large/006tNc79gy1fdyd3gt5c5j30ki076dgr.jpg" alt=""></p>
<p>H(x)是任意一种理想的映射</p>
<p>残差网络希望第2类权重层能够与<strong>F(x)拟合使得H(x) = F(x) + x</strong></p>
</li>
<li><p>F(x)是一个残差映射w.r.t 恒</p>
<ul>
<li>如果说恒等是理想，很容易将权重值设定为0；</li>
<li>如果理想化映射更接近于恒等映射，便更容易发现微小波动。</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tNc79gy1fdyd6l6pwhj30kl074my2.jpg" alt=""></p>
</li>
<li><p>我们假设优化残差映射比优化原始的，无参考映射(unreferenced mapping)更容易。在极端情况下，如果一个identity mapping是最佳的，那么将残差推到零比通过一堆非线性层的identity mapping更容易。</p>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;本文是何凯明大神的又一篇CVPR最佳论文。&lt;/li&gt;
&lt;li&gt;网络越深越难训练，所以我们提出一个residual learning framework从而减轻网络的训练，该网络比以前使用的网络要深得多。&lt;/li&gt;
&lt;li&gt;我们明确地将参考层的输入来作为学习残差函数，而不是学习无参考的函数（unreferenced functions）。&lt;/li&gt;
&lt;li&gt;我们提供全面的经验证据，表明这些残留网络更容易优化，并可以从显着增加的深度中获得准确性。&lt;/li&gt;
&lt;li&gt;这些残留网络的集合在ImageNet测试集上达到3.57％的误差。 该结果在ILSVRC 2015分类任务中荣获第一名。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深度对于许多CV领域的任务都十分重要的。&lt;/strong&gt;由于我们网络很深，我们在COCO对象检测数据集上获得了28％的相对改进。我们还荣获了ImageNet检测，ImageNet定位，COCO检测和COCO分割任务的第一名。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/blog/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>目标检测论文笔记：R-FCN</title>
    <link href="http://jacobkong.github.io/blog/3678248031/"/>
    <id>http://jacobkong.github.io/blog/3678248031/</id>
    <published>2017-02-27T07:51:24.000Z</published>
    <updated>2018-05-28T07:35:11.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>提出了一个region-based, fully convolutional的网络来准确高效的进行物体检测。</li>
<li>不同于Fast/Faster R-CNN，其应用了计算成本很高的每个区域子网络数百次，本论文的region-based detector是完全卷积化的，几乎一张图像上所有的计算都是共享的。</li>
<li>为了实现这一目标，我们提出position-sensitive score maps，以解决在图像分类的平移不变性（translation-invariance）和物体检测中的平移可变性（translation-variance）之间的困境。</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>最近流行的用于目标检测的深度学习框架依据RoI层的不同可以分为两大subnetworks：<ul>
<li>一类是独立于RoIs的、共享的、fully convolutional的subnetwork。</li>
<li>另一类是RoI-wise的subnetwork，不共享计算。</li>
</ul>
</li>
<li>在图像分类网络中，一个convolutional subnetwork会以一个sptial pooling layer跟随着几个fully-connected  layer最为结尾，所以图像分类中的sptial pooling layer自然转化为目标检测中的RoI pooling layer。</li>
<li>ResNet和GoogleLeNets都被设计成fully convolutional的。</li>
<li>在ResNet论文中，Faster R-CNN中的RoI pooling layer被不自然的插入到两个卷积层集之间，带来了准确率的提升，<strong>但是速度由于unshared per-RoI计算降低。</strong></li>
<li>对于<strong>图像分类</strong>任务来说：更倾向于平移不变性。对于<strong>图像检测</strong>任务来说：更倾向于<strong>平移变换性</strong>。</li>
<li>假设图像分类网络中更深层的卷积层对translation不敏感，所以为了解决translation invariance和translation variance之间的困难，ResNet将RoI pooling layer插入到了卷积神经网络之间。这个区域特定的操作打破了平移不变性，并且在不同区域之间进行评估时，RoI之后的卷积层不再是平移不变的。</li>
<li>为了将translation variance结合到FCN中，我们通过使用一组专用卷积层作为FCN输出来构造一组位置敏感得分图（position-sensitive score maps）。每一个得分图将相对于相对空间位置（例如，“在对象的左边”）的位置信息进行编码。在这个FCN之上，我们附加一个位置敏感的RoI池层（position-sensitive RoI pooling layer），从这些得分图中获取信息，没有跟随的权重的（卷积/ fc）层。</li>
</ul>
<h2 id="Our-approach"><a href="#Our-approach" class="headerlink" title="Our approach"></a>Our approach</h2><ul>
<li><p>本论文的方法参考R-CNN，也是使用two-stage的目标检测策略。</p>
<ul>
<li>region proposal</li>
<li>region classification</li>
</ul>
</li>
<li><p>虽然不依赖于region proposal的目标检测方法确实存在，如SSD何YOLO，但是region-based system依旧在几个基准上保持领先的准确性。</p>
</li>
<li><p>Overall architecture of R-FCN:</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgy1fd58i0j1icj30ql0dcmzl.jpg" alt=""></p>
<p>用RPN来提出candidate RoIs，然后这些RoIs被应用到score maps，在RPN和R-FCN之间共享特征。</p>
<p>给定一个RoI，R-FCN架构对RoI进行分类（分为物体类别或者背景）。所有可学习权值的层都是卷积层，并且是在整张图片上计算得到的权重。<strong>最后卷积层为每个类别产生一个$k^2$个position-sensitive score maps</strong>，因此具有带有C个对象类别（背景为+1）的$k^2(C + 1)$通道的输出层。</p>
<p><img src="https://ww3.sinaimg.cn/large/006tNc79gy1fdvwup92g7j302p06e0sp.jpg" alt=""></p>
<p><strong>每一个category有一个$k^2$的score map</strong>，对于这里来说k=3，所以最后RoI pooling层产生3x3x(C+1)维的feature map。</p>
</li>
<li><p>RPN以一个position-sensitive RoI pooling layer结束，该层聚合最后卷积层的输出并产生每个RoI的分数。我们的位置敏感的RoI pooling layer进行选择性合并，each of the k × k bin aggregates responses from only one score map out of the bank of k × k score maps。利用端到端训练，这个RoI层管理最后的卷积层以学习专门的position-sensitive score maps。 </p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcly1fd597p3an8j30vs0en40p.jpg" alt=""></p>
</li>
</ul>
<h3 id="Backbone-architecture"><a href="#Backbone-architecture" class="headerlink" title="Backbone architecture"></a>Backbone architecture</h3><ul>
<li>本论文R-FCN基于ResNet-101。</li>
<li>ResNet-101具有100个卷积层，后面是global average pooling和一个1000-class的fc层。我们移去了average pooling layer and the fc layer，仅使用convolutional layer来计算feature maps。</li>
<li>我们使用ResNet-101，在ImageNet上进行预训练，ResNet-101中的最后一个卷积块是2048-d，并且我们附加随机初始化的1024-d 1×1卷积层以减小尺寸。然后，我们应用$k^2(C + 1)$通道卷积层来生成分数图，如下所述。</li>
</ul>
<h3 id="Position-sensitive-score-maps-amp-Position-sensitive-RoI-pooling"><a href="#Position-sensitive-score-maps-amp-Position-sensitive-RoI-pooling" class="headerlink" title="Position-sensitive score maps &amp; Position-sensitive RoI pooling."></a>Position-sensitive score maps &amp; Position-sensitive RoI pooling.</h3><ul>
<li><p>为了将位置信息显式编码到每个RoI中，我们将RoI矩形划分为k x k个bins。</p>
</li>
<li><p>最后的卷积层为每个类别产生的$k^2$个分数图。在第(i, j)个bin内，我们定义了一个位置敏感的RoI池化操作，从而只在第(i, j)个score map上进行池化：</p>
<script type="math/tex; mode=display">
r_c(i, j|\theta)=\sum_{(x,y)\in bin(i,j)}{z_{i,j,c}(x+x_0,y+y_0|\theta)/n}</script><ul>
<li>其中$r_c(i, j)$表示从c-th类别中得到的(i,j)-th bin的池化响应。</li>
<li>$z_{i,j,c}$是$k^2(C+1)$个score maps中的一个score map。</li>
<li>$(x_0,y_0)$表示一个RoI的左上角。</li>
<li>$n$表示这个bin中的像素的数量。</li>
<li>$\theta$表示这个网络中所有的可学习权重。</li>
<li>该pooling属于AVE，也可以用MAX。</li>
</ul>
</li>
<li><p>对每个类别k*k的score map进行平均，最后每个RoI得到一个C+1维的向量。然后求loss，它们用于评估训练期间的交叉熵损失和推理期间的RoIs排名。</p>
</li>
<li><p>bounding boxes regression。对每个RoI产生一个$4k^2$维的向量。然后通过average voting将其聚合成4维向量。</p>
</li>
<li><p>在RoI层之后没有可学习的层次，实现了几乎无成本地区的计算、加速训练和推理。</p>
</li>
</ul>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ul>
<li><p>利用提前计算好的region proposal，很容易来端到端的R-FCN架构训练。</p>
</li>
<li><p>loss function:</p>
<script type="math/tex; mode=display">
L(s,t_{x,y,w,h})=L_{cls}(s_{c^*})+\lambda[c^*>0]L_{reg}(t,t^*)</script></li>
<li><p>本框架很容易在训练的时候使用online hard example mining (OHEM)。</p>
<ul>
<li>我们的per-RoI计算可以进行几乎cost-free的example mining。</li>
<li>在前向传播中：假设每张图片N个proposals。我们计算所有N个proposal的loss，排序，选择最高的B个RoIs。然后在选中的proposal上进行反向传播。</li>
</ul>
</li>
<li><p>decay：0.0005</p>
</li>
<li><p>momentum：0.9</p>
</li>
<li><p>single-scale training。</p>
</li>
<li><p>B=128</p>
</li>
<li><p>lr = 0.001 ~20k, 0.0001 ~ 10k</p>
</li>
<li><p>同Faster R-CNN一趟，使用4步alternating training，在训练RPN和训练R-FCN之间。</p>
</li>
</ul>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul>
<li>为公平期间，在300个RoIs上进行评估，结果之后用NMS进行处理，IoU阈值0.3</li>
</ul>
<h3 id="A-trous-and-stride"><a href="#A-trous-and-stride" class="headerlink" title="À trous and stride"></a>À trous and stride</h3><ul>
<li>将ResNet-10的有效stride从32减为16像素，提高了score map的分辨率。</li>
<li>conv4阶段之前（stride=16）的所有层都没有改变。</li>
<li>第一个conv5块儿的stride从2改为1，并且conv5阶段的卷积核都被改为hole algorithm，（Algorithme à trous）以补偿减少的步幅。</li>
<li>为了公平比较，RPN在conv4之上进行计算。从而RPN不被à trous影响。</li>
</ul>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>R-CNN已经说明了带深度网络的区域候选的有效性。R-CNN计算那些关于裁剪不正常的覆盖区域的卷积网络，并且计算在区域直接是不共享的。SPPnet，Fast R-CNN和Faster R-CNN是半卷积的（semi-convolutional），在卷积子网络中是计算共享的，在另一个子网络是各自计算独立的区域。</p>
<p>物体检测器可以被认为是全卷积模型。OverFeat 检测物体通过在convolutional feature maps上进行多尺度的窗口滑动。 在某些情况下，可以将单精度的滑动窗口改造成一个单层的卷积层。在Faster R-CNN中的RPN组件是一个全卷积检测器，用来预测是一个关于多尺寸的参考边框的实际边框。原始的RPN是class-agnostic（class无关的）。但是对应的class-specific是可应用的。</p>
<p>另一个用于物体检测的是fc layer（fully-connected）用来基于整幅图片的完整物体检测。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;提出了一个region-based, fully convolutional的网络来准确高效的进行物体检测。&lt;/li&gt;
&lt;li&gt;不同于Fast/Faster R-CNN，其应用了计算成本很高的每个区域子网络数百次，本论文的region-based detector是完全卷积化的，几乎一张图像上所有的计算都是共享的。&lt;/li&gt;
&lt;li&gt;为了实现这一目标，我们提出position-sensitive score maps，以解决在图像分类的平移不变性（translation-invariance）和物体检测中的平移可变性（translation-variance）之间的困境。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/blog/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/blog/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/blog/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：SSD</title>
    <link href="http://jacobkong.github.io/blog/3118967289/"/>
    <id>http://jacobkong.github.io/blog/3118967289/</id>
    <published>2017-02-13T09:53:54.000Z</published>
    <updated>2017-02-16T02:30:35.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><ul>
<li>Jaccard overlap, Jaccard similarity:<br>Jaccard coefficient:<script type="math/tex; mode=display">
J(A,B)=\frac{|A\cap B|}{|A\cup B|}</script>A,B分别代表符合某种条件的集合：两个集合交集的大小/两个集合并集的大小，交集=并集意味着2个集合完全重合。<br><strong>所以Jaccard overlap其实就是IoU。</strong><a id="more"></a>
</li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>SSD: 利用单个深度神经网络的目标检测方法。将边界框的输出空间离散化为一组默认框，在每个feature map位置上有着不同的宽高比和尺度。</li>
<li>在预测的时候，网络针对每个默认框中的每个存在的对象类别产生分数，并且对框的进行调整以更好地匹配对象形状。</li>
<li><strong>在多尺度图像处理方面</strong>，网络组合来自具有不同分辨率的多个feature map的预测，以自然地处理各种尺寸的对象。</li>
<li>相比于基于object proposal的方法，SSD是简单地，因为它能够<strong>完全消除</strong>proposal generation和后续的<strong>像素或者特征重冲采样阶段</strong>，所有的计算都封装在单独的网络中。</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>目前的目标检测系统是以下方法的变体：假设边界框（bounding box），对每个框进行像素或特征重取样，采用高质量分类器。</li>
<li>评估速度方法：<strong>SPF (seconds per frame)</strong>.</li>
<li>提出第一个基于深度网络的不需要为BB进行resample pixels or features的目标检测器，并能够同样达到高准确率。</li>
<li>本论文的贡献（具体看论文）：<ul>
<li>引入了SSD。</li>
<li>SSD的核心。</li>
<li>为了实现高检测准确率，引入了在不同尺度和横纵比的feature maps上进行预测。</li>
<li>End-to-end training 以及高准确率，机试在低分辨率图片。</li>
<li>在PASCAL VOC、COCO和ILSVRC上进行试验，具有很强的竞争力。</li>
</ul>
</li>
</ul>
<h2 id="The-Single-Shot-Detector-SSD"><a href="#The-Single-Shot-Detector-SSD" class="headerlink" title="The Single Shot Detector (SSD)"></a>The Single Shot Detector (SSD)</h2><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><ul>
<li>基于前向卷积神经网络，产生固定尺寸的BB集，以及这些BB中存在物体的分数，之后跟随者一个非极大值抑制步骤来产生最终检测。</li>
<li><p>网络前面的几层是基于标准的用于产生高质量图像分类的架构，我们成为<strong>基础网络</strong>。我们给网络然后添加了<strong>辅助的结构</strong>来产生检测结构。辅助网络具备以下关键特征：</p>
<ul>
<li><p><strong>用于检测的多尺寸特征图。</strong>在基础网络后面<strong>添加额外几个卷积层</strong>，在尺寸上逐层递减，从而能够在不同尺寸上检测。（Overfeat和YOLO都只是在单独尺寸的feature map上进行操作。）</p>
</li>
<li><p><strong>用来预测的卷积预测器</strong>（Convolutional predictors）。</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgy1fcq4bw59jtj30j109i3zs.jpg" alt=""></p>
</li>
<li><p><strong>默认的boxes和aspect ratios。</strong>我们将一组默认边界框与每个feature map单元关联，用于网络顶部的多个特征映射。在每个feature map单元格中，我们预测相对于单元格中的默认框形状的偏移，以及指示每个框中存在类实例的每类分数。（本论文中的default boxes类似于Faster R-CNN中的anchor boxes，然而我们将他用于不同分辨率的几个feature maps中）</p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgy1fcp21vqfpgj30le0ey0xb.jpg" alt=""></p>
</li>
</ul>
</li>
</ul>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ul>
<li><p>训练SSD和训练使用region proposals的典型检测器之间的<strong>关键区别是</strong>：ground truth信息需要分配给固定的检测器输出集合中的特定输出。</p>
</li>
<li><p>训练涉及到：</p>
<ul>
<li>choosing the set of default boxes and scales for detection。</li>
<li>hard negative mining。</li>
<li>data augmentation strategies（数据增加策略）。</li>
</ul>
</li>
</ul>
<h4 id="Matching-strategy"><a href="#Matching-strategy" class="headerlink" title="Matching strategy"></a><strong>Matching strategy</strong></h4><p>在训练过程中，对于每一个ground truth，我们都从默认框中选择每个不同的位置、aspect ratio、scale的bounding boxes。首先匹配最好的jaccard overlap的default box（类似于MultiBox），但与MultiBox不同的是，我们然后匹配default box与任何ground truth，只要jaccard overlap高于阈值（0.5）。<strong>这样简化了学习问题。</strong></p>
<h4 id="Training-objective"><a href="#Training-objective" class="headerlink" title="Training objective"></a><strong>Training objective</strong></h4><p>整体的代价函数是localization loss和confidence loss之和：</p>
<script type="math/tex; mode=display">
L(x,c,l,g)=\frac{1}{N}(L_{conf}(x,c)+\alpha L_{loc}(x,l,g))</script><ul>
<li><p>N是匹配的default boxes的数量，N=0时，loss=0。</p>
</li>
<li><p>localization loss是predicted box和ground truth box之间的<strong>Smooth L1 loss</strong>（类似于Faster R-CNN）。我们预测default box的中心$(cx,cy)$，以及宽度$(w)$和长度$(h)$。</p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcjw1fcqyi2ogdnj30bj03l3yr.jpg" alt=""></p>
</li>
<li><p>confidence loss是多个类confidence$(c)$之间的<strong>softmax loss</strong>。</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgy1fcqyjitcw7j30eh01kjrg.jpg" alt=""></p>
</li>
<li><p>权值$\alpha$通过交叉验证设为1。</p>
</li>
</ul>
<h4 id="Choosing-scales-and-aspect-ratios-for-default-boxes"><a href="#Choosing-scales-and-aspect-ratios-for-default-boxes" class="headerlink" title="Choosing scales and aspect ratios for default boxes"></a>Choosing scales and aspect ratios for default boxes</h4><ul>
<li><p>不同于将照片处理为不同尺寸再结合结果的方法，本论文通过利用单个神经网络中不同层的feature maps，可以达到同样的效果，同时可以在所有尺寸中共享权值。</p>
</li>
<li><p><strong>利用较低层的feature maps可以提高semantic segmentation质量，应为较低层往往可以捕捉到更精细的细节。</strong></p>
</li>
<li><p>我们同时使用较低和较高层的feature maps来进行检测。</p>
</li>
<li><p>网络中不同层的feature maps有着不同的接受域的尺寸。</p>
</li>
<li><p>假设我们想要使用m个feature maps用来检测，则每个feature map的default boxes的scale可以这样计算：</p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcjw1fcr0sa2183j308k0190sn.jpg" alt=""></p>
</li>
<li><p>通过结合在许多feature maps上所有位置上的所有的有着不同scale和aspect ratio的default boxes，我们可以产生对不同物体大小和形状的各种预测。</p>
</li>
<li><p>如下图中，狗在8x8的feature map中没有匹配的default box，因此在训练中会被作为负样本，但是在4x4的feature map中有着匹配的feature map。</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcjw1fcr148r96xj30e70563zd.jpg" alt=""></p>
</li>
</ul>
<h4 id="Hard-negative-mining"><a href="#Hard-negative-mining" class="headerlink" title="Hard negative mining"></a>Hard negative mining</h4><ul>
<li>通过匹配阶段后，default boxes中会产生大量的negatives，尤其是当可能的default boxes数量非常大时。<strong>这导致positive和negative时间严重的不平衡。</strong></li>
<li>我们将negative examples的default boxes通过其最高的confidence loss进行排序，然后选择较高的几个，使negative examples和positive examples之间的比例保持在3:1之间。<strong>这样会更快的优化和更稳定的训练。</strong></li>
</ul>
<h2 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h2><ul>
<li>所有的实验都是基于VGG16。</li>
<li>将fc6和fc7转化为卷积层，从fc6和fc7中取样子参数。</li>
<li>将pool5从2x2-s2转化为3x3-s1。</li>
<li>使用a trous algorithm来填补“holes”。</li>
<li>移去了所有的dropout层和fc8层。</li>
<li>用SGD进行微调。</li>
<li>学习率$10^{-3}$，动量0.9，weight decay是0.0005，batch size是32.</li>
</ul>
<h3 id="PASCAL-VOC2007"><a href="#PASCAL-VOC2007" class="headerlink" title="PASCAL VOC2007"></a>PASCAL VOC2007</h3><ul>
<li><strong>“xavier” method</strong>来初始化新加入的层的参数。</li>
<li>通过detection analysis tool分析后，显示SSD有着<strong>更少的localization错误</strong>，因为其能够直接去学习regress物体的形状，并分类，而非使用两个互相解耦的步骤。</li>
<li>然而，SSD对于相似的物体会有更多的混淆，特别是animals，一部分原因。</li>
<li>SSD对bounding boxes尺寸是十分敏感的。提升输入尺寸可能会提升小物体检测，但依旧有许多空间提升。</li>
</ul>
<h3 id="Model-analysis"><a href="#Model-analysis" class="headerlink" title="Model analysis"></a>Model analysis</h3><ul>
<li>Data augmentation很重要。</li>
<li>更多的default boxes的形状可能会更好。</li>
<li>Atrous is faster：如果不使用更改后的VGG-16，虽然结果一样，但是速度回降低20%。</li>
<li>不同分辨率中多个输出层会更好。<strong>SSD的主要贡献</strong>是在不同输出层上使用不同尺度的默认框。</li>
</ul>
<h3 id="PASCAL-VOC2012"><a href="#PASCAL-VOC2012" class="headerlink" title="PASCAL VOC2012"></a>PASCAL VOC2012</h3><ul>
<li>和PASCAL VOC2007一样的实验设置。</li>
<li>在2012 trainval+2007 trainval+2007 test上进行训练，在2012 test上进行测试。</li>
</ul>
<h3 id="COCO"><a href="#COCO" class="headerlink" title="COCO"></a>COCO</h3><ul>
<li>COCO中的物体比PASCAL VOC中的物体小，所以我们在所有层上使用更小的default boxes</li>
</ul>
<h3 id="Data-Augmentation-for-Small-Object-Accuracy"><a href="#Data-Augmentation-for-Small-Object-Accuracy" class="headerlink" title="Data Augmentation for Small Object Accuracy"></a>Data Augmentation for Small Object Accuracy</h3><ul>
<li>对SSD来说，对小物体分类的任务相对Faster R-CNN来说会更难。</li>
<li>Data augmentation对于特高性能是十分显著的，尤其是小数据及。</li>
<li>改进SSD的另一种方法是设计更好的平铺默认框（tiling of default boxes），使其位置和尺度更好地与特征图上每个位置的接收场对准。</li>
</ul>
<h3 id="Inference-time"><a href="#Inference-time" class="headerlink" title="Inference time"></a>Inference time</h3><ul>
<li>考虑到从我们的方法生成的大量框，有必要在推理期间有效地执行非最大抑制（nms）。</li>
<li>通过使用0.01的限制阈值，我们可以过滤大多数bounding boxes。 然后我们应用nms，每个类别的jaccard重叠0.45，并保持每个图像前200个检测。</li>
<li>80%的前向传递时间被花费在了base network，所以使用一个更快的base network可以提高速度。</li>
</ul>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul>
<li>有两种已建立的用于图像中的对象检测的方法类别，<strong>一种基于滑动窗口，另一种基于region proposal classification。</strong></li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>我们模型的一个<strong>关键特性</strong>是使用多尺度卷积边界框输出附加到网络顶部的多个特征图。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;知识点&quot;&gt;&lt;a href=&quot;#知识点&quot; class=&quot;headerlink&quot; title=&quot;知识点&quot;&gt;&lt;/a&gt;知识点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Jaccard overlap, Jaccard similarity:&lt;br&gt;Jaccard coefficient:&lt;script type=&quot;math/tex; mode=display&quot;&gt;
J(A,B)=\frac{|A\cap B|}{|A\cup B|}&lt;/script&gt;A,B分别代表符合某种条件的集合：两个集合交集的大小/两个集合并集的大小，交集=并集意味着2个集合完全重合。&lt;br&gt;&lt;strong&gt;所以Jaccard overlap其实就是IoU。&lt;/strong&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/blog/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/blog/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/blog/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：YOLO9000</title>
    <link href="http://jacobkong.github.io/blog/2102833929/"/>
    <id>http://jacobkong.github.io/blog/2102833929/</id>
    <published>2017-02-08T03:56:20.000Z</published>
    <updated>2018-05-18T04:04:53.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>YOLO9000: a state-of-the-art, real-time 的目标检测系统，可以检测超过9000种的物体分类。</li>
<li>本论文提出两个模型，<strong>YOLOv2和YOLO9000</strong>。</li>
<li>YOLOv2：<ul>
<li>是对YOLO改进后的提升模型。</li>
<li>利用新颖的，多尺度训练的方法，YOLOv2模型可以在多种尺度上运行，在速度与准确性上更容易去trade off。</li>
</ul>
</li>
<li>YOLO9000：<ul>
<li>是提出的一种联合在检测和分类数据集上训练的模型，<strong>这种联合训练的方法使得YOLO9000能够为没有标签的检测数据目标类预测</strong>。</li>
<li>可以检测超过9000个类。</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>目前，许多检测方法依旧约束在很小的物体集上。</li>
<li>目前，目标检测数据集相比于用于分类和标注的数据集来说，是有限制的。<ul>
<li>最常见的检测数据集包含数十到数十万的图像，具有几十到几百个标签，比如Pascal、CoCo、ImageNet。 </li>
<li>分类数据集具有数以百万计的图像，具有数万或数十万种类别，如ImageNet。</li>
</ul>
</li>
<li>目标检测数据集永远不会达到和分类数据集一样的等级。</li>
<li>本论文提出一种方法，利用分类数据集来作为检测数据集，将两种截然不同的数据集结合。</li>
<li>本论文提出一个在目标检测和分类数据集上联合训练的方法。<strong>此方法利用标记的检测图像学习精确定位对象，而它使用分类图像增加其词汇和鲁棒性。</strong></li>
</ul>
<h2 id="Better"><a href="#Better" class="headerlink" title="Better"></a>Better</h2><ul>
<li><p>YOLO产生很多的定位错误。而且YOLO相比于region proposal-based方法有着相对较低的recall（查全率）。<strong>所以主要任务是在保持分类准确率的前提下，提高recall和减少定位错误。</strong></p>
</li>
<li><p>我们从过去的工作中融合了我们自己的各种新想法，以提高YOLO的性能。 结果的摘要可以在表中找到：</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcjw1fcizngrvjoj30eq05paas.jpg" alt=""></p>
</li>
</ul>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a><strong>Batch Normalization</strong></h3><ul>
<li>得到2%的mAP的提升，使用Batch Normalization，我们可以从模型中删除dropout，而不会出现过度缺陷。</li>
</ul>
<h3 id="High-Resolution-Classiﬁer"><a href="#High-Resolution-Classiﬁer" class="headerlink" title="High Resolution Classiﬁer"></a><strong>High Resolution Classiﬁer</strong></h3><ul>
<li>对于YOLOv2，我们首先在ImageNet对全部448×448分辨率图像上进行10epochs的微调来调整分类网络。  然后我们在检测时调整resulting network。 这种高分辨率分类网络使我们增加了近4％的mAP。</li>
</ul>
<h3 id="Convolutional-With-Anchor-Boxes"><a href="#Convolutional-With-Anchor-Boxes" class="headerlink" title="Convolutional With Anchor Boxes"></a><strong>Convolutional With Anchor Boxes</strong></h3><ul>
<li>YOLO利用卷积特征提取器最顶端的全连接层来直接预测BB的坐标。而Faster R-CNN是利用首选的priors来预测BB。</li>
<li>预测BB的偏移而不是坐标可以简化问题，并使网络更容易学习。本论文从YOLO中移去了全连接层，并且利用anchor box来预测BB。</li>
<li>我们移去了pooling层，使得网络的卷积层的输出有更高的像素。</li>
<li>同时将网络缩减到在416*416像素的图片上操作。 我们这样做是因为我们想要特征图中具有奇数个位置，因此存在单个中心单元。</li>
<li>当我们移动到anchor boxes时，我们也将class prediction机制与空间位置解耦，而是为每个anchor box预测的类和对象。 同YOLO一样，objectness prediction仍然预测ground truth和所提出的框的IOU，并且class predictions预测该类的条件概率，假定存在对象。(没太懂)</li>
</ul>
<h3 id="Dimension-Clusters"><a href="#Dimension-Clusters" class="headerlink" title="Dimension Clusters"></a><strong>Dimension Clusters</strong></h3><ul>
<li><p>将YOLO与anchor boxes结合有两个问题，<strong>第一个是anchor box的长宽是认为选定的。</strong></p>
</li>
<li><p>我们不是手动选择先验（priors），而是在训练集边界框上运行k-means聚类，以自动找到好的先验。</p>
<ul>
<li><p>我们真正想要的是导致良好的IOU分数的priors，这是独立于盒子的大小。 因此，对于我们的distance metric，我们使用：</p>
<script type="math/tex; mode=display">
d(box, centroid) = 1-IOU(box,centroid)</script></li>
<li><p>我们选择<strong>k = 5</strong>作为模型复杂性和高召回率之间的良好权衡。这样非常不同于相比于人工选择的boxes。更多的又高又瘦的boxes。</p>
</li>
</ul>
</li>
</ul>
<h3 id="Direct-location-prediction"><a href="#Direct-location-prediction" class="headerlink" title="Direct location prediction"></a><strong>Direct location prediction</strong></h3><ul>
<li><p>将YOLO与anchor boxes结合有两个问题，<strong>第二个模型不稳定，特别是在早期迭代中。</strong></p>
</li>
<li><p>并非预测偏移，我们遵循YOLO的方法并预测相对于网格单元的位置的位置坐标。 这将ground truth限制在0和1之间。我们使用<strong>逻辑激活</strong>来约束网络的预测落在该范围内。</p>
</li>
<li><p>网络为每一个BB预测5个坐标：$t_x, t_y, t_w, t_h, t_o$.</p>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcjw1fcjb6g0tdpj309203r0st.jpg" alt="">  <img src="https://ww1.sinaimg.cn/large/006tKfTcjw1fcjbl2jwc6j30bj09k3z4.jpg" alt=""></p>
</li>
<li><p>结合Dimension Clusters和Direct location prediction，YOLO提升5%的mAP。</p>
</li>
</ul>
<h3 id="Fine-Grained-Features"><a href="#Fine-Grained-Features" class="headerlink" title="Fine-Grained Features"></a><strong>Fine-Grained Features</strong></h3><ul>
<li>修改后的YOLO在13<em>13的feature map上进行检测。 虽然这对于大对象是足够的，但是它可以从用于定位较小对象的<em>*细粒度特征</em></em>中受益。</li>
<li>添加一个传递层，将分辨率从前面的层变为从26 x 26分辨率。</li>
</ul>
<h3 id="Multi-Scale-Training"><a href="#Multi-Scale-Training" class="headerlink" title="Multi-Scale Training"></a><strong>Multi-Scale Training</strong></h3><ul>
<li>我们希望YOLOv2可以足够鲁邦在不同尺寸的images上进行训练。</li>
<li>并非使用固定的输入图像尺寸，我们在每几次迭代后改变网络。每10batches，我们的网络随机选择一个新的图像尺寸。</li>
</ul>
<h2 id="Faster"><a href="#Faster" class="headerlink" title="Faster"></a>Faster</h2><ul>
<li>大多数检测框架依赖VGG-16作为基本特征提取器。VGG-16是一个强大、准确的分类网络，但是也很复杂。</li>
<li>YOLO框架使用的基于Googlenet架构的修改后的网络。比VGG-16快速，但是准确性比VGG-16稍差。</li>
</ul>
<h3 id="Darknet-19"><a href="#Darknet-19" class="headerlink" title="Darknet-19"></a>Darknet-19</h3><ul>
<li><p>供YOLOv2使用的新的分类模型。</p>
</li>
<li><p>最终模型叫做darknet-19，有着19个卷积层和5个maxpooling层。</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcjw1fck21bicicj308j0byabg.jpg" alt=""></p>
</li>
<li><p>Darknet-19处理每张图片只需要5.58 billion的操作。</p>
</li>
</ul>
<h3 id="Training-for-classification"><a href="#Training-for-classification" class="headerlink" title="Training for classification"></a>Training for classification</h3><ul>
<li>在标准的ImageNet 1000类的数据集上利用随机梯度下降训练160 epochs。开始学习率为0.1，polynomial rate decay 是4，weight decay是0.0005，动量是0.9。</li>
<li>在训练期间，我们使用标准的数据增加技巧，包括随机裁剪，旋转，以及色调，饱和度和曝光偏移。</li>
<li>在224x224分辨率的图像上进行预训练，然后在448x448分辨率的图像上进行微调。</li>
</ul>
<h3 id="Training-for-detection"><a href="#Training-for-detection" class="headerlink" title="Training for detection"></a>Training for detection</h3><ul>
<li>我们通过去除最后的卷积层来修改这个网络，并且替代地增加具有1024个滤波器的三个3×3卷积层，每个跟随着具有我们需要检测所需的输出数量的最后的1×1卷积层。</li>
<li><strong>passthrough层</strong>的添加：使网络能够使用fine grain feature。</li>
</ul>
<h2 id="Stronger"><a href="#Stronger" class="headerlink" title="Stronger"></a>Stronger</h2><ul>
<li>本论文提出一种机制，用来将分类和检测数据结合起来再一起训练。<ul>
<li>在训练过程中，当看到用于检测的被标注的图片，我们会使用基于YOLOv2的代价函数进行反向传播。</li>
<li>在训练过程中，当看到分类图片，我们只从框架中用来分类部分来传递损失。</li>
</ul>
</li>
<li>这种方法的challenge：<ul>
<li>检测数据集中的标签是大分类，而分类数据集的标签是小分类，<strong>所以我们需要找一个方法来融合这些标签</strong>。</li>
<li>用来分类的许多方法都是使用softmax层来计算最后的概率分布，<strong>使用softmax层会假设类之间是互斥的</strong>，但是如何用本方法融合数据集，类之间本身不是互斥的。</li>
</ul>
</li>
<li>我们所以使用multi-label模型来结合数据集，不假设类之间互斥。<strong>这种方法忽略了我们已知的数据的结构。</strong></li>
</ul>
<h3 id="Hierarchical-classification"><a href="#Hierarchical-classification" class="headerlink" title="Hierarchical classification"></a>Hierarchical classification</h3><ul>
<li>ImageNet标签是从<strong>WordNet</strong>中得来，<strong>一种结构化概念和标签之间如何联系的语言数据库</strong>。</li>
<li>WordNet是<strong>连接图结构</strong>，而非树。我们相反并不实用整个图结构，我们将问题简化成从ImageNet的概念中构建有结构的树。</li>
<li>WordTree</li>
</ul>
<h3 id="Dataset-combination-with-WordTree"><a href="#Dataset-combination-with-WordTree" class="headerlink" title="Dataset combination with WordTree"></a>Dataset combination with WordTree</h3><ul>
<li><p>我们可以使用WordTree来介个数据集。</p>
</li>
<li><p>将数据集中分类映射成树中的下义词。</p>
</li>
<li><p>举例：将ImageNet和COCO数据集结合：</p>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcgy1fckj905lbrj30bm0co75r.jpg" alt=""></p>
</li>
<li><p>WordNet十分多样化，所以我们可以利用这种技术到大多数数据集。</p>
</li>
</ul>
<h3 id="Joint-classiﬁcation-and-detection"><a href="#Joint-classiﬁcation-and-detection" class="headerlink" title="Joint classiﬁcation and detection"></a>Joint classiﬁcation and detection</h3><ul>
<li>将COCO数据集和ImageNet数据集结合，训练处一个特别大规模的检测器。</li>
<li>对应的WordTree有9418个类。</li>
<li>ImageNet是一个更大的数据集，因此我们通过对COCO进行过采样来平衡数据集，使ImageNet只以4：1的倍数来增大。</li>
<li>当我们的网络看见一张用来检测的图片，我们正常反向传播loss。对于分类loss，我们只在该label对应层次之上反向传播loss。<strong>比如：如果标签是“dog”，我们会在树中的“German Shepherd”和“Golden Retriever”中进一步预测错误，因为我们没有这些信息。</strong></li>
<li>当我们的网络看见一张用来分来的照片，我们只反向传递分类loss。</li>
<li>使用这种联合训练，YOLO 9000使用COCO中的检测数据学习找到图像中的对象，并使用ImageNet中的数据学习分类各种各样的对象。</li>
<li>在ImageNet上利用YOLO9000来做detection，从而进行评估。ImageNet和COCO只有44个相同的类分类，意味着YOLO9000在利用部分监督来进行检测。</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>本论文提出两个模型，<strong>YOLOv2和YOLO9000</strong>。<ul>
<li>YOLOv2：是对YOLO改进后的提升模型。更快更先进。此外，它可以以各种图像大小运行，以提供速度和精度之间的权衡。</li>
<li>YOLO9000：是提出的一种联合在检测和分类数据集上训练的模型，可以为没有任何标注检测标签的数据进行检测。可以检测超过9000个类。使用WordTree技术来组合不同来源的数据。</li>
</ul>
</li>
<li>我们创造出许多目标检测之外的技术：<ul>
<li>WordTree representation.</li>
<li>Dataset combination.</li>
<li>Multi-scale training.</li>
</ul>
</li>
<li>下一步工作：我们希望利用相似的技术来进行weakly supervised image segmentation.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;YOLO9000: a state-of-the-art, real-time 的目标检测系统，可以检测超过9000种的物体分类。&lt;/li&gt;
&lt;li&gt;本论文提出两个模型，&lt;strong&gt;YOLOv2和YOLO9000&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;YOLOv2：&lt;ul&gt;
&lt;li&gt;是对YOLO改进后的提升模型。&lt;/li&gt;
&lt;li&gt;利用新颖的，多尺度训练的方法，YOLOv2模型可以在多种尺度上运行，在速度与准确性上更容易去trade off。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;YOLO9000：&lt;ul&gt;
&lt;li&gt;是提出的一种联合在检测和分类数据集上训练的模型，&lt;strong&gt;这种联合训练的方法使得YOLO9000能够为没有标签的检测数据目标类预测&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;可以检测超过9000个类。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/blog/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/blog/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/blog/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：YOLO</title>
    <link href="http://jacobkong.github.io/blog/2094641206/"/>
    <id>http://jacobkong.github.io/blog/2094641206/</id>
    <published>2017-02-02T11:49:53.000Z</published>
    <updated>2017-02-08T03:57:58.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>之前的物体检测的方法是使用分类器来进行检测。</li>
<li>相反，本论文将对象检测作为空间分离的边界框和相关类概率的回归问题。</li>
<li>本论文的YOLO模型能达到45fps的实时图像处理效果。</li>
<li>Fast YOLO：小型的网络版本，可达到155fps。</li>
<li>与目前的检测系统相比，YOLO会产生更多的定位错误，但是会更少的去在背景中产生false positive。</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><p>DPM: use a sliding window approach where the classiﬁer is run at evenly spaced locations over the entire image.</p>
</li>
<li><p>R-CNN: use region proposal methods to ﬁrst generate potential bounding boxes in an image and then run a classiﬁer on these proposed boxes. 具有<strong>slow</strong>和<strong>hard to optimize</strong>的缺点。</p>
</li>
<li><p>本论文将目标检测问题重新组织成<strong>single regression problem</strong>. 从图像像素转为<strong>bounding box coordinates</strong>和<strong>class probabilities</strong>.</p>
</li>
<li><p>YOLO框架：</p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgy1fcfslf8g7bj30bs06b756.jpg" alt=""></p>
<ul>
<li>A <strong>single convolutional network</strong> simultaneously predicts multiple bounding boxes and class probabilities for those boxes.</li>
<li>YOLO trains on full images and directly optimizes detection performance.</li>
</ul>
</li>
<li><p>YOLO模型的优势：</p>
<ul>
<li>First, YOLO is extremely <strong>fast</strong>.<ul>
<li>regression problem.</li>
<li>no batch processing on a Titan X.</li>
</ul>
</li>
<li>Second, YOLO reasons globally about the image when making predictions.<ul>
<li>YOLO makes <strong>less than half</strong> the number of background errors compared to Fast R-CNN.</li>
</ul>
</li>
<li>Third, YOLO learns <strong>generalizable representations</strong> of objects.</li>
</ul>
</li>
<li><p>YOLO在准确性方面依旧落后与其他先进的检测系统，但是可以快速的标注图片中的物体，特别是小物体。</p>
</li>
</ul>
<h2 id="Unified-Detection"><a href="#Unified-Detection" class="headerlink" title="Unified Detection"></a>Unified Detection</h2><ul>
<li><p>本论文将物体检测中单独的组件统一到一个单一的神经网络中。网络利用整个图像的各个特征来预测每一个BB。而且同时为一张图片中所有的类预测所用的BB。</p>
</li>
<li><p>YOLO可以<strong>end-to-end</strong>来训练，而且能在保持高平均准确率的同时达到<strong>实时要求</strong>。</p>
</li>
<li><p>系统将输入图片分为$S*S$的网格单元。如果物体的中心落入某个格子，那么这个格子将会用来检测这个物体。</p>
</li>
<li><p>每个网格单元会预测<strong>B</strong>个bounding box以及这些框的置信值。</p>
</li>
<li><p>每个bounding box会有5个预测值：$x,y,w,h$和置信值confidence，$confidence = Pr(Object)*IOU^{truth}_{pred}$.</p>
</li>
<li><p>每个网格单元也预测<strong>C</strong>个条件类概率，$Pr(Class_i|Object)$，<strong>在一个网格单元包含一个物体的前提下，它属于某个类的概率</strong>。我们只为每个网格单元预测一组类概率，而不考虑框B的数量。</p>
</li>
<li><p>在测试的时候，通过如下公式来给出对某一个box来说某一类的confidence score：</p>
<script type="math/tex; mode=display">
Pr(Class_{i}|Object)*Pr(Object)*IOU^{truth}_{pred}=Pr(Class_{i})*IOU^{truth}_{pred}</script></li>
<li><p>Model示例：</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcjw1fcgkqg2fdyj309608lgmg.jpg" alt=""></p>
<p>每个grid cell预测B个bounding boxes，每个框的confidence和C个类概率。</p>
</li>
</ul>
<h3 id="Network-Design"><a href="#Network-Design" class="headerlink" title="Network Design"></a>Network Design</h3><ul>
<li><p>YOLO网络结构图：</p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgy1fcgkv58b9lj30oc0aota4.jpg" alt=""></p>
</li>
<li><p>起初的<strong>卷积层</strong>用来从图像中提取特征。</p>
</li>
<li><p>全连接层用来预测输出的概率和坐标。</p>
</li>
<li><p>24个卷积层，之后跟着2个全连接层</p>
</li>
<li><p>最终输出是7 x 7 x 30的张量。</p>
</li>
<li><p>Fast YOLO和YOLO之间所有的训练和测试参数一样。</p>
</li>
<li><p>在ImageNet上进行卷积层的预训练。</p>
</li>
</ul>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ul>
<li><p>在ImageNet上预训练卷积层。预训练前20层卷积层，之后跟随者一个average-pooling layer和一个fully connected layer.</p>
</li>
<li><p>将预训练的模型用来检测，<strong>论文Ren et al.显示给与训练好的模型添加卷积和连接层能够提高性能。</strong>所以添加了<strong>额外的4个卷积层和2个全连接层</strong>，其权值随机初始化。</p>
</li>
<li><p>将像素从224x224提升到448x448。</p>
</li>
<li><p>最后一层同时预测class probabilities和bounding box coordinates. 其中涉及到BB的长宽规范化。</p>
</li>
<li><p>由于sum-squared error的缺点，增加边界框坐标预测的损失，并减少对不包含对象的框的置信度预测的损失。</p>
</li>
<li><p>large boxes中的偏差matter less than 与small boxes中的偏差。</p>
</li>
<li><p>YOLO为每一个网格单元预测多个BB，但是在测试期间，我们只想每一个物体有一个BB预测框来做响应，我们选择具有最高IOU的BB来作为响应框。</p>
</li>
<li><p>总的<strong>loss function</strong>:</p>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcgy1fch1eth6atj30do08hdgg.jpg" alt=""></p>
</li>
<li><p>135 epochs</p>
</li>
<li><p>batch size：64</p>
</li>
<li><p>动量：0.9</p>
</li>
<li><p>decay：0.0005</p>
</li>
<li><p>为防止过拟合，我们使用dropout和extensive data augmentation技术。</p>
</li>
</ul>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul>
<li>在测试图像中预测检测只需要一个网络评估，与一般的classifier-based methods不同。</li>
<li>Non-maximal suppression可以用来修复multiple detections。</li>
</ul>
<h2 id="Comparison-to-Other-Detection-Systems"><a href="#Comparison-to-Other-Detection-Systems" class="headerlink" title="Comparison to Other Detection Systems"></a>Comparison to Other Detection Systems</h2><ul>
<li>检测流水线往往开始于提取健壮特征集（Haar, SIFT, HOG, convolutional features）,然后分类器或者定位器用来识别特征空间的物体，这些分类器或者定位器往往在整个图像上或者在图像的子区域中滑动窗口。</li>
<li>与DPM的比较。</li>
<li>与R-CNN的比较。每个图片值预测98个bounding boxes。</li>
<li>与其他快速检测器的比较。相比于单类检测器，YOLO可以同时检测多种物体。</li>
<li>与Deep MultiBox的比较。YOLO是一个完整的检测系统。</li>
<li>与OverFeat的比较。OverFeat是一个disjoint的系统，OverFeat优化定位，而非检测性能。需要大量的后处理。</li>
<li>与MultiGrasp的比较。执行比目标检测更简单的任务。</li>
</ul>
<h3 id=""><a href="#" class="headerlink" title=" "></a> </h3>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;之前的物体检测的方法是使用分类器来进行检测。&lt;/li&gt;
&lt;li&gt;相反，本论文将对象检测作为空间分离的边界框和相关类概率的回归问题。&lt;/li&gt;
&lt;li&gt;本论文的YOLO模型能达到45fps的实时图像处理效果。&lt;/li&gt;
&lt;li&gt;Fast YOLO：小型的网络版本，可达到155fps。&lt;/li&gt;
&lt;li&gt;与目前的检测系统相比，YOLO会产生更多的定位错误，但是会更少的去在背景中产生false positive。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/blog/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/blog/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/blog/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习实践经验：用Faster R-CNN训练Caltech数据集——训练检测</title>
    <link href="http://jacobkong.github.io/blog/464905881/"/>
    <id>http://jacobkong.github.io/blog/464905881/</id>
    <published>2017-01-16T22:32:24.000Z</published>
    <updated>2017-02-25T16:22:18.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前面已经介绍了如何准备数据集，以及如何修改数据集读写接口来操作数据集，接下来我来说明一下怎么来训练网络和之后的检测过程。</p>
<a id="more"></a>
<h2 id="修改模型文件"><a href="#修改模型文件" class="headerlink" title="修改模型文件"></a>修改模型文件</h2><p>faster rcnn有两种各种训练方式:</p>
<ul>
<li>Alternative training(alt-opt)</li>
<li>Approximate joint training(end-to-end)</li>
</ul>
<p>两种方法有什么不同，可以参考我<a href="http://jacobkong.github.io/posts/3802700508/">这篇博客</a>，推荐使用第二种，因为第二种使用的显存更小，而且训练会更快，同时准确率差不多，两种方式需要修改的代码是不一样的，同时faster rcnn提供了三种训练模型，小型的ZF model，中型的VGG_CNN_M_1024和大型的VGG16,论文中说VGG16效果比其他两个好，但是同时占用更大的GPU显存(~11GB)</p>
<p>我使用的是<strong>VGG model + alternative training</strong>，需要检测的类别只有一类，加上背景所以总共是两类(background + person)。</p>
<p>下面修改模型文件：</p>
<ol>
<li><p>py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/stage1_fast_rcnn_train.pt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &apos;data&apos;  </div><div class="line">  type: &apos;Python&apos;  </div><div class="line">  top: &apos;data&apos;  </div><div class="line">  top: &apos;rois&apos;  </div><div class="line">  top: &apos;labels&apos;  </div><div class="line">  top: &apos;bbox_targets&apos;  </div><div class="line">  top: &apos;bbox_inside_weights&apos;  </div><div class="line">  top: &apos;bbox_outside_weights&apos;  </div><div class="line">  python_param &#123;  </div><div class="line">    module: &apos;roi_data_layer.layer&apos;  </div><div class="line">    layer: &apos;RoIDataLayer&apos;  </div><div class="line">    param_str: &quot;&apos;num_classes&apos;: 2&quot; #按训练集类别改，该值为类别数+1  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &quot;cls_score&quot;  </div><div class="line">  type: &quot;InnerProduct&quot;  </div><div class="line">  bottom: &quot;fc7&quot;  </div><div class="line">  top: &quot;cls_score&quot;  </div><div class="line">  param &#123; </div><div class="line">  lr_mult: 1.0</div><div class="line">  &#125;  </div><div class="line">  param &#123;</div><div class="line">  lr_mult: 2.0 </div><div class="line">  &#125;  </div><div class="line">  inner_product_param &#123;  </div><div class="line">    num_output: 2 #按训练集类别改，该值为类别数+1  </div><div class="line">    weight_filler &#123;  </div><div class="line">      type: &quot;gaussian&quot;  </div><div class="line">      std: 0.01  </div><div class="line">    &#125;  </div><div class="line">    bias_filler &#123;  </div><div class="line">      type: &quot;constant&quot;  </div><div class="line">      value: 0  </div><div class="line">    &#125;  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &quot;bbox_pred&quot;  </div><div class="line">  type: &quot;InnerProduct&quot;  </div><div class="line">  bottom: &quot;fc7&quot;  </div><div class="line">  top: &quot;bbox_pred&quot;  </div><div class="line">  param &#123; </div><div class="line">  lr_mult: 1.0 </div><div class="line">  &#125;  </div><div class="line">  param &#123; </div><div class="line">  lr_mult: 2.0 </div><div class="line">  &#125;  </div><div class="line">  inner_product_param &#123;  </div><div class="line">    num_output: 8 #按训练集类别改，该值为（类别数+1）*4，四个顶点坐标  </div><div class="line">    weight_filler &#123;  </div><div class="line">      type: &quot;gaussian&quot;  </div><div class="line">      std: 0.001  </div><div class="line">    &#125;  </div><div class="line">    bias_filler &#123;  </div><div class="line">      type: &quot;constant&quot;  </div><div class="line">      value: 0  </div><div class="line">    &#125;  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/stage1_rpn_train.pt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &apos;input-data&apos;  </div><div class="line">  type: &apos;Python&apos;  </div><div class="line">  top: &apos;data&apos;  </div><div class="line">  top: &apos;im_info&apos;  </div><div class="line">  top: &apos;gt_boxes&apos;  </div><div class="line">  python_param &#123;  </div><div class="line">    module: &apos;roi_data_layer.layer&apos;  </div><div class="line">    layer: &apos;RoIDataLayer&apos;  </div><div class="line">    param_str: &quot;&apos;num_classes&apos;: 2&quot; #按训练集类别改，该值为类别数+1  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/stage2_fast_rcnn_train.pt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &apos;data&apos;  </div><div class="line">  type: &apos;Python&apos;  </div><div class="line">  top: &apos;data&apos;  </div><div class="line">  top: &apos;rois&apos;  </div><div class="line">  top: &apos;labels&apos;  </div><div class="line">  top: &apos;bbox_targets&apos;  </div><div class="line">  top: &apos;bbox_inside_weights&apos;  </div><div class="line">  top: &apos;bbox_outside_weights&apos;  </div><div class="line">  python_param &#123;  </div><div class="line">    module: &apos;roi_data_layer.layer&apos;  </div><div class="line">    layer: &apos;RoIDataLayer&apos;  </div><div class="line">    param_str: &quot;&apos;num_classes&apos;: 2&quot; #按训练集类别改，该值为类别数+1  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &quot;cls_score&quot;  </div><div class="line">  type: &quot;InnerProduct&quot;  </div><div class="line">  bottom: &quot;fc7&quot;  </div><div class="line">  top: &quot;cls_score&quot;  </div><div class="line">  param &#123; </div><div class="line">  lr_mult: 1.0 </div><div class="line">  &#125;  </div><div class="line">  param &#123; </div><div class="line">  lr_mult: 2.0 </div><div class="line">  &#125;  </div><div class="line">  inner_product_param &#123;  </div><div class="line">    num_output: 2 #按训练集类别改，该值为类别数+1  </div><div class="line">    weight_filler &#123;  </div><div class="line">      type: &quot;gaussian&quot;  </div><div class="line">      std: 0.01  </div><div class="line">    &#125;  </div><div class="line">    bias_filler &#123;  </div><div class="line">      type: &quot;constant&quot;  </div><div class="line">      value: 0  </div><div class="line">    &#125;  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &quot;bbox_pred&quot;  </div><div class="line">  type: &quot;InnerProduct&quot;  </div><div class="line">  bottom: &quot;fc7&quot;  </div><div class="line">  top: &quot;bbox_pred&quot;  </div><div class="line">  param &#123; </div><div class="line">  lr_mult: 1.0</div><div class="line">  &#125;  </div><div class="line">  param &#123; </div><div class="line">  lr_mult: 2.0 </div><div class="line">  &#125;  </div><div class="line">  inner_product_param &#123;  </div><div class="line">    num_output: 8 #按训练集类别改，该值为（类别数+1）*4,四个顶点坐标  </div><div class="line">    weight_filler &#123;  </div><div class="line">      type: &quot;gaussian&quot;  </div><div class="line">      std: 0.001  </div><div class="line">    &#125;  </div><div class="line">    bias_filler &#123;  </div><div class="line">      type: &quot;constant&quot;  </div><div class="line">      value: 0  </div><div class="line">    &#125;  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/stage2_rpn_train.pt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &apos;input-data&apos;  </div><div class="line">  type: &apos;Python&apos;  </div><div class="line">  top: &apos;data&apos;  </div><div class="line">  top: &apos;im_info&apos;  </div><div class="line">  top: &apos;gt_boxes&apos;  </div><div class="line">  python_param &#123;  </div><div class="line">    module: &apos;roi_data_layer.layer&apos;  </div><div class="line">    layer: &apos;RoIDataLayer&apos;  </div><div class="line">    param_str: &quot;&apos;num_classes&apos;: 2&quot; #按训练集类别改，该值为类别数+1  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &quot;cls_score&quot;  </div><div class="line">  type: &quot;InnerProduct&quot;  </div><div class="line">  bottom: &quot;fc7&quot;  </div><div class="line">  top: &quot;cls_score&quot;  </div><div class="line">  inner_product_param &#123;  </div><div class="line">    num_output: 2 #按训练集类别改，该值为类别数+1  </div><div class="line">  &#125;  </div><div class="line">&#125;  </div><div class="line"></div><div class="line">layer &#123;  </div><div class="line">  name: &quot;bbox_pred&quot;  </div><div class="line">  type: &quot;InnerProduct&quot;  </div><div class="line">  bottom: &quot;fc7&quot;  </div><div class="line">  top: &quot;bbox_pred&quot;  </div><div class="line">  inner_product_param &#123;  </div><div class="line">    num_output: 84 #按训练集类别改，该值为（类别数+1）*4,四个顶点坐标</div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="训练测试"><a href="#训练测试" class="headerlink" title="训练测试"></a>训练测试</h2><p>训练前还需要注意几个地方：</p>
<ol>
<li><p>cache问题：</p>
<p>假如你之前训练了官方的VOC2007的数据集或其他的数据集，是会产生cache的问题的，建议在重新训练新的数据之前将其删除。</p>
<ul>
<li><code>py-faster-rcnn/output</code></li>
<li><code>py-faster-rcnn/data/cache</code></li>
</ul>
</li>
<li><p>训练参数</p>
<p>参数放在如下文件:</p>
<p><code>py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/stage_fast_rcnn_solver*.pt</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">base_lr: 0.001</div><div class="line">lr_policy: &apos;step&apos;</div><div class="line">step_size: 30000</div><div class="line">display: 20</div><div class="line">....</div></pre></td></tr></table></figure>
<p>迭代次数在文件py-faster-rcnn/tools/train_faster_rcnn_alt_opt.py中进行修改:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">max_iters = [80000, 40000, 80000, 40000]</div></pre></td></tr></table></figure>
<p>分别对应rpn第1阶段，fast rcnn第1阶段，rpn第2阶段，fast rcnn第2阶段的迭代次数，自己修改即可，不过注意这里的值不要小于上面的solver里面的step_size的大小，大家自己修改吧</p>
</li>
</ol>
<h3 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h3><p>首先修改<code>experiments/scripts/faster_rcnn_alt_opt.sh</code>成如下，修改地方已标注：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"><span class="comment"># Usage:</span></div><div class="line"><span class="comment"># ./experiments/scripts/faster_rcnn_alt_opt.sh GPU NET DATASET [options args to &#123;train,test&#125;_net.py]</span></div><div class="line"><span class="comment"># DATASET is only pascal_voc for now</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Example:</span></div><div class="line"><span class="comment"># ./experiments/scripts/faster_rcnn_alt_opt.sh 0 VGG_CNN_M_1024 pascal_voc \</span></div><div class="line"><span class="comment">#   --set EXP_DIR foobar RNG_SEED 42 TRAIN.SCALES "[400, 500, 600, 700]"</span></div><div class="line"></div><div class="line"><span class="built_in">set</span> -x</div><div class="line"><span class="built_in">set</span> <span class="_">-e</span></div><div class="line"></div><div class="line"><span class="built_in">export</span> PYTHONUNBUFFERED=<span class="string">"True"</span></div><div class="line"></div><div class="line">GPU_ID=<span class="variable">$1</span></div><div class="line">NET=<span class="variable">$2</span></div><div class="line">NET_lc=<span class="variable">$&#123;NET,,&#125;</span></div><div class="line">DATASET=<span class="variable">$3</span></div><div class="line"></div><div class="line">array=( <span class="variable">$@</span> )</div><div class="line">len=<span class="variable">$&#123;#array[@]&#125;</span></div><div class="line">EXTRA_ARGS=<span class="variable">$&#123;array[@]:3:$len&#125;</span></div><div class="line">EXTRA_ARGS_SLUG=<span class="variable">$&#123;EXTRA_ARGS// /_&#125;</span></div><div class="line"></div><div class="line"><span class="keyword">case</span> <span class="variable">$DATASET</span> <span class="keyword">in</span></div><div class="line">  caltech)                     <span class="comment"># 这里将pascal_voc改为caltech</span></div><div class="line">    TRAIN_IMDB=<span class="string">"caltech_train"</span> <span class="comment"># 改为与factor.py中命名的name格式相同，为caltech_train</span></div><div class="line">    TEST_IMDB=<span class="string">"caltech_test"</span>   <span class="comment"># 改为与factor.py中命名的name格式相同，为caltech_test</span></div><div class="line">    PT_DIR=<span class="string">"caltech"</span>           <span class="comment"># 这里将pascal_voc改为caltech</span></div><div class="line">    ITERS=40000</div><div class="line">    ;;</div><div class="line">  coco)</div><div class="line">    <span class="built_in">echo</span> <span class="string">"Not implemented: use experiments/scripts/faster_rcnn_end2end.sh for coco"</span></div><div class="line">    <span class="built_in">exit</span></div><div class="line">    ;;</div><div class="line">  *)</div><div class="line">    <span class="built_in">echo</span> <span class="string">"No dataset given"</span></div><div class="line">    <span class="built_in">exit</span></div><div class="line">    ;;</div><div class="line"><span class="keyword">esac</span></div><div class="line"></div><div class="line">LOG=<span class="string">"experiments/logs/faster_rcnn_alt_opt_<span class="variable">$&#123;NET&#125;</span>_<span class="variable">$&#123;EXTRA_ARGS_SLUG&#125;</span>.txt.`date +'%Y-%m-%d_%H-%M-%S'`"</span></div><div class="line"><span class="built_in">exec</span> &amp;&gt; &gt;(tee <span class="_">-a</span> <span class="string">"<span class="variable">$LOG</span>"</span>)</div><div class="line"><span class="built_in">echo</span> Logging output to <span class="string">"<span class="variable">$LOG</span>"</span></div><div class="line"></div><div class="line">time ./tools/train_faster_rcnn_alt_opt.py --gpu <span class="variable">$&#123;GPU_ID&#125;</span> \</div><div class="line">  --net_name <span class="variable">$&#123;NET&#125;</span> \</div><div class="line">  --weights data/imagenet_models/<span class="variable">$&#123;NET&#125;</span>.v2.caffemodel \</div><div class="line">  --imdb <span class="variable">$&#123;TRAIN_IMDB&#125;</span> \</div><div class="line">  --cfg experiments/cfgs/faster_rcnn_alt_opt.yml \</div><div class="line">  <span class="variable">$&#123;EXTRA_ARGS&#125;</span></div><div class="line"></div><div class="line"><span class="built_in">set</span> +x</div><div class="line">NET_FINAL=`grep <span class="string">"Final model:"</span> <span class="variable">$&#123;LOG&#125;</span> | awk <span class="string">'&#123;print $3&#125;'</span>`</div><div class="line"><span class="built_in">set</span> -x</div><div class="line"></div><div class="line">time ./tools/test_net.py --gpu <span class="variable">$&#123;GPU_ID&#125;</span> \</div><div class="line">  --def models/<span class="variable">$&#123;PT_DIR&#125;</span>/<span class="variable">$&#123;NET&#125;</span>/faster_rcnn_alt_opt/faster_rcnn_test.pt \</div><div class="line">  --net <span class="variable">$&#123;NET_FINAL&#125;</span> \</div><div class="line">  <span class="comment">#--net output/faster_rcnn_alt_opt/train/ZF_faster_rcnn_final.caffemodel \</span></div><div class="line">  --imdb <span class="variable">$&#123;TEST_IMDB&#125;</span> \</div><div class="line">  --cfg experiments/cfgs/faster_rcnn_alt_opt.yml \</div><div class="line">  <span class="variable">$&#123;EXTRA_ARGS&#125;</span></div></pre></td></tr></table></figure>
<p>调用如下命令进行训练及测试，从上面代码可以看出，该shell文件在训练完后会接着进行测试，但是我的测试集没有标注，所以测试的时候会报错，但是由于Caltech数据集的测试结果有专门的评估代码，所以我不用faster r-cnn提供的代码进行测试，而是直接进行检测生成坐标，用专门的评估代码进行检测。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> py-faster-rcnn</div><div class="line">./experiments/scripts/faster_rcnn_alt_opt.sh 0 VGG16 caltech</div></pre></td></tr></table></figure>
<ul>
<li>参数1：指定gpu_id。</li>
<li>参数2：指定网络模型参数。</li>
<li>参数3：数据集名称，目前只能为<code>pascal_voc</code>。</li>
</ul>
<p>在训练过程中，会调用<code>py_faster_rcnn/tools/train_faster_rcnn_alt_opt.py</code>文件开始训练网络。</p>
<h3 id="可能会出现的Bugs"><a href="#可能会出现的Bugs" class="headerlink" title="可能会出现的Bugs"></a>可能会出现的Bugs</h3><h4 id="AssertionError-assert-boxes-2-gt-boxes-0-all"><a href="#AssertionError-assert-boxes-2-gt-boxes-0-all" class="headerlink" title="AssertionError: assert (boxes[:, 2] &gt;= boxes[:, 0]).all()"></a>AssertionError: assert (boxes[:, 2] &gt;= boxes[:, 0]).all()</h4><h5 id="问题重现"><a href="#问题重现" class="headerlink" title="问题重现"></a>问题重现</h5><p>在训练过程中可能会出现如下报错：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">File &quot;/py-faster-rcnn/tools/../lib/datasets/imdb.py&quot;, line 108, in </div><div class="line">append_flipped_images </div><div class="line">	assert (boxes[:, 2] &gt;= boxes[:, 0]).all() </div><div class="line">AssertionError</div></pre></td></tr></table></figure>
<h5 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h5><p>检查自己数据发现，左上角坐标 (x, y) 可能为0，或标定区域溢出图片（即坐标为负数），而faster rcnn会对Xmin,Ymin,Xmax,Ymax进行减一操作，如果Xmin为0，减一后变为65535，从而在左右翻转图片时导致如上错误发生。</p>
<h5 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h5><ol>
<li><p>修改<code>lib/datasets/imdb.py</code>中的<code>append_flipped_images()</code>函数：</p>
<p>数据整理，在一行代码为 <code>boxes[:, 2] = widths[i] - oldx1 - 1</code>下加入代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> b <span class="keyword">in</span> range(len(boxes)):</div><div class="line">	<span class="keyword">if</span> boxes[b][<span class="number">2</span>]&lt; boxes[b][<span class="number">0</span>]:</div><div class="line">		boxes[b][<span class="number">0</span>] = <span class="number">0</span></div></pre></td></tr></table></figure>
</li>
<li><p>修改<code>lib/datasets/caltech.py</code>，<code>_load_pascal_annotation()</code>函数，将对Xmin,Ymin,Xmax,Ymax减一去掉，变为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Load object bounding boxes into a data frame.</span></div><div class="line">        <span class="keyword">for</span> ix, obj <span class="keyword">in</span> enumerate(objs):</div><div class="line">            bbox = obj.find(<span class="string">'bndbox'</span>)</div><div class="line">            <span class="comment"># Make pixel indexes 0-based</span></div><div class="line">            <span class="comment"># 这里我把‘-1’全部删除掉了，防止有的数据是0开始，然后‘-1’导致变为负数，产生AssertError错误</span></div><div class="line">            x1 = float(bbox.find(<span class="string">'xmin'</span>).text)</div><div class="line">            y1 = float(bbox.find(<span class="string">'ymin'</span>).text)</div><div class="line">            x2 = float(bbox.find(<span class="string">'xmax'</span>).text)</div><div class="line">            y2 = float(bbox.find(<span class="string">'ymax'</span>).text)</div><div class="line">            cls = self._class_to_ind[obj.find(<span class="string">'name'</span>).text.lower().strip()]</div><div class="line">            boxes[ix, :] = [x1, y1, x2, y2]</div><div class="line">            gt_classes[ix] = cls</div><div class="line">            overlaps[ix, cls] = <span class="number">1.0</span></div><div class="line">            seg_areas[ix] = (x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>)</div></pre></td></tr></table></figure>
</li>
<li><p>（可选）如果1和2可以解决问题，就没必要用方法3。修改<code>lib/fast_rcnn/config.py</code>，不使图片实现翻转，如下改为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># Use horizontally-flipped images during training? </div><div class="line">__C.TRAIN.USE_FLIPPED = False</div></pre></td></tr></table></figure>
</li>
</ol>
<p>如果如上三种方法都无法解决该问题，那么肯定是你的数据集坐标出现小于等于0的数，你<strong>应该一一排查</strong>。</p>
<h4 id="训练fast-rcnn时出现loss-nan的情况。"><a href="#训练fast-rcnn时出现loss-nan的情况。" class="headerlink" title="训练fast rcnn时出现loss=nan的情况。"></a>训练fast rcnn时出现loss=nan的情况。</h4><h5 id="问题重现-1"><a href="#问题重现-1" class="headerlink" title="问题重现"></a>问题重现</h5><p><img src="https://ww1.sinaimg.cn/large/006tNbRwly1fcuq2kgkwgj30v10hddsn.jpg" alt=""></p>
<h5 id="问题分析-1"><a href="#问题分析-1" class="headerlink" title="问题分析"></a>问题分析</h5><p>这是由于模型不收敛，导致loss迅速增长。</p>
<p>而我出现以上现象的原因主要是因为我在出现AssertionError的时候直接使用了第三种方法导致的。也就是禁用图片翻转。</p>
<h5 id="问题解决-1"><a href="#问题解决-1" class="headerlink" title="问题解决"></a>问题解决</h5><p>启用图片翻转。</p>
<h3 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h3><p>训练后的模型放在<code>output/faster_rcnn_alt_opt/train/VGG16_faster_rcnn_final.caffemodel</code>，该模型可以用于之后的检测。</p>
<h2 id="检测"><a href="#检测" class="headerlink" title="检测"></a>检测</h2><h3 id="检测步骤"><a href="#检测步骤" class="headerlink" title="检测步骤"></a>检测步骤</h3><p>经过以上训练后，就可以用得到的模型来进行检测了。检测所参考的代码是<code>tools/demo.py</code>，具体步骤如下：</p>
<ol>
<li>将<code>output/faster_rcnn_alt_opt/train/VGG16_faster_rcnn_final.caffemodel</code>，拷贝到<code>data/faster_rcnn_models</code>下，命名为<code>VGG16_Caltech_faster_rcnn__final.caffemodel</code></li>
<li>进入<code>tools/</code>文件夹中，拷贝<code>demo.py</code>为<code>demo_caltech.py</code>。</li>
<li>修改demo_caltech.py代码如下：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"></div><div class="line"><span class="comment"># --------------------------------------------------------</span></div><div class="line"><span class="comment"># Faster R-CNN</span></div><div class="line"><span class="comment"># Copyright (c) 2015 Microsoft</span></div><div class="line"><span class="comment"># Licensed under The MIT License [see LICENSE for details]</span></div><div class="line"><span class="comment"># Written by Ross Girshick</span></div><div class="line"><span class="comment"># --------------------------------------------------------</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> matplotlib</div><div class="line">matplotlib.use(<span class="string">'Agg'</span>);</div><div class="line"><span class="string">"""</span></div><div class="line">Demo script showing detections in sample images.</div><div class="line"></div><div class="line">See README.md for installation instructions before running.</div><div class="line">"""</div><div class="line"></div><div class="line"><span class="keyword">import</span> _init_paths</div><div class="line"><span class="keyword">from</span> fast_rcnn.config <span class="keyword">import</span> cfg</div><div class="line"><span class="keyword">from</span> fast_rcnn.test <span class="keyword">import</span> im_detect</div><div class="line"><span class="keyword">from</span> fast_rcnn.nms_wrapper <span class="keyword">import</span> nms</div><div class="line"><span class="keyword">from</span> utils.timer <span class="keyword">import</span> Timer</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</div><div class="line"><span class="keyword">import</span> caffe, os, sys, cv2</div><div class="line"><span class="keyword">import</span> argparse</div><div class="line"></div><div class="line">CLASSES = (<span class="string">'__background__'</span>, <span class="comment"># 这里改为自己的类别</span></div><div class="line">           <span class="string">'person'</span>)</div><div class="line"></div><div class="line">NETS = &#123;<span class="string">'vgg16'</span>: (<span class="string">'VGG16'</span>,</div><div class="line">                  <span class="string">'VGG16_Caltech_faster_rcnn_final.caffemodel'</span>), <span class="comment">#这里需要修改为训练后得到的模型的名称</span></div><div class="line">        <span class="string">'zf'</span>: (<span class="string">'ZF'</span>,</div><div class="line">                  <span class="string">'ZF_Caltech_faster_rcnn_final.caffemodel'</span>)&#125; <span class="comment">#这里需要修改为训练后得到的模型的名称</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">vis_detections</span><span class="params">(im, image_name, class_name, dets, thresh=<span class="number">0.5</span>)</span>:</span></div><div class="line">    <span class="string">"""Draw detected bounding boxes."""</span></div><div class="line">    inds = np.where(dets[:, <span class="number">-1</span>] &gt;= thresh)[<span class="number">0</span>]</div><div class="line">    <span class="keyword">if</span> len(inds) == <span class="number">0</span>:</div><div class="line">        <span class="keyword">return</span></div><div class="line"></div><div class="line">    im = im[:, :, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>)]</div><div class="line">    fig, ax = plt.subplots(figsize=(<span class="number">12</span>, <span class="number">12</span>))</div><div class="line">    ax.imshow(im, aspect=<span class="string">'equal'</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> inds:</div><div class="line">        bbox = dets[i, :<span class="number">4</span>]</div><div class="line">        score = dets[i, <span class="number">-1</span>]</div><div class="line"></div><div class="line">        ax.add_patch(</div><div class="line">            plt.Rectangle((bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]),</div><div class="line">                          bbox[<span class="number">2</span>] - bbox[<span class="number">0</span>],</div><div class="line">                          bbox[<span class="number">3</span>] - bbox[<span class="number">1</span>], fill=<span class="keyword">False</span>,</div><div class="line">                          edgecolor=<span class="string">'red'</span>, linewidth=<span class="number">3.5</span>)</div><div class="line">            )</div><div class="line">        ax.text(bbox[<span class="number">0</span>], bbox[<span class="number">1</span>] - <span class="number">2</span>,</div><div class="line">                <span class="string">'&#123;:s&#125; &#123;:.3f&#125;'</span>.format(class_name, score),</div><div class="line">                bbox=dict(facecolor=<span class="string">'blue'</span>, alpha=<span class="number">0.5</span>),</div><div class="line">                fontsize=<span class="number">14</span>, color=<span class="string">'white'</span>)</div><div class="line"></div><div class="line">    ax.set_title((<span class="string">'&#123;&#125; detections with '</span></div><div class="line">                  <span class="string">'p(&#123;&#125; | box) &gt;= &#123;:.1f&#125;'</span>).format(class_name, class_name,</div><div class="line">                                                  thresh),</div><div class="line">                  fontsize=<span class="number">14</span>)</div><div class="line">    plt.axis(<span class="string">'off'</span>)</div><div class="line">    plt.tight_layout()</div><div class="line">    plt.draw()</div><div class="line">    plt.savefig(<span class="string">'/home/jk/py-faster-rcnn/output/faster_rcnn_alt_opt/test/'</span>+image_name) <span class="comment">#将检测后的图片保存到相应的路径</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span><span class="params">(net, image_name)</span>:</span></div><div class="line">    <span class="string">"""Detect object classes in an image using pre-computed object proposals."""</span></div><div class="line"></div><div class="line">    <span class="comment"># Load the demo image</span></div><div class="line">    im_file = os.path.join(cfg.DATA_DIR, <span class="string">'VOCdevkit/Caltech/JPEGImages'</span>, image_name)</div><div class="line">    im = cv2.imread(im_file)</div><div class="line"></div><div class="line">    <span class="comment"># Detect all object classes and regress object bounds</span></div><div class="line">    timer = Timer()</div><div class="line">    timer.tic()</div><div class="line">    scores, boxes = im_detect(net, im)</div><div class="line">    timer.toc()</div><div class="line">    <span class="keyword">print</span> (<span class="string">'Detection took &#123;:.3f&#125;s for '</span></div><div class="line">           <span class="string">'&#123;:d&#125; object proposals'</span>).format(timer.total_time, boxes.shape[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Visualize detections for each class</span></div><div class="line">    CONF_THRESH = <span class="number">0.85</span> <span class="comment"># 设置权值，越低检测出的框越多</span></div><div class="line">    NMS_THRESH = <span class="number">0.3</span></div><div class="line">    <span class="keyword">for</span> cls_ind, cls <span class="keyword">in</span> enumerate(CLASSES[<span class="number">1</span>:]):</div><div class="line">        cls_ind += <span class="number">1</span> <span class="comment"># because we skipped background</span></div><div class="line">        cls_boxes = boxes[:, <span class="number">4</span>*cls_ind:<span class="number">4</span>*(cls_ind + <span class="number">1</span>)]</div><div class="line">        cls_scores = scores[:, cls_ind]</div><div class="line">        dets = np.hstack((cls_boxes,</div><div class="line">                          cls_scores[:, np.newaxis])).astype(np.float32)</div><div class="line">        keep = nms(dets, NMS_THRESH)</div><div class="line">        dets = dets[keep, :]</div><div class="line">        vis_detections(im, image_name, cls, dets, thresh=CONF_THRESH)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_args</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""Parse input arguments."""</span></div><div class="line">    parser = argparse.ArgumentParser(description=<span class="string">'Faster R-CNN demo'</span>)</div><div class="line">    parser.add_argument(<span class="string">'--gpu'</span>, dest=<span class="string">'gpu_id'</span>, help=<span class="string">'GPU device id to use [0]'</span>,</div><div class="line">                        default=<span class="number">0</span>, type=int)</div><div class="line">    parser.add_argument(<span class="string">'--cpu'</span>, dest=<span class="string">'cpu_mode'</span>,</div><div class="line">                        help=<span class="string">'Use CPU mode (overrides --gpu)'</span>,</div><div class="line">                        action=<span class="string">'store_true'</span>)</div><div class="line">    parser.add_argument(<span class="string">'--net'</span>, dest=<span class="string">'demo_net'</span>, help=<span class="string">'Network to use [vgg16]'</span>,</div><div class="line">                        choices=NETS.keys(), default=<span class="string">'vgg16'</span>)</div><div class="line"></div><div class="line">    args = parser.parse_args()</div><div class="line"></div><div class="line">    <span class="keyword">return</span> args</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    cfg.TEST.HAS_RPN = <span class="keyword">True</span>  <span class="comment"># Use RPN for proposals</span></div><div class="line"></div><div class="line">    args = parse_args()</div><div class="line"></div><div class="line">    prototxt = os.path.join(cfg.MODELS_DIR, NETS[args.demo_net][<span class="number">0</span>],</div><div class="line">                            <span class="string">'faster_rcnn_alt_opt'</span>, <span class="string">'faster_rcnn_test.pt'</span>)</div><div class="line">    caffemodel = os.path.join(cfg.DATA_DIR, <span class="string">'faster_rcnn_models'</span>,</div><div class="line">                              NETS[args.demo_net][<span class="number">1</span>])</div><div class="line"></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(caffemodel):</div><div class="line">        <span class="keyword">raise</span> IOError((<span class="string">'&#123;:s&#125; not found.\nDid you run ./data/script/'</span></div><div class="line">                       <span class="string">'fetch_faster_rcnn_models.sh?'</span>).format(caffemodel))</div><div class="line"></div><div class="line">    <span class="keyword">if</span> args.cpu_mode:</div><div class="line">        caffe.set_mode_cpu()</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        caffe.set_mode_gpu()</div><div class="line">        caffe.set_device(args.gpu_id)</div><div class="line">        cfg.GPU_ID = args.gpu_id</div><div class="line">    net = caffe.Net(prototxt, caffemodel, caffe.TEST)</div><div class="line"></div><div class="line">    <span class="keyword">print</span> <span class="string">'\n\nLoaded network &#123;:s&#125;'</span>.format(caffemodel)</div><div class="line"></div><div class="line">    <span class="comment"># Warmup on a dummy image</span></div><div class="line">    im = <span class="number">128</span> * np.ones((<span class="number">300</span>, <span class="number">500</span>, <span class="number">3</span>), dtype=np.uint8)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">2</span>):</div><div class="line">        _, _= im_detect(net, im)</div><div class="line"></div><div class="line">    testfile_path = <span class="string">'/home/jk/py-faster-rcnn/data/VOCdevkit/Caltech/ImageSets/Main/test.txt'</span></div><div class="line">    <span class="keyword">with</span> open(testfile_path) <span class="keyword">as</span> f:</div><div class="line">        im_names = [x.strip()+<span class="string">'.jpg'</span> <span class="keyword">for</span> x <span class="keyword">in</span> f.readlines()] <span class="comment"># 从test.txt文件中读取图片文件名，找到相应的图片进行检测。也可以使用如下的方法，把项检测的图片存到tools/demo/文件夹下进行读取检测</span></div><div class="line"></div><div class="line">    <span class="comment">#im_names = ['set06_V002_I00023.jpg', 'set06_V002_I00072.jpg', 'set06_V002_I00097.jpg',</span></div><div class="line">    <span class="comment">#            'set06_V002_I00151.jpg', 'set07_V010_I00247.jpg']</span></div><div class="line">    <span class="keyword">for</span> im_name <span class="keyword">in</span> im_names:</div><div class="line">        <span class="keyword">print</span> <span class="string">'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'</span></div><div class="line">        <span class="keyword">print</span> <span class="string">'Demo for data/demo/&#123;&#125;'</span>.format(im_name)</div><div class="line">        demo(net, im_name)</div><div class="line"></div><div class="line">    plt.show()</div></pre></td></tr></table></figure>
<p>在命令行中输入一下命令进行检测：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python tools/demo_caltech.py</div></pre></td></tr></table></figure>
<h3 id="检测结果"><a href="#检测结果" class="headerlink" title="检测结果"></a>检测结果</h3><p>放几张检测后的结果图，感觉检测效果并不是很好，很多把背景当成行人的错误：</p>
<p><img src="https://ww2.sinaimg.cn/large/006y8lValy1fd36adqacrj30hr0h8qe6.jpg" alt=""></p>
<p><img src="https://ww4.sinaimg.cn/large/006y8lValy1fd36apj94fj30ho0h7wpo.jpg" alt=""></p>
<p><img src="https://ww1.sinaimg.cn/large/006y8lValy1fd36aznxnlj30hq0h1k2b.jpg" alt=""></p>
<p><img src="https://ww3.sinaimg.cn/large/006y8lValy1fd36bnalbwj30hk0h749k.jpg" alt=""></p>
<h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol>
<li><strong><a href="http://www.itdadao.com/articles/c15a253094p0.html" target="_blank" rel="external">使用Faster-Rcnn进行目标检测(实践篇)</a></strong></li>
<li><a href="https://github.com/zeyuanxy/fast-rcnn/blob/master/help/train/README.md" target="_blank" rel="external"><strong>Train Fast-RCNN on Another Dataset</strong></a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;前面已经介绍了如何准备数据集，以及如何修改数据集读写接口来操作数据集，接下来我来说明一下怎么来训练网络和之后的检测过程。&lt;/p&gt;
    
    </summary>
    
      <category term="经验" scheme="http://jacobkong.github.io/blog/categories/%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/blog/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/blog/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/blog/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习实践经验：用Faster R-CNN训练Caltech数据集——修改读写接口</title>
    <link href="http://jacobkong.github.io/blog/4113466123/"/>
    <id>http://jacobkong.github.io/blog/4113466123/</id>
    <published>2017-01-16T22:32:24.000Z</published>
    <updated>2017-02-18T11:17:14.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这部分主要讲如何修改Faster R-CNN的代码，来训练自己的数据集，首先确保你已经编译安装了py-faster-rcnn，并且准备好了数据集，具体可参考我<a href="http://jacobkong.github.io/posts/2093106769/">上一篇文章</a>。</p>
<a id="more"></a>
<h2 id="py-faster-rcnn文件结构"><a href="#py-faster-rcnn文件结构" class="headerlink" title="py-faster-rcnn文件结构"></a>py-faster-rcnn文件结构</h2><ul>
<li>caffe-fast-rcnn<br>这里是caffe框架目录，用来进行caffe编译安装</li>
<li>data<br>用来存放pre trained模型，比如ImageNet上的，要训练的数据集以及读取文件的cache缓存。</li>
<li>experiments<br>存放配置文件，运行的log文件，另外这个目录下有scripts 用来获取imagenet的模型，以及作者训练好的fast rcnn模型，以及相应的pascal-voc数据集</li>
<li>lib<br>用来存放一些python接口文件，如其下的datasets主要负责数据库读取，config负责cnn一些训练的配置选项</li>
<li>matlab<br>放置matlab与python的接口，用matlab来调用实现detection</li>
<li>models<br>里面存放了三个模型文件，小型网络的ZF，大型网络VGG16，中型网络VGG_CNN_M_1024</li>
<li>output<br>这里存放的是训练完成后的输出目录，默认会在default文件夹下</li>
<li>tools<br>里面存放的是训练和测试的Python文件</li>
</ul>
<h2 id="修改训练代码"><a href="#修改训练代码" class="headerlink" title="修改训练代码"></a>修改训练代码</h2><h3 id="所要操作文件结构介绍"><a href="#所要操作文件结构介绍" class="headerlink" title="所要操作文件结构介绍"></a>所要操作文件结构介绍</h3><p>所有需要修改的训练代码都放到了<code>py-faster-rcnn/lib</code>文件夹下，我们进入文件夹，里面主要用到的文件夹有：</p>
<ul>
<li>datasets：该目录下主要存放读写数据接口。</li>
<li>fast-rcnn：该目录下主要存放的是python的训练和测试脚本，以及训练的配置文件。</li>
<li>roi_data_layer：该目录下主要存放一些ROI处理操作文件。</li>
<li>utils：该目录下主要存放一些通用操作比如非极大值nms，以及计算bounding box的重叠率等常用功能。</li>
</ul>
<p>读写数据接口都放在<code>datasets/</code>文件夹下，我们进入文件夹，里面主要文件有：</p>
<ul>
<li>factory.py：这是个工厂类，用类生成imdb类并且返回数据库共网络训练和测试使用。</li>
<li>imdb.py：这是数据库读写类的基类，分装了许多db的操作，但是具体的一些文件读写需要继承继续读写</li>
<li>pascal_voc.py：这是imdb的子类，里面定义许多函数用来进行所有的数据读写操作。</li>
</ul>
<p>从上面可以看出，我们主要对<code>pascal_voc.py</code>文件进行修改。</p>
<h3 id="pascal-voc-py文件代码分析"><a href="#pascal-voc-py文件代码分析" class="headerlink" title="pascal_voc.py文件代码分析"></a>pascal_voc.py文件代码分析</h3><p>我们主要是基于<code>pasca_voc.py</code>这个文件进行修改，里面有几个重要的函数需要介绍：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, image_set, devkit_path=None)</span>:</span> <span class="comment"># 这个是初始化函数，它对应着的是pascal_voc的数据集访问格式。</span></div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">image_path_at</span><span class="params">(self, i)</span>:</span> <span class="comment"># 根据第i个图像样本返回其对应的path，其调用image_path_from_index(self, index):作为其具体实现。</span></div><div class="line">        </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">image_path_from_index</span><span class="params">(self, index)</span>:</span> <span class="comment"># 实现了 image_path的具体功能</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_load_image_set_index</span><span class="params">(self)</span>:</span> <span class="comment"># 加载了样本的list文件，根据ImageSet/Main/文件夹下的文件进行image_index的加载。</span></div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_default_path</span><span class="params">(self)</span>:</span> <span class="comment"># 获得数据集地址</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gt_roidb</span><span class="params">(self)</span>:</span> <span class="comment"># 读取并返回ground_truth的db</span></div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rpn_roidb</span><span class="params">(self)</span>:</span> <span class="comment"># 加载rpn产生的roi，调用_load_rpn_roidb(self, gt_roidb):函数作为其具体实现</span></div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_load_rpn_roidb</span><span class="params">(self, gt_roidb)</span>:</span> <span class="comment"># 加载rpn_file</span></div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_load_pascal_annotation</span><span class="params">(self, index)</span>:</span> <span class="comment"># 这个函数是读取gt的具体实现</span></div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_write_voc_results_file</span><span class="params">(self, all_boxes)</span>:</span> <span class="comment"># 将voc的检测结果写入到文件</span></div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_do_python_eval</span><span class="params">(self, output_dir = <span class="string">'output'</span>)</span>:</span> <span class="comment"># 根据python的evluation接口来做结果的分析</span></div></pre></td></tr></table></figure>
<h3 id="修改pascal-voc-py文件"><a href="#修改pascal-voc-py文件" class="headerlink" title="修改pascal_voc.py文件"></a>修改pascal_voc.py文件</h3><p>要想对自己的数据集进行读取，我们主要是进行<code>pascal_voc.py</code>文件的修改，但是为了不破坏源文件，我们可以将<code>pascal_voc.py</code>进行拷贝复制，从而进行修改。这里我将<code>pascal_voc.py</code>文件拷贝成<code>caltech.py</code>文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cp pascal_voc.py caltech.py</div></pre></td></tr></table></figure>
<p>下面我们对<code>caltech.py</code>文件进行修改，在这里我会一一列举每个我修改过的函数。这里按照文件中的顺序排列。。</p>
<h4 id="init函数修改"><a href="#init函数修改" class="headerlink" title="init函数修改"></a><strong>init</strong>函数修改</h4><p>这里是原始的pascal_voc的init函数，在这里，由于我们自己的数据集往往比voc的数据集要更简单的一些，在作者额代码里面用了很多的路径拼接，我们不用去迎合他的格式，将这些操作简单化即可。</p>
<h5 id="原始的函数"><a href="#原始的函数" class="headerlink" title="原始的函数"></a>原始的函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, image_set, year, devkit_path=None)</span>:</span></div><div class="line">        imdb.__init__(self, <span class="string">'voc_'</span> + year + <span class="string">'_'</span> + image_set)</div><div class="line">        self._year = year</div><div class="line">        self._image_set = image_set</div><div class="line">        self._devkit_path = self._get_default_path() <span class="keyword">if</span> devkit_path <span class="keyword">is</span> <span class="keyword">None</span> \</div><div class="line">                            <span class="keyword">else</span> devkit_path</div><div class="line">        self._data_path = os.path.join(self._devkit_path, <span class="string">'VOC'</span> + self._year)</div><div class="line">        self._classes = (<span class="string">'__background__'</span>, <span class="comment"># always index 0</span></div><div class="line">                         <span class="string">'aeroplane'</span>, <span class="string">'bicycle'</span>, <span class="string">'bird'</span>, <span class="string">'boat'</span>,</div><div class="line">                         <span class="string">'bottle'</span>, <span class="string">'bus'</span>, <span class="string">'car'</span>, <span class="string">'cat'</span>, <span class="string">'chair'</span>,</div><div class="line">                         <span class="string">'cow'</span>, <span class="string">'diningtable'</span>, <span class="string">'dog'</span>, <span class="string">'horse'</span>,</div><div class="line">                         <span class="string">'motorbike'</span>, <span class="string">'person'</span>, <span class="string">'pottedplant'</span>,</div><div class="line">                         <span class="string">'sheep'</span>, <span class="string">'sofa'</span>, <span class="string">'train'</span>, <span class="string">'tvmonitor'</span>)</div><div class="line">        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))</div><div class="line">        self._image_ext = <span class="string">'.jpg'</span></div><div class="line">        self._image_index = self._load_image_set_index()</div><div class="line">        <span class="comment"># Default to roidb handler</span></div><div class="line">        self._roidb_handler = self.selective_search_roidb</div><div class="line">        self._salt = str(uuid.uuid4())</div><div class="line">        self._comp_id = <span class="string">'comp4'</span></div><div class="line"></div><div class="line">        <span class="comment"># PASCAL specific config options</span></div><div class="line">        self.config = &#123;<span class="string">'cleanup'</span>     : <span class="keyword">True</span>,</div><div class="line">                       <span class="string">'use_salt'</span>    : <span class="keyword">True</span>,</div><div class="line">                       <span class="string">'use_diff'</span>    : <span class="keyword">False</span>,</div><div class="line">                       <span class="string">'matlab_eval'</span> : <span class="keyword">False</span>,</div><div class="line">                       <span class="string">'rpn_file'</span>    : <span class="keyword">None</span>,</div><div class="line">                       <span class="string">'min_size'</span>    : <span class="number">2</span>&#125;</div><div class="line"></div><div class="line">        <span class="keyword">assert</span> os.path.exists(self._devkit_path), \</div><div class="line">                <span class="string">'VOCdevkit path does not exist: &#123;&#125;'</span>.format(self._devkit_path)</div><div class="line">        <span class="keyword">assert</span> os.path.exists(self._data_path), \</div><div class="line">                <span class="string">'Path does not exist: &#123;&#125;'</span>.format(self._data_path)</div></pre></td></tr></table></figure>
<h5 id="修改后的函数"><a href="#修改后的函数" class="headerlink" title="修改后的函数"></a>修改后的函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, image_set, devkit_path=None)</span>:</span><span class="comment"># initial function，把year删除</span></div><div class="line">        imdb.__init__(self, image_set) <span class="comment"># imageset is train.txt or test.txt</span></div><div class="line">        self._image_set = image_set</div><div class="line">        self._devkit_path = devkit_path <span class="comment"># devkit_path = '~/py-faster-rcnn/data/VOCdevkit'</span></div><div class="line">        self._data_path = os.path.join(self._devkit_path, <span class="string">'Caltech'</span>) <span class="comment"># _data_path = '~/py-faster-rcnn/data/VOCdevkit/Caltech'</span></div><div class="line">        self._classes = (<span class="string">'__background__'</span>, <span class="comment"># always index 0</span></div><div class="line">                         <span class="string">'person'</span>) <span class="comment"># 我只有‘background’和‘person’两类</span></div><div class="line">        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))</div><div class="line">        self._image_ext = <span class="string">'.jpg'</span></div><div class="line">        self._image_index = self._load_image_set_index()</div><div class="line">        <span class="comment"># Default to roidb handler</span></div><div class="line">        self._roidb_handler = self.selective_search_roidb</div><div class="line">        self._salt = str(uuid.uuid4())</div><div class="line">        self._comp_id = <span class="string">'comp4'</span></div><div class="line"></div><div class="line">        <span class="comment"># PASCAL specific config options</span></div><div class="line">        self.config = &#123;<span class="string">'cleanup'</span>     : <span class="keyword">True</span>,</div><div class="line">                       <span class="string">'use_salt'</span>    : <span class="keyword">True</span>,</div><div class="line">                       <span class="string">'use_diff'</span>    : <span class="keyword">True</span>, <span class="comment"># 我把use_diff改为true了，因为我的数据集xml文件中没有&lt;difficult&gt;标签，否则之后训练会报错</span></div><div class="line">                       <span class="string">'matlab_eval'</span> : <span class="keyword">False</span>,</div><div class="line">                       <span class="string">'rpn_file'</span>    : <span class="keyword">None</span>,</div><div class="line">                       <span class="string">'min_size'</span>    : <span class="number">2</span>&#125;</div><div class="line"></div><div class="line">        <span class="keyword">assert</span> os.path.exists(self._devkit_path), \</div><div class="line">                <span class="string">'VOCdevkit path does not exist: &#123;&#125;'</span>.format(self._devkit_path)</div><div class="line">        <span class="keyword">assert</span> os.path.exists(self._data_path), \</div><div class="line">                <span class="string">'Path does not exist: &#123;&#125;'</span>.format(self._data_path)</div></pre></td></tr></table></figure>
<h4 id="load-image-set-index函数修改"><a href="#load-image-set-index函数修改" class="headerlink" title="_load_image_set_index函数修改"></a>_load_image_set_index函数修改</h4><h5 id="原始的函数-1"><a href="#原始的函数-1" class="headerlink" title="原始的函数"></a>原始的函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_load_image_set_index</span><span class="params">(self)</span>:</span></div><div class="line">      <span class="string">"""</span></div><div class="line">          Load the indexes listed in this dataset's image set file.</div><div class="line">          """</div><div class="line">      <span class="comment"># Example path to image set file:</span></div><div class="line">      <span class="comment"># self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt</span></div><div class="line">      image_set_file = os.path.join(self._data_path, <span class="string">'ImageSets'</span>, <span class="string">'Main'</span>,</div><div class="line">                                    self._image_set + <span class="string">'.txt'</span>)</div><div class="line">      <span class="keyword">assert</span> os.path.exists(image_set_file), \</div><div class="line">      <span class="string">'Path does not exist: &#123;&#125;'</span>.format(image_set_file)</div><div class="line">      <span class="keyword">with</span> open(image_set_file) <span class="keyword">as</span> f:</div><div class="line">          image_index = [x.strip() <span class="keyword">for</span> x <span class="keyword">in</span> f.readlines()]</div><div class="line">          <span class="keyword">return</span> image_index</div></pre></td></tr></table></figure>
<h5 id="修改后的函数-1"><a href="#修改后的函数-1" class="headerlink" title="修改后的函数"></a>修改后的函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_load_image_set_index</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Load the indexes listed in this dataset's image set file.</div><div class="line">        """</div><div class="line">        <span class="comment"># Example path to image set file:</span></div><div class="line">        <span class="comment"># self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt</span></div><div class="line">        <span class="comment"># /home/jk/py-faster-rcnn/data/VOCdevkit/Caltech/ImageSets/Main/train.txt</span></div><div class="line">        image_set_file = os.path.join(self._data_path, <span class="string">'ImageSets'</span>, <span class="string">'Main'</span>,</div><div class="line">                                      self._image_set + <span class="string">'.txt'</span>)</div><div class="line">        <span class="keyword">assert</span> os.path.exists(image_set_file), \</div><div class="line">                <span class="string">'Path does not exist: &#123;&#125;'</span>.format(image_set_file)</div><div class="line">        <span class="keyword">with</span> open(image_set_file) <span class="keyword">as</span> f:</div><div class="line">            image_index = [x.strip() <span class="keyword">for</span> x <span class="keyword">in</span> f.readlines()]</div><div class="line">        <span class="keyword">return</span> image_index</div></pre></td></tr></table></figure>
<p>其实没改，只是加了一行注释，从而更好理解路径问题。</p>
<h4 id="get-default-path函数修改"><a href="#get-default-path函数修改" class="headerlink" title="_get_default_path函数修改"></a>_get_default_path函数修改</h4><p>直接注释即可</p>
<h4 id="load-pascal-annotation函数修改"><a href="#load-pascal-annotation函数修改" class="headerlink" title="_load_pascal_annotation函数修改"></a>_load_pascal_annotation函数修改</h4><h5 id="原始的函数-2"><a href="#原始的函数-2" class="headerlink" title="原始的函数"></a>原始的函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_load_pascal_annotation</span><span class="params">(self, index)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Load image and bounding boxes info from XML file in the PASCAL VOC</div><div class="line">        format.</div><div class="line">        """</div><div class="line">        filename = os.path.join(self._data_path, <span class="string">'Annotations'</span>, index + <span class="string">'.xml'</span>)</div><div class="line">        tree = ET.parse(filename)</div><div class="line">        objs = tree.findall(<span class="string">'object'</span>)</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.config[<span class="string">'use_diff'</span>]:</div><div class="line">            <span class="comment"># Exclude the samples labeled as difficult</span></div><div class="line">            non_diff_objs = [</div><div class="line">                obj <span class="keyword">for</span> obj <span class="keyword">in</span> objs <span class="keyword">if</span> int(obj.find(<span class="string">'difficult'</span>).text) == <span class="number">0</span>]</div><div class="line">            <span class="comment"># if len(non_diff_objs) != len(objs):</span></div><div class="line">            <span class="comment">#     print 'Removed &#123;&#125; difficult objects'.format(</span></div><div class="line">            <span class="comment">#         len(objs) - len(non_diff_objs))</span></div><div class="line">            objs = non_diff_objs</div><div class="line">        num_objs = len(objs)</div><div class="line"></div><div class="line">        boxes = np.zeros((num_objs, <span class="number">4</span>), dtype=np.uint16)</div><div class="line">        gt_classes = np.zeros((num_objs), dtype=np.int32)</div><div class="line">        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)</div><div class="line">        <span class="comment"># "Seg" area for pascal is just the box area</span></div><div class="line">        seg_areas = np.zeros((num_objs), dtype=np.float32)</div><div class="line"></div><div class="line">        <span class="comment"># Load object bounding boxes into a data frame.</span></div><div class="line">        <span class="keyword">for</span> ix, obj <span class="keyword">in</span> enumerate(objs):</div><div class="line">            bbox = obj.find(<span class="string">'bndbox'</span>)</div><div class="line">            <span class="comment"># Make pixel indexes 0-based</span></div><div class="line">            x1 = float(bbox.find(<span class="string">'xmin'</span>).text) - <span class="number">1</span></div><div class="line">            y1 = float(bbox.find(<span class="string">'ymin'</span>).text) - <span class="number">1</span></div><div class="line">            x2 = float(bbox.find(<span class="string">'xmax'</span>).text) - <span class="number">1</span></div><div class="line">            y2 = float(bbox.find(<span class="string">'ymax'</span>).text) - <span class="number">1</span></div><div class="line">            cls = self._class_to_ind[obj.find(<span class="string">'name'</span>).text.lower().strip()]</div><div class="line">            boxes[ix, :] = [x1, y1, x2, y2]</div><div class="line">            gt_classes[ix] = cls</div><div class="line">            overlaps[ix, cls] = <span class="number">1.0</span></div><div class="line">            seg_areas[ix] = (x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>)</div><div class="line"></div><div class="line">        overlaps = scipy.sparse.csr_matrix(overlaps)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> &#123;<span class="string">'boxes'</span> : boxes,</div><div class="line">                <span class="string">'gt_classes'</span>: gt_classes,</div><div class="line">                <span class="string">'gt_overlaps'</span> : overlaps,</div><div class="line">                <span class="string">'flipped'</span> : <span class="keyword">False</span>,</div><div class="line">                <span class="string">'seg_areas'</span> : seg_areas&#125;</div></pre></td></tr></table></figure>
<h5 id="修改后的函数-2"><a href="#修改后的函数-2" class="headerlink" title="修改后的函数"></a>修改后的函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_load_pascal_annotation</span><span class="params">(self, index)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Load image and bounding boxes info from XML file in the PASCAL VOC</div><div class="line">        format.</div><div class="line">        """</div><div class="line">        filename = os.path.join(self._data_path, <span class="string">'Annotations'</span>, index + <span class="string">'.xml'</span>)</div><div class="line">        tree = ET.parse(filename)</div><div class="line">        objs = tree.findall(<span class="string">'object'</span>)</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.config[<span class="string">'use_diff'</span>]:</div><div class="line">            <span class="comment"># Exclude the samples labeled as difficult</span></div><div class="line">            non_diff_objs = [</div><div class="line">                obj <span class="keyword">for</span> obj <span class="keyword">in</span> objs <span class="keyword">if</span> int(obj.find(<span class="string">'difficult'</span>).text) == <span class="number">0</span>]</div><div class="line">            <span class="comment"># if len(non_diff_objs) != len(objs):</span></div><div class="line">            <span class="comment">#     print 'Removed &#123;&#125; difficult objects'.format(</span></div><div class="line">            <span class="comment">#         len(objs) - len(non_diff_objs))</span></div><div class="line">            objs = non_diff_objs</div><div class="line">        num_objs = len(objs)</div><div class="line"></div><div class="line">        boxes = np.zeros((num_objs, <span class="number">4</span>), dtype=np.uint16)</div><div class="line">        gt_classes = np.zeros((num_objs), dtype=np.int32)</div><div class="line">        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)</div><div class="line">        <span class="comment"># "Seg" area for pascal is just the box area</span></div><div class="line">        seg_areas = np.zeros((num_objs), dtype=np.float32)</div><div class="line"></div><div class="line">        <span class="comment"># Load object bounding boxes into a data frame.</span></div><div class="line">        <span class="keyword">for</span> ix, obj <span class="keyword">in</span> enumerate(objs):</div><div class="line">            bbox = obj.find(<span class="string">'bndbox'</span>)</div><div class="line">            <span class="comment"># Make pixel indexes 0-based</span></div><div class="line">            <span class="comment"># 这里我把‘-1’全部删除掉了，防止有的数据是0开始，然后‘-1’导致变为负数，产生AssertError错误</span></div><div class="line">            x1 = float(bbox.find(<span class="string">'xmin'</span>).text)</div><div class="line">            y1 = float(bbox.find(<span class="string">'ymin'</span>).text)</div><div class="line">            x2 = float(bbox.find(<span class="string">'xmax'</span>).text)</div><div class="line">            y2 = float(bbox.find(<span class="string">'ymax'</span>).text)</div><div class="line">            cls = self._class_to_ind[obj.find(<span class="string">'name'</span>).text.lower().strip()]</div><div class="line">            boxes[ix, :] = [x1, y1, x2, y2]</div><div class="line">            gt_classes[ix] = cls</div><div class="line">            overlaps[ix, cls] = <span class="number">1.0</span></div><div class="line">            seg_areas[ix] = (x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>)</div><div class="line"></div><div class="line">        overlaps = scipy.sparse.csr_matrix(overlaps)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> &#123;<span class="string">'boxes'</span> : boxes,</div><div class="line">                <span class="string">'gt_classes'</span>: gt_classes,</div><div class="line">                <span class="string">'gt_overlaps'</span> : overlaps,</div><div class="line">                <span class="string">'flipped'</span> : <span class="keyword">False</span>,</div><div class="line">                <span class="string">'seg_areas'</span> : seg_areas&#125;</div></pre></td></tr></table></figure>
<h4 id="main函数修改"><a href="#main函数修改" class="headerlink" title="main函数修改"></a>main函数修改</h4><p>原始的函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="keyword">from</span> datasets.pascal_voc <span class="keyword">import</span> pascal_voc</div><div class="line">    d = pascal_voc(<span class="string">'trainval'</span>, <span class="string">'2007'</span>)</div><div class="line">    res = d.roidb</div><div class="line">    <span class="keyword">from</span> IPython <span class="keyword">import</span> embed; embed()</div></pre></td></tr></table></figure>
<p>修改后的函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="keyword">from</span> datasets.caltech <span class="keyword">import</span> caltech <span class="comment"># 导入caltech包</span></div><div class="line">    d = caltech(<span class="string">'train'</span>, <span class="string">'/home/jk/py-faster-rcnn/data/VOCdevkit'</span>)<span class="comment">#调用构造函数，传入imageset和路径</span></div><div class="line">    res = d.roidb</div><div class="line">    <span class="keyword">from</span> IPython <span class="keyword">import</span> embed; embed()</div></pre></td></tr></table></figure>
<p>至此读取接口修改完毕，该文件中的其他函数并未修改。</p>
<h3 id="修改factory-py文件"><a href="#修改factory-py文件" class="headerlink" title="修改factory.py文件"></a>修改factory.py文件</h3><p>当网络训练时会调用factory里面的get方法获得相应的imdb，首先在文件头import 把pascal_voc改成caltech</p>
<p>在这个文件作者生成了多个数据库的路径，我们自己数据库只要给定根路径即可，修改主要有以下4个</p>
<ul>
<li>函数之后有两个多级的for循环，也将其注释</li>
<li>直接定义<code>devkit</code>。</li>
<li>利用创建自己的训练和测试的imdb set，这里的name的格式为<code>caltech_{}</code>。</li>
</ul>
<h4 id="原始的代码"><a href="#原始的代码" class="headerlink" title="原始的代码"></a>原始的代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># --------------------------------------------------------</span></div><div class="line"><span class="comment"># Fast R-CNN</span></div><div class="line"><span class="comment"># Copyright (c) 2015 Microsoft</span></div><div class="line"><span class="comment"># Licensed under The MIT License [see LICENSE for details]</span></div><div class="line"><span class="comment"># Written by Ross Girshick</span></div><div class="line"><span class="comment"># --------------------------------------------------------</span></div><div class="line"></div><div class="line"><span class="string">"""Factory method for easily getting imdbs by name."""</span></div><div class="line"></div><div class="line">__sets = &#123;&#125;</div><div class="line"></div><div class="line"><span class="keyword">from</span> datasets.pascal_voc <span class="keyword">import</span> pascal_voc</div><div class="line"><span class="keyword">from</span> datasets.coco <span class="keyword">import</span> coco</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># Set up voc_&lt;year&gt;_&lt;split&gt; using selective search "fast" mode</span></div><div class="line"><span class="keyword">for</span> year <span class="keyword">in</span> [<span class="string">'2007'</span>, <span class="string">'2012'</span>]:</div><div class="line">    <span class="keyword">for</span> split <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>, <span class="string">'trainval'</span>, <span class="string">'test'</span>]:</div><div class="line">        name = <span class="string">'voc_&#123;&#125;_&#123;&#125;'</span>.format(year, split)</div><div class="line">        __sets[name] = (<span class="keyword">lambda</span> split=split, year=year: pascal_voc(split, year))</div><div class="line"></div><div class="line"><span class="comment"># Set up coco_2014_&lt;split&gt;</span></div><div class="line"><span class="keyword">for</span> year <span class="keyword">in</span> [<span class="string">'2014'</span>]:</div><div class="line">    <span class="keyword">for</span> split <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>, <span class="string">'minival'</span>, <span class="string">'valminusminival'</span>]:</div><div class="line">        name = <span class="string">'coco_&#123;&#125;_&#123;&#125;'</span>.format(year, split)</div><div class="line">        __sets[name] = (<span class="keyword">lambda</span> split=split, year=year: coco(split, year))</div><div class="line"></div><div class="line"><span class="comment"># Set up coco_2015_&lt;split&gt;</span></div><div class="line"><span class="keyword">for</span> year <span class="keyword">in</span> [<span class="string">'2015'</span>]:</div><div class="line">    <span class="keyword">for</span> split <span class="keyword">in</span> [<span class="string">'test'</span>, <span class="string">'test-dev'</span>]:</div><div class="line">        name = <span class="string">'coco_&#123;&#125;_&#123;&#125;'</span>.format(year, split)</div><div class="line">        __sets[name] = (<span class="keyword">lambda</span> split=split, year=year: coco(split, year))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_imdb</span><span class="params">(name)</span>:</span></div><div class="line">    <span class="string">"""Get an imdb (image database) by name."""</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> __sets.has_key(name):</div><div class="line">        <span class="keyword">raise</span> KeyError(<span class="string">'Unknown dataset: &#123;&#125;'</span>.format(name))</div><div class="line">    <span class="keyword">return</span> __sets[name]()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">list_imdbs</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""List all registered imdbs."""</span></div><div class="line">    <span class="keyword">return</span> __sets.keys()</div></pre></td></tr></table></figure>
<h4 id="修改后的文件"><a href="#修改后的文件" class="headerlink" title="修改后的文件"></a>修改后的文件</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># --------------------------------------------------------</span></div><div class="line"><span class="comment"># Fast R-CNN</span></div><div class="line"><span class="comment"># Copyright (c) 2015 Microsoft</span></div><div class="line"><span class="comment"># Licensed under The MIT License [see LICENSE for details]</span></div><div class="line"><span class="comment"># Written by Ross Girshick</span></div><div class="line"><span class="comment"># --------------------------------------------------------</span></div><div class="line"></div><div class="line"><span class="string">"""Factory method for easily getting imdbs by name."""</span></div><div class="line"></div><div class="line">__sets = &#123;&#125;</div><div class="line"></div><div class="line"><span class="keyword">from</span> datasets.caltech <span class="keyword">import</span> caltech <span class="comment"># 导入caltech包</span></div><div class="line"><span class="comment">#from datasets.coco import coco</span></div><div class="line"><span class="comment">#import numpy as np</span></div><div class="line"></div><div class="line">devkit = <span class="string">'/home/jk/py-faster-rcnn/data/VOCdevkit'</span></div><div class="line"><span class="comment"># Set up voc_&lt;year&gt;_&lt;split&gt; using selective search "fast" mode</span></div><div class="line"><span class="comment">#for year in ['2007', '2012']:</span></div><div class="line"><span class="comment">#    for split in ['train', 'val', 'trainval', 'test']:</span></div><div class="line"><span class="comment">#        name = 'voc_&#123;&#125;_&#123;&#125;'.format(year, split)</span></div><div class="line"><span class="comment">#        __sets[name] = (lambda split=split, year=year: pascal_voc(split, year))</span></div><div class="line"></div><div class="line"><span class="comment"># Set up coco_2014_&lt;split&gt;</span></div><div class="line"><span class="comment">#for year in ['2014']:</span></div><div class="line"><span class="comment">#    for split in ['train', 'val', 'minival', 'valminusminival']:</span></div><div class="line"><span class="comment">#        name = 'coco_&#123;&#125;_&#123;&#125;'.format(year, split)</span></div><div class="line"><span class="comment">#        __sets[name] = (lambda split=split, year=year: coco(split, year))</span></div><div class="line"></div><div class="line"><span class="comment"># Set up coco_2015_&lt;split&gt;</span></div><div class="line"><span class="comment">#for year in ['2015']:</span></div><div class="line"><span class="comment">#    for split in ['test', 'test-dev']:</span></div><div class="line"><span class="comment">#        name = 'coco_&#123;&#125;_&#123;&#125;'.format(year, split)</span></div><div class="line"><span class="comment">#        __sets[name] = (lambda split=split, year=year: coco(split, year))</span></div><div class="line"></div><div class="line"><span class="comment"># Set up caltech_&lt;split&gt;</span></div><div class="line"><span class="keyword">for</span> split <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'test'</span>]:</div><div class="line">    name = <span class="string">'caltech_&#123;&#125;'</span>.format(split)</div><div class="line">    __sets[name] = (<span class="keyword">lambda</span> imageset=split, devkit=devkit: caltech(imageset, devkit))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_imdb</span><span class="params">(name)</span>:</span></div><div class="line">    <span class="string">"""Get an imdb (image database) by name."""</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> __sets.has_key(name):</div><div class="line">        <span class="keyword">raise</span> KeyError(<span class="string">'Unknown dataset: &#123;&#125;'</span>.format(name))</div><div class="line">    <span class="keyword">return</span> __sets[name]()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">list_imdbs</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""List all registered imdbs."""</span></div><div class="line">    <span class="keyword">return</span> __sets.keys()</div></pre></td></tr></table></figure>
<h3 id="修改init-py文件"><a href="#修改init-py文件" class="headerlink" title="修改init.py文件"></a>修改<strong>init</strong>.py文件</h3><p>在行首添加上 <code>from .caltech import caltech</code></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>坐标的顺序我再说一次，要左上右下，并且x1必须要小于x2，这个是基本，反了会在坐标水平变换的时候会出错，坐标从0开始，如果已经是0，则不需要再-1。</li>
<li>训练图像的大小不要太大，否则生成的OP也会太多，速度太慢，图像样本大小最好调整到500，600左右，然后再提取OP</li>
<li>如果读取并生成pkl文件之后，实际数据内容或者顺序还有问题，记得要把data/cache/下面的pkl文件给删掉。</li>
</ul>
<h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol>
<li><a href="http://www.cnblogs.com/louyihang-loves-baiyan/p/4903231.html" target="_blank" rel="external"><strong>Fast RCNN训练自己的数据集 （2修改读写接口）</strong></a></li>
<li><a href="http://www.cnblogs.com/CarryPotMan/p/5390336.html" target="_blank" rel="external"><strong>Faster R-CNN教程</strong></a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;这部分主要讲如何修改Faster R-CNN的代码，来训练自己的数据集，首先确保你已经编译安装了py-faster-rcnn，并且准备好了数据集，具体可参考我&lt;a href=&quot;http://jacobkong.github.io/posts/2093106769/&quot;&gt;上一篇文章&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="经验" scheme="http://jacobkong.github.io/blog/categories/%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/blog/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/blog/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/blog/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习实践经验：用Faster R-CNN训练行人检测数据集Caltech——准备工作</title>
    <link href="http://jacobkong.github.io/blog/2093106769/"/>
    <id>http://jacobkong.github.io/blog/2093106769/</id>
    <published>2017-01-15T22:32:24.000Z</published>
    <updated>2017-02-28T06:55:04.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Faster R-CNN是Ross Girshick大神在Fast R-CNN基础上提出的又一个更加快速、更高mAP的用于目标检测的深度学习框架，它对Fast R-CNN进行的最主要的优化就是在Region Proposal阶段，引入了Region Proposal Network (RPN)来进行Region Proposal，同时可以达到和检测网络共享整个图片的卷积网络特征的目标，使得region proposal几乎是<strong>cost free</strong>的。</p>
<p>关于Faster R-CNN的详细介绍，可以参考我<a href="http://jacobkong.github.io/posts/3802700508/">上一篇博客</a>。</p>
<p>Faster R-CNN的代码是开源的，有两个版本：<a href="https://github.com/ShaoqingRen/faster_rcnn" target="_blank" rel="external">MATLAB版本(<strong>faster_rcnn</strong>)</a>，<a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="external">Python版本(<strong>py-faster-rcnn</strong>)</a>。</p>
<p>这里我主要使用的是Python版本，Python版本在测试期间会比MATLAB版本慢10%，因为Python layers中的一些操作是在CPU中执行的，但是准确率应该是差不多的。</p>
<a id="more"></a>
<h2 id="准备工作1——py-faster-rcnn的编译安装测试"><a href="#准备工作1——py-faster-rcnn的编译安装测试" class="headerlink" title="准备工作1——py-faster-rcnn的编译安装测试"></a>准备工作1——py-faster-rcnn的编译安装测试</h2><h3 id="py-faster-rcnn的编译安装"><a href="#py-faster-rcnn的编译安装" class="headerlink" title="py-faster-rcnn的编译安装"></a>py-faster-rcnn的编译安装</h3><ol>
<li><p>克隆Faster R-CNN仓库：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> --recursive https://github.com/rbgirshick/py-faster-rcnn.git</div></pre></td></tr></table></figure>
<p>一定要加上<code>--recursive</code>标志，假设克隆后的文件夹名字叫<code>py-faster-rcnn</code></p>
</li>
<li><p>编译Cython模块：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> py-faster-rcnn/lib</div><div class="line">make</div></pre></td></tr></table></figure>
</li>
<li><p>编译里面的Caffe和pycaffe：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> py-faster-rcnn/caffe-fast-rcnn</div><div class="line"><span class="comment"># 按照编译Caffe的方法，进行编译</span></div><div class="line"><span class="comment"># 注意Makefile.config的修改，这里不再赘述Caffe的安装</span></div><div class="line"><span class="comment"># 编译</span></div><div class="line">make -j8 &amp;&amp; make pycaffe</div></pre></td></tr></table></figure>
</li>
<li><p>这里贴上我的<code>Makefile.config</code>文件代码，根据你的情况进行相应修改</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## Refer to http://caffe.berkeleyvision.org/installation.html</span></div><div class="line"><span class="comment"># Contributions simplifying and improving our build system are welcome!</span></div><div class="line"></div><div class="line"><span class="comment"># cuDNN acceleration switch (uncomment to build with cuDNN).</span></div><div class="line">USE_CUDNN := 1</div><div class="line"></div><div class="line"><span class="comment"># CPU-only switch (uncomment to build without GPU support).</span></div><div class="line"><span class="comment"># CPU_ONLY := 1</span></div><div class="line"></div><div class="line"><span class="comment"># uncomment to disable IO dependencies and corresponding data layers</span></div><div class="line"><span class="comment"># USE_OPENCV := 0</span></div><div class="line"><span class="comment"># USE_LEVELDB := 0</span></div><div class="line"><span class="comment"># USE_LMDB := 0</span></div><div class="line"></div><div class="line"><span class="comment"># uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)</span></div><div class="line"><span class="comment"># You should not set this flag if you will be reading LMDBs with any</span></div><div class="line"><span class="comment"># possibility of simultaneous read and write</span></div><div class="line"><span class="comment"># ALLOW_LMDB_NOLOCK := 1</span></div><div class="line"></div><div class="line"><span class="comment"># Uncomment if you're using OpenCV 3</span></div><div class="line">OPENCV_VERSION := 3</div><div class="line"></div><div class="line"><span class="comment"># To customize your choice of compiler, uncomment and set the following.</span></div><div class="line"><span class="comment"># N.B. the default for Linux is g++ and the default for OSX is clang++</span></div><div class="line"><span class="comment"># CUSTOM_CXX := g++</span></div><div class="line"></div><div class="line"><span class="comment"># CUDA directory contains bin/ and lib/ directories that we need.</span></div><div class="line">CUDA_DIR := /usr/<span class="built_in">local</span>/cuda</div><div class="line"><span class="comment"># On Ubuntu 14.04, if cuda tools are installed via</span></div><div class="line"><span class="comment"># "sudo apt-get install nvidia-cuda-toolkit" then use this instead:</span></div><div class="line"><span class="comment"># CUDA_DIR := /usr</span></div><div class="line"></div><div class="line"><span class="comment"># CUDA architecture setting: going with all of them.</span></div><div class="line"><span class="comment"># For CUDA &lt; 6.0, comment the *_50 lines for compatibility.</span></div><div class="line">CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \</div><div class="line">-gencode arch=compute_20,code=sm_21 \</div><div class="line">-gencode arch=compute_30,code=sm_30 \</div><div class="line">-gencode arch=compute_35,code=sm_35 \</div><div class="line">-gencode arch=compute_50,code=sm_50 \</div><div class="line">-gencode arch=compute_50,code=compute_50</div><div class="line"></div><div class="line"><span class="comment"># BLAS choice:</span></div><div class="line"><span class="comment"># atlas for ATLAS (default)</span></div><div class="line"><span class="comment"># mkl for MKL</span></div><div class="line"><span class="comment"># open for OpenBlas</span></div><div class="line">BLAS :=mkl</div><div class="line"><span class="comment"># Custom (MKL/ATLAS/OpenBLAS) include and lib directories.</span></div><div class="line"><span class="comment"># Leave commented to accept the defaults for your choice of BLAS</span></div><div class="line"><span class="comment"># (which should work)!</span></div><div class="line"><span class="comment"># BLAS_INCLUDE := /path/to/your/blas</span></div><div class="line"><span class="comment"># BLAS_LIB := /path/to/your/blas</span></div><div class="line"></div><div class="line"><span class="comment"># Homebrew puts openblas in a directory that is not on the standard search path</span></div><div class="line"><span class="comment"># BLAS_INCLUDE := $(shell brew --prefix openblas)/include</span></div><div class="line"><span class="comment"># BLAS_LIB := $(shell brew --prefix openblas)/lib</span></div><div class="line"></div><div class="line"><span class="comment"># This is required only if you will compile the matlab interface.</span></div><div class="line"><span class="comment"># MATLAB directory should contain the mex binary in /bin.</span></div><div class="line">MATLAB_DIR := /usr/<span class="built_in">local</span>/MATLAB/R2016b</div><div class="line"><span class="comment"># MATLAB_DIR := /Applications/MATLAB_R2012b.app</span></div><div class="line"></div><div class="line"><span class="comment"># <span class="doctag">NOTE:</span> this is required only if you will compile the python interface.</span></div><div class="line"><span class="comment"># We need to be able to find Python.h and numpy/arrayobject.h.</span></div><div class="line"><span class="comment"># PYTHON_INCLUDE := /usr/include/python2.7 \</span></div><div class="line">/usr/lib/python2.7/dist-packages/numpy/core/include</div><div class="line"><span class="comment"># Anaconda Python distribution is quite popular. Include path:</span></div><div class="line"><span class="comment"># Verify anaconda location, sometimes it's in root.</span></div><div class="line">ANACONDA_HOME := $(HOME)/anaconda</div><div class="line">PYTHON_INCLUDE := $(ANACONDA_HOME)/include \</div><div class="line">$(ANACONDA_HOME)/include/python2.7 \</div><div class="line">$(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \</div><div class="line">$ /usr/include/python2.7</div><div class="line"><span class="comment"># Uncomment to use Python 3 (default is Python 2)</span></div><div class="line"><span class="comment"># PYTHON_LIBRARIES := boost_python3 python3.5m</span></div><div class="line"><span class="comment"># PYTHON_INCLUDE := /usr/include/python3.5m \</span></div><div class="line"><span class="comment"># /usr/lib/python3.5/dist-packages/numpy/core/include</span></div><div class="line"></div><div class="line"><span class="comment"># We need to be able to find libpythonX.X.so or .dylib.</span></div><div class="line"><span class="comment"># PYTHON_LIB := /usr/lib</span></div><div class="line">PYTHON_LIB := $(ANACONDA_HOME)/lib</div><div class="line"></div><div class="line"><span class="comment"># Homebrew installs numpy in a non standard path (keg only)</span></div><div class="line"><span class="comment"># PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include</span></div><div class="line"><span class="comment"># PYTHON_LIB += $(shell brew --prefix numpy)/lib</span></div><div class="line"></div><div class="line"><span class="comment"># Uncomment to support layers written in Python (will link against Python libs)</span></div><div class="line">WITH_PYTHON_LAYER := 1</div><div class="line"></div><div class="line"><span class="comment"># Whatever else you find you need goes here.</span></div><div class="line"><span class="comment"># INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include</span></div><div class="line"><span class="comment"># LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib</span></div><div class="line">INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/<span class="built_in">local</span>/include /usr/include/hdf5/serial </div><div class="line">LIBRARY_DIRS := $(PYTHON_LIB) /usr/<span class="built_in">local</span>/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial</div><div class="line"></div><div class="line"><span class="comment"># If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies</span></div><div class="line"><span class="comment"># INCLUDE_DIRS += $(shell brew --prefix)/include</span></div><div class="line"><span class="comment"># LIBRARY_DIRS += $(shell brew --prefix)/lib</span></div><div class="line"></div><div class="line"><span class="comment"># Uncomment to use `pkg-config` to specify OpenCV library paths.</span></div><div class="line"><span class="comment"># (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)</span></div><div class="line"><span class="comment"># USE_PKG_CONFIG := 1</span></div><div class="line"></div><div class="line"><span class="comment"># N.B. both build and distribute dirs are cleared on `make clean`</span></div><div class="line">BUILD_DIR := build</div><div class="line">DISTRIBUTE_DIR := distribute</div><div class="line"></div><div class="line"><span class="comment"># Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171</span></div><div class="line"><span class="comment"># DEBUG := 1</span></div><div class="line"></div><div class="line"><span class="comment"># The ID of the GPU that 'make runtest' will use to run unit tests.</span></div><div class="line">TEST_GPUID := 0</div><div class="line"></div><div class="line"><span class="comment"># enable pretty build (comment to see full commands)</span></div><div class="line">Q ?= @</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Demo运行"><a href="#Demo运行" class="headerlink" title="Demo运行"></a>Demo运行</h3><p>为了检验你的py-faster-rcnn是否成功安装，作者给出了一个demo，可以利用在PASCAL VOC2007数据集上体现训练好的模型，来进行demo的运行，步骤如下：</p>
<ol>
<li><p>下载预训练好的Faster R-CNN检测器：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> py-faster-rcnn</div><div class="line">./data/scripts/fetch_faster_rcnn_models.sh</div></pre></td></tr></table></figure>
<p>这条命令会自动下载名为<code>faster_rcnn_models.tgz</code>的文件，解压后会创建<code>data/faster_rcnn_models</code>文件夹，里面会有两个模型：</p>
<ul>
<li>ZF_faster_rcnn_final.caffemodel：在ZF网络模型下训练所得</li>
<li>VGG16_faster_rcnn_final.caffemodel：在VGG16网络模型下训练所得。</li>
</ul>
</li>
<li><p>运行demo：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> py-faster-rcnn</div><div class="line">./tools/demo.py</div></pre></td></tr></table></figure>
</li>
<li><p>demo会检测5张图片，这5张图片放在<code>data/demo/</code>文件夹下，其中一张的检测结果如下：</p>
<p><img src="https://ww2.sinaimg.cn/large/006tNbRwgy1fcthbwzgobj30gv0e0wh9.jpg" alt=""></p>
</li>
<li><p>至此如果上述过程没有出错，那么py-faster-rcnn算是成功编译安装。</p>
</li>
<li><p>若出现报错如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ImportError: /xx/xx/xx/py-faster-rcnn/tools/../lib/nms/cpu_nms.so: undefined symbol: PyFPE_jbuf</div></pre></td></tr></table></figure>
<p>需要将<code>lib/fast_rcnn/nms_wrapper.py</code>文件中的<code>from nms.cpu_nms import cpu_nms</code>注释掉即可。</p>
</li>
</ol>
<h2 id="准备工作2——Caltech数据集"><a href="#准备工作2——Caltech数据集" class="headerlink" title="准备工作2——Caltech数据集"></a>准备工作2——Caltech数据集</h2><p>由于Faster R-CNN的一部分实验是在PASCAL VOC2007数据集上进行的，所以要想用Faster R-CNN训练我们自己的数据集，首先应该搞清楚PASCAL VOC2007数据集中的目录、图片、标注格式，这样我们才能用自己的数据集制作出类似于PASCAL VOC2007类似的数据集，供Faster R-CNN来进行训练及测试。</p>
<h3 id="获取PASCAL-VOC2007数据集"><a href="#获取PASCAL-VOC2007数据集" class="headerlink" title="获取PASCAL VOC2007数据集"></a>获取PASCAL VOC2007数据集</h3><p>这一部分不是必须的，如果你需要PASCAL VOC2007数据集，可以利用以下命令获取数据集，但<strong>我们下载VOC数据集的目的主要是观察他的文件结构和文件内容，以便于我们构建符合要求的自己的数据集。</strong></p>
<ol>
<li><p>创建一个专门用来存数据集的地方，假设是<code>$HOME/data</code>文件夹。</p>
</li>
<li><p>下载PASCAL VOC2007的训练、验证和测试数据集：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span>/data</div><div class="line">wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar</div><div class="line">wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar</div></pre></td></tr></table></figure>
</li>
<li><p>下载完后用以下命令解压：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tar xvf VOCtrainval_06-Nov-2007.tar</div><div class="line">tar xvf VOCtest_06-Nov-2007.tar</div></pre></td></tr></table></figure>
</li>
<li><p>会得到如下文件结构：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$HOME</span>/data/VOCdevkit/                        <span class="comment"># 根文件夹</span></div><div class="line"><span class="variable">$HOME</span>/data/VOCdevkit/VOC2007                 <span class="comment"># VOC2007文件夹</span></div><div class="line"><span class="variable">$HOME</span>/data/VOCdevkit/VOC2007/Annotations     <span class="comment"># 标记文件夹</span></div><div class="line"><span class="variable">$HOME</span>/data/VOCdevkit/VOC2007/ImageSets       <span class="comment"># 供train.txt、test.txt、val.txt等文件存放的文件夹</span></div><div class="line"><span class="variable">$HOME</span>/data/VOCdevkit/VOC2007/JPEGImages      <span class="comment"># 存放图片文件夹</span></div><div class="line"><span class="comment"># ... 以及其他的文件夹及子文件夹 ...</span></div></pre></td></tr></table></figure>
</li>
<li><p>创建快捷方式symlinks来连接到VOC数据集存放的地方：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> py-faster-rcnn/data</div><div class="line">ln <span class="_">-s</span> <span class="variable">$HOME</span>/data/VOCdevkit/ VOCdevkit</div></pre></td></tr></table></figure>
<p>这里需要把<code>$HOME/data/VOCdevkit/</code>改为你存放<code>VOCdevkit</code>文件夹的路径</p>
<p><strong>最好使用symlinks来在共享同一份数据集，防止数据集多处拷贝，占用空间。</strong></p>
</li>
<li><p>至此VOC数据集创建完毕。</p>
</li>
</ol>
<h3 id="PASCAL-VOC数据集的分析"><a href="#PASCAL-VOC数据集的分析" class="headerlink" title="PASCAL VOC数据集的分析"></a>PASCAL VOC数据集的分析</h3><p>PASCAL VOC数据集的文件结构，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">└── VOCdevkit</div><div class="line">    └── VOC2007　</div><div class="line">        ├── Annotations　　</div><div class="line">        ├── ImageSets　　</div><div class="line">        │   ├── Layout　　</div><div class="line">        │   ├── Main　　</div><div class="line">        │   └── Segmentation　　</div><div class="line">        ├── JPEGImages　　</div><div class="line">        ├── SegmentationClass　　</div><div class="line">        └── SegmentationObject</div></pre></td></tr></table></figure>
<h4 id="Annotations"><a href="#Annotations" class="headerlink" title="Annotations"></a>Annotations</h4><p>该文件夹主要用来存放图片标注（即为ground truth），文件是.xml格式，每张图片都有一个.xml文件与之对应。选取其中一个文件进行如下分析：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">annotation</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">folder</span>&gt;</span>VOC2007<span class="tag">&lt;/<span class="name">folder</span>&gt;</span> # 必须有，父文件夹的名称</div><div class="line">	<span class="tag">&lt;<span class="name">filename</span>&gt;</span>000005.jpg<span class="tag">&lt;/<span class="name">filename</span>&gt;</span>　#　必须有</div><div class="line">	<span class="tag">&lt;<span class="name">source</span>&gt;</span>　# 可有可无</div><div class="line">		<span class="tag">&lt;<span class="name">database</span>&gt;</span>The VOC2007 Database<span class="tag">&lt;/<span class="name">database</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">annotation</span>&gt;</span>PASCAL VOC2007<span class="tag">&lt;/<span class="name">annotation</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">image</span>&gt;</span>flickr<span class="tag">&lt;/<span class="name">image</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">flickrid</span>&gt;</span>325991873<span class="tag">&lt;/<span class="name">flickrid</span>&gt;</span></div><div class="line">	<span class="tag">&lt;/<span class="name">source</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">owner</span>&gt;</span>　# 可有可无</div><div class="line">		<span class="tag">&lt;<span class="name">flickrid</span>&gt;</span>archintent louisville<span class="tag">&lt;/<span class="name">flickrid</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>?<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">	<span class="tag">&lt;/<span class="name">owner</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">size</span>&gt;</span>　# 表示图像大小</div><div class="line">		<span class="tag">&lt;<span class="name">width</span>&gt;</span>500<span class="tag">&lt;/<span class="name">width</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">height</span>&gt;</span>375<span class="tag">&lt;/<span class="name">height</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">depth</span>&gt;</span>3<span class="tag">&lt;/<span class="name">depth</span>&gt;</span></div><div class="line">	<span class="tag">&lt;/<span class="name">size</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">segmented</span>&gt;</span>0<span class="tag">&lt;/<span class="name">segmented</span>&gt;</span>　# 用于分割</div><div class="line">	<span class="tag">&lt;<span class="name">object</span>&gt;</span>　# 目标信息，类别，bbox信息，图片中每个目标对应一个<span class="tag">&lt;<span class="name">object</span>&gt;</span>标签</div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>chair<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">pose</span>&gt;</span>Rear<span class="tag">&lt;/<span class="name">pose</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">truncated</span>&gt;</span>0<span class="tag">&lt;/<span class="name">truncated</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">difficult</span>&gt;</span>0<span class="tag">&lt;/<span class="name">difficult</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">bndbox</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">xmin</span>&gt;</span>263<span class="tag">&lt;/<span class="name">xmin</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">ymin</span>&gt;</span>211<span class="tag">&lt;/<span class="name">ymin</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">xmax</span>&gt;</span>324<span class="tag">&lt;/<span class="name">xmax</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">ymax</span>&gt;</span>339<span class="tag">&lt;/<span class="name">ymax</span>&gt;</span></div><div class="line">		<span class="tag">&lt;/<span class="name">bndbox</span>&gt;</span></div><div class="line">	<span class="tag">&lt;/<span class="name">object</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">object</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>chair<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">pose</span>&gt;</span>Unspecified<span class="tag">&lt;/<span class="name">pose</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">truncated</span>&gt;</span>1<span class="tag">&lt;/<span class="name">truncated</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">difficult</span>&gt;</span>1<span class="tag">&lt;/<span class="name">difficult</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">bndbox</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">xmin</span>&gt;</span>5<span class="tag">&lt;/<span class="name">xmin</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">ymin</span>&gt;</span>244<span class="tag">&lt;/<span class="name">ymin</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">xmax</span>&gt;</span>67<span class="tag">&lt;/<span class="name">xmax</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">ymax</span>&gt;</span>374<span class="tag">&lt;/<span class="name">ymax</span>&gt;</span></div><div class="line">		<span class="tag">&lt;/<span class="name">bndbox</span>&gt;</span></div><div class="line">	<span class="tag">&lt;/<span class="name">object</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">annotation</span>&gt;</span></div></pre></td></tr></table></figure>
<p><strong>需要注意的</strong>，对于我们自己准备的xml标记文件中，每个<code>&lt;object&gt;</code>标签中的<code>&lt;xmin&gt;</code>和<code>&lt;ymin&gt;</code>标签中所对应的坐标值最好大于0，千万不能为负数，否则在训练过程中会报错：<code>AssertionError: assert (boxes[:, 2]) &gt;= boxes[:, 0]).all()</code>，如下：</p>
<p><img src="https://ww2.sinaimg.cn/large/006tNbRwly1fctnwv48gzj30h701waa0.jpg" alt=""></p>
<p>所以为了能够顺利训练，一定要仔细检查自己的xml文件中的左上角的坐标是否都为正。我被这个bug卡了一两天，最终把自己标记中所有的错误坐标找出来，才得以顺利训练。</p>
<h4 id="ImageSets"><a href="#ImageSets" class="headerlink" title="ImageSets"></a>ImageSets</h4><p>ImageSets文件夹下有三个子文件夹，这里我们只需关注Main文件夹即可。Main文件夹下主要用到的是train.txt、val.txt、test.txt、trainval.txt文件，每个文件中写着供训练、验证、测试所用的文件名的集合，如下：</p>
<p><img src="https://ww3.sinaimg.cn/large/006tNbRwly1fctobmg8rkj302f03vjrc.jpg" alt=""></p>
<h4 id="JPEGImages"><a href="#JPEGImages" class="headerlink" title="JPEGImages"></a>JPEGImages</h4><p>JPEGImages文件夹下主要存放着所有的.jpg文件格式的输入图片，不在赘述。</p>
<h3 id="制作VOC类似的Caltech数据集"><a href="#制作VOC类似的Caltech数据集" class="headerlink" title="制作VOC类似的Caltech数据集"></a>制作VOC类似的Caltech数据集</h3><p>经过以上对PASCAL VOC数据集文件结构的分析，我们仿照其，创建首先创建类似的文件结构即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">└── VOCdevkit</div><div class="line">    └── VOC2007　</div><div class="line">    └── Caltech　</div><div class="line">        ├── Annotations　　</div><div class="line">        ├── ImageSets　　　</div><div class="line">        │   └── Main　　</div><div class="line">        └── JPEGImages</div></pre></td></tr></table></figure>
<p>我建议将Caltech文件创建一个symlinks链接到VOCdevkit文件夹之下，因为这样会方便之后训练代码的修改。</p>
<ul>
<li>至于<a href="https://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/" target="_blank" rel="external">Caltech数据集</a>如何从.seq文件转化为一张张.jpg图片，这里可以<a href="https://github.com/mitmul/caltech-pedestrian-dataset-converter" target="_blank" rel="external">参考这里</a>。</li>
<li>至于Annotations中一个个.xml标记文件是实验室师兄给我的，<a href="https://github.com/mitmul/caltech-pedestrian-dataset-converter" target="_blank" rel="external">上面提到的方法</a>也可以转化，但是并不符合要求。</li>
<li>至于ImageSets中的train.txt是根据.xml文件得来的，test.txt是每个seq中每隔30帧取一帧图片得来的。</li>
</ul>
<p>以上所有和<a href="https://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/" target="_blank" rel="external">Caltech数据集</a>有关的文件，都可以直接邮件与我联系，我直接发给你，可以省下不少制作数据集的时间。</p>
<h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol>
<li><a href="http://www.cnblogs.com/louyihang-loves-baiyan/p/4885659.html?utm_source=tuicool&amp;utm_medium=referral" target="_blank" rel="external"><strong>FastRCNN 训练自己数据集 (1编译配置)</strong></a></li>
<li><a href="https://saicoco.github.io/object-detection-4/" target="_blank" rel="external"><strong>目标检测—Faster RCNN2</strong></a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Faster R-CNN是Ross Girshick大神在Fast R-CNN基础上提出的又一个更加快速、更高mAP的用于目标检测的深度学习框架，它对Fast R-CNN进行的最主要的优化就是在Region Proposal阶段，引入了Region Proposal Network (RPN)来进行Region Proposal，同时可以达到和检测网络共享整个图片的卷积网络特征的目标，使得region proposal几乎是&lt;strong&gt;cost free&lt;/strong&gt;的。&lt;/p&gt;
&lt;p&gt;关于Faster R-CNN的详细介绍，可以参考我&lt;a href=&quot;http://jacobkong.github.io/posts/3802700508/&quot;&gt;上一篇博客&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;Faster R-CNN的代码是开源的，有两个版本：&lt;a href=&quot;https://github.com/ShaoqingRen/faster_rcnn&quot;&gt;MATLAB版本(&lt;strong&gt;faster_rcnn&lt;/strong&gt;)&lt;/a&gt;，&lt;a href=&quot;https://github.com/rbgirshick/py-faster-rcnn&quot;&gt;Python版本(&lt;strong&gt;py-faster-rcnn&lt;/strong&gt;)&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;这里我主要使用的是Python版本，Python版本在测试期间会比MATLAB版本慢10%，因为Python layers中的一些操作是在CPU中执行的，但是准确率应该是差不多的。&lt;/p&gt;
    
    </summary>
    
      <category term="经验" scheme="http://jacobkong.github.io/blog/categories/%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/blog/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/blog/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/blog/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：Faster R-CNN</title>
    <link href="http://jacobkong.github.io/blog/3802700508/"/>
    <id>http://jacobkong.github.io/blog/3802700508/</id>
    <published>2016-12-16T22:32:24.000Z</published>
    <updated>2017-03-09T06:32:01.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li><strong>Region Proposal的计算</strong>是基于Region Proposal算法来假设物体位置的物体检测网络比如：<strong>SPPnet, Fast R-CNN</strong>运行时间的瓶颈。</li>
<li>Faster R-CNN引入了<strong>Region Proposal Network（RPN）</strong>来和检测网络共享整个图片的卷积网络特征，因此使得region proposal几乎是<strong>cost free</strong>的。</li>
<li>RPN-&gt;预测物体边界（object bounds）和在每一位置的分数（objectness score）</li>
<li>通过在一个网络中共享RPN和Fast R-CNN的卷积特征来融合两者——<strong>使用“attention”机制。</strong></li>
<li>300 proposals pre image.</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>RP是当前许多先进检测系统的瓶颈。</li>
<li>Region proposal methods:<ul>
<li>Selective Search: one of the most popular method </li>
<li>EdgeBoxes: trade off between proposal quality and speed.</li>
<li>region proposal这一步依旧和检测网络花费同样多的时间。</li>
</ul>
</li>
<li>Fast R-CNN生成的feature map 也能用来生成RP。在这些卷积特征之上我们通过这样的方式构建RPN：通过添加几个额外的卷积层来模拟一个regular grid上每一个位置的regress region bounds和objectness scores。<strong>所以RPN也是一种fully convolutional network(FCN)</strong>，从而可以端到端训练来产生detection proposals。</li>
<li><strong>anchor boxes</strong>：references at multiple scales and aspect ratios. 我们的方法可以看成pyramid of regression reference，从而避免枚举多尺寸、多横纵比的images或者filters</li>
</ul>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul>
<li>R-CNN主要是一个分类器，他不能预测object bounds，他的准确性依赖于Region proposal模块的表现</li>
</ul>
<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><ul>
<li>由两个模块组成：<ul>
<li>第一个模块：A deep fuuly convolutional network that proposes regions，<strong>用来proposes regions</strong>.</li>
<li>第二个模块：Fast R-CNN检测器，使用第一模块提出的regions。</li>
</ul>
</li>
<li><strong>Attention mechanisms</strong>：RPN module告诉Fast R-CNN module 往哪里看（where to look）</li>
</ul>
<h3 id="Region-Proposal-Networks"><a href="#Region-Proposal-Networks" class="headerlink" title="Region Proposal Networks"></a>Region Proposal Networks</h3><ul>
<li><p>输入：一张<strong>任意尺寸</strong>的图片。</p>
</li>
<li><p>输出：一组矩形object proposal，每个proposal都有一个score。</p>
</li>
<li><p>是一个fully convolutional network（<strong>FCN</strong>），由于我们需要在RPN和Fast RCNN之间共享权值，所以我们假设两个网络<strong>共享一组共同的卷积层</strong>。</p>
</li>
<li><p>为了生成region proposals，我们在<strong>最后一层共享卷积层</strong>输出的feature map上滑动一个微型网络。这个微型网络将输入的feature map上的nxn的空间窗口作为输入。每一个滑动窗口被映射为一个低维特征(ZF: 256-d, VGG: 512-d, 之后跟着ReLU层)。这些特征然后被送到两个sibling全连接层中——<strong>一个box-regression(reg)层</strong>和<strong>一个box-classification(cls)层。</strong></p>
</li>
<li><p>注意：因为微型网络以滑动窗口方式操作，<strong>所以完全连接层在所有空间位置上共享。</strong> 这种结构自然地通过一个n×n卷积层，后面是两个同级<strong>1×1</strong>卷积层（分别用于rpn_reg和rpn_cls）来实现。</p>
</li>
<li><p>生成region proposal的思路：<br><img src="https://ww4.sinaimg.cn/large/006tKfTcjw1fcb3fr2k40j30cq07tt9a.jpg" alt=""></p>
</li>
<li><p>rpn网络结构定义如下：</p>
<p><img src="https://ww4.sinaimg.cn/large/006tNc79ly1fdgik44luaj30pf0di40j.jpg" alt=""></p>
<p>​</p>
<p>​</p>
</li>
</ul>
<h4 id="Anchors"><a href="#Anchors" class="headerlink" title="Anchors"></a>Anchors</h4><ul>
<li>假设每个位置最大可能的proposal的数量为k，在每个sliding-window位置，同时预测几个RP：<ul>
<li><em>reg layer</em>：有4k个输出</li>
<li><em>cls layer</em>：有2k个输出，指出该每一个proposal是否是object，<strong>estimate probability of object or not object for each proposal</strong>。</li>
</ul>
</li>
<li>k个proposal相对于k个参考框（reference boxes）而参数化，我们将参考框称为<strong>anchor</strong>。</li>
<li>一个anchor位于sliding window的中间，同时关联着一个scale和aspect ration。</li>
</ul>
<h5 id="Translation-Invariant-Anchors-平移不变性"><a href="#Translation-Invariant-Anchors-平移不变性" class="headerlink" title="Translation-Invariant Anchors(平移不变性)"></a>Translation-Invariant Anchors(平移不变性)</h5><ul>
<li>如果移动了一张图像中的一个物体，这proposal应该也移动了，而且相同的函数可以预测出热议未知的proposal。MultiBox不具备如此功能</li>
<li>平移不变性可以减少模型大小。</li>
</ul>
<h5 id="Multi-Scale-Anchor-as-Regression-References"><a href="#Multi-Scale-Anchor-as-Regression-References" class="headerlink" title="Multi-Scale Anchor as Regression References"></a>Multi-Scale Anchor as Regression References</h5><ul>
<li>Two popular ways for multi-scale predictions:<ul>
<li>第一种：based on image/feature pyramids, 如：DPM and CNN-based methods。图像被resized成不同尺寸，然后为每一种尺寸计算feature maps(HOG或者deep convolutional features)。这种方法比较<strong>费时</strong>。</li>
<li>第二种：use sliding windows of multiple scales (and/or aspect ratios) on the feature maps.——<strong>filters金字塔</strong>。第二种方法经常和第一种方法联合使用</li>
</ul>
</li>
<li>本论文的方法：<strong>anchor金字塔</strong>——more cost-efficient，只依靠单尺寸的图像和feature map。</li>
<li>The design of multiscale anchors is a <strong>key </strong>component for <strong>sharing features</strong> without extra cost for addressing scales.</li>
</ul>
<h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><ul>
<li><p>为了训练RPN，我们给每个anchor设置了一个二元标签（是物体或者不是物体）</p>
</li>
<li><p>两类anchor是有<strong>正标签</strong>（is object）的：</p>
<ul>
<li>anchor/anchors with highest IoU overlap with a ground-truth box。</li>
<li>an anchor that has IoU overlap higher than 0.7 with any ground-truth box.</li>
<li>第二种方法更好检测正样本，在第二种情况下如果找不到正样本，那么使用第一种。</li>
</ul>
</li>
<li><p>如果一个anchor和任何ground-truth boxes的IoU值小于0.3，那么该anchor为<strong>负标签</strong></p>
</li>
<li><p>非正非负样本对training objective没有用。</p>
</li>
<li><p>Loss Function：</p>
<p>​</p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcjw1fcbwo1ut3hj309q02hjrj.jpg" alt=""></p>
<p>$N<em>{cls}=256,N</em>{reg}=256*9=2304,\lambda=10$，这样两个loss就可以权重基本相当了。</p>
</li>
<li><p>Bounding box regression</p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcjw1fcbyiywha8j308k0300sy.jpg" alt=""></p>
<p>这个可以考虑为从anchor box回归到附近的ground truth box。</p>
</li>
<li><p>和R-CNN和Fast R-CNN的bounding box regression方法不同的是：</p>
<ul>
<li>前两种的回归是在从任意大小RoI中提取的特征进行回归的，所以regression weights在<strong>所有尺寸中共享</strong>。</li>
<li>在我们的方法中，用于回归的特征都是同一个3x3的空间特征。考虑到变化的尺寸，有k个不同的bounding boxe回归器去学习，每一个回归器负责去学习一个尺寸一个衡重比的anchor。所以k个回归器是<strong>不共享权值的</strong>。所以<strong>得益于anchor的设计，即使特征规定，我们依旧可以去预测不同尺寸的box。</strong></li>
</ul>
</li>
</ul>
<h4 id="Training-RPNs"><a href="#Training-RPNs" class="headerlink" title="Training RPNs"></a>Training RPNs</h4><ul>
<li><strong>image-centric</strong> sampling strategy</li>
<li>mini-batch: arises from a single image that contains many positive and negative example anchors.</li>
<li>随机在一张图片中采样256个anchors来计算一个mini-batch的loss function。正负anchors = 1:1.</li>
<li>all new layers的<strong>权值初始化</strong>：高斯分布$(\mu = 0, \sigma = 0.01)$，all other layers（比如共享卷积层）用ImageNet来<strong>权值初始化</strong>。用ZF net来进行进行<strong>微调</strong>。</li>
<li><strong>学习率</strong>：0.001(60k)-&gt;0.0001(20k)</li>
<li><strong>动量</strong>：0.9</li>
<li><strong>weight decay</strong>: 0.0005</li>
</ul>
<h3 id="Sharing-Feature-for-RPN-and-Fast-R-CNN"><a href="#Sharing-Feature-for-RPN-and-Fast-R-CNN" class="headerlink" title="Sharing Feature for RPN and Fast R-CNN"></a>Sharing Feature for RPN and Fast R-CNN</h3><ul>
<li><p><strong>sharing convolutional layers between the two networks, rather than learning two separate networks</strong></p>
</li>
<li><p>三种特征共享的方法：</p>
<ul>
<li><p>Alternating training：迭代，先训练PRN，然后用proposal去训练Fast R-CNN。被Fast R-CNN微调的网络然后用来初始化PRN，以此迭代。本论文所有的实现都是使用该方法。</p>
</li>
<li><p>Approximate joint training：</p>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcjw1fccbdrnzhij30bd0b1gml.jpg" alt=""></p>
<p>RPN和Fast R-CNN融合到一个网络中进行训练。在每次SGD迭代过程中：</p>
<ul>
<li>前向传递：RPN产生region proposals，这些proposals被当做固定的、提前计算好的proposal来训练Fast R-CNN检测器。</li>
<li>反向传递：对于共享层来说，来自RPN的loss和Fast R-CNN的loss结合.</li>
<li>但是这种方法不考虑Bounding Boxes，忽略了proposal boxes的坐标也是网络的输出。所以这种方法叫做approximate</li>
</ul>
</li>
<li><p>Non-approximate joint training: 考虑Bounding Boxes。</p>
</li>
</ul>
</li>
<li><p>4-step Alternating Training:</p>
<ul>
<li>Step 1: train the RPN, initialized with an ImageNet-pre-trained model and <strong>ﬁne-tuned</strong> end-to-end for the region proposal task.</li>
<li>Step 2: train a separate detection network by Fast R-CNN using the proposals generated by the step-1 RPN. 同样使用ImageNet-pre-trained model来初始化。<strong>此时两个网络并没有共享卷积层。</strong></li>
<li>Step 3: use the detector network to initialize RPN training but we ﬁx the shared convolutional layers and only ﬁne-tune the layers unique to RPN. <strong>现在两个网络共享卷积层</strong></li>
<li>Step 4: keeping the shared convolutional layers ﬁxed, we ﬁne-tune the unique layers of Fast R-CNN.</li>
</ul>
</li>
</ul>
<h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><ul>
<li>Multi-scale与speed-accuracy之间的trade-off</li>
<li>To reduce redundancy, we adopt <strong>non-maximum suppression (NMS)</strong> on the proposal regions based on their cls scores.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Region Proposal的计算&lt;/strong&gt;是基于Region Proposal算法来假设物体位置的物体检测网络比如：&lt;strong&gt;SPPnet, Fast R-CNN&lt;/strong&gt;运行时间的瓶颈。&lt;/li&gt;
&lt;li&gt;Faster R-CNN引入了&lt;strong&gt;Region Proposal Network（RPN）&lt;/strong&gt;来和检测网络共享整个图片的卷积网络特征，因此使得region proposal几乎是&lt;strong&gt;cost free&lt;/strong&gt;的。&lt;/li&gt;
&lt;li&gt;RPN-&amp;gt;预测物体边界（object bounds）和在每一位置的分数（objectness score）&lt;/li&gt;
&lt;li&gt;通过在一个网络中共享RPN和Fast R-CNN的卷积特征来融合两者——&lt;strong&gt;使用“attention”机制。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;300 proposals pre image.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/blog/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/blog/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/blog/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：Fast R-CNN</title>
    <link href="http://jacobkong.github.io/blog/1679631826/"/>
    <id>http://jacobkong.github.io/blog/1679631826/</id>
    <published>2016-12-07T22:32:24.000Z</published>
    <updated>2017-03-09T04:27:47.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><ul>
<li>mAP：detection quality.</li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>本文提出一种基于快速区域的卷积网络方法（快速R-CNN）用于对象检测。</li>
<li>快速R-CNN采用多项创新技术来提高训练和测试速度，同时提高检测精度。</li>
<li>采用VGG16的网络：VGG: 16 layers of 3x3 convolution interleaved with max pooling + 3 fully-connected layers</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>物体检测相对于图像分类是更复杂的，应为需要物体准确的位置。<ul>
<li>首先，必须处理许多候选对象位置（通常称为“proposal”）。</li>
<li>其次，这些候选者只提供粗略的定位，必须进行精确定位才能实现精确定位。</li>
<li>这些问题的解决方案经常损害 <strong>速度</strong> ， <strong>准确性</strong> 或 <strong>简单性</strong> 。</li>
</ul>
</li>
</ul>
<h3 id="R-CNN-and-SPPnet"><a href="#R-CNN-and-SPPnet" class="headerlink" title="R-CNN and SPPnet"></a>R-CNN and SPPnet</h3><ul>
<li>R-CNN(Region-based Convolution Network)具有几个显著的缺点：<ul>
<li>训练是一个多级管道。</li>
<li>训练在空间和时间上是昂贵的。</li>
<li>物体检测速度很慢。</li>
</ul>
</li>
<li>R-CNN是慢的，因为它对每个对象proposal执行ConvNet正向传递，而不共享计算（sharing computation）。</li>
<li>Spatial pyramid pooling networks（SPPnets），利用sharing computation对R-CNN进行了加速，但是SPPnets也具有明显的缺点，像R-CNN一样，SPPnets也需要：<ul>
<li>训练是一个多阶段流程，</li>
<li>涉及提取特征，</li>
<li>用对数损失精简网络</li>
<li>训练SVM</li>
<li>赋予边界框回归。</li>
<li>特征也需要也写入磁盘。</li>
</ul>
</li>
<li>但与R-CNN <strong>不同</strong> ，在[11]中提出的fine-tuning算法不能更新在空间金字塔池之前的卷积层。 不出所料，这种限制（固定的卷积层）限制了非常深的网络的精度。</li>
</ul>
<h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><ul>
<li>Fast R-CNN优点：</li>
</ul>
<ol>
<li>比R-CNN，SPPnet更高的检测质量（mAP）</li>
<li>训练是单阶段的，使用多任务损失（multi-task loss）</li>
<li>训练可以更新所有网络层</li>
<li>特征缓存不需要磁盘存储</li>
</ol>
<h2 id="Fast-R-CNN-architecture-and-training"><a href="#Fast-R-CNN-architecture-and-training" class="headerlink" title="Fast R-CNN architecture and training"></a>Fast R-CNN architecture and training</h2><ul>
<li><p>整体框架<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfo7na6fij30jk0efdju.jpg" alt=""></p>
</li>
<li><p>快速R-CNN网络将<strong>整个图像</strong>和<strong>一组object proposals</strong>作为输入。</p>
<ul>
<li>网络首先使用几个卷积（conv）和最大池层来处理整个图像，以产生conv feature map。</li>
<li>然后，对于每个对象proposal， <strong>感兴趣区域（RoI）池层</strong> 从特征图中抽取固定长度的特征向量。</li>
<li>每个特征向量被馈送到完全连接（fc）层序列，其最终分支成两个同级输出层：<ul>
<li>一个产生对K个对象类加上全部捕获的“背景”类的softmax概率估计(one that produces softmax probability estimates over K object classes plus a catch-all “background” class)</li>
<li>另一个对每个K对象类输出四个实数，每组4个值编码提炼定义K个类中的一个的的边界框位置。(another layer that outputs four real-valued numbers for each of the K object classes. Each set of 4 values encodes reﬁned bounding-box positions for one of the K classes.)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="The-RoI-pooling-layer"><a href="#The-RoI-pooling-layer" class="headerlink" title="The RoI pooling layer"></a>The RoI pooling layer</h3><ul>
<li>Rol pooling layer的作用主要有两个：<ul>
<li>一个是将image中的RoI定位到feature map中对应patch</li>
<li>另一个是用一个单层的SPP layer将这个feature map patch下采样为大小固定的feature再传入全连接层。</li>
</ul>
</li>
<li>RoI池层使用最大池化将任何有效的RoI区域内的特征转换成具有H×W（例如，7×7）的固定空间范围的小feature map，其中H和W是层<strong>超参数</strong> 它们独立于任何特定的RoI。</li>
<li>在本文中，RoI是conv feature map中的一个矩形窗口。</li>
<li>每个RoI由定义其左上角（r，c）及其高度和宽度（h，w）的四元组（r，c，h，w）定义。</li>
<li>RoI层仅仅是Sppnets中的spatial pyramid pooling layer的特殊形式，其中<strong>只有一个金字塔层</strong>.</li>
</ul>
<h3 id="Initializing-from-pre-trained-networks"><a href="#Initializing-from-pre-trained-networks" class="headerlink" title="Initializing from pre-trained networks"></a>Initializing from pre-trained networks</h3><ul>
<li>用了3个预训练的ImageNet网络（CaffeNet/ VGG_CNN_M_1024 /VGG16）。预训练的网络初始化Fast RCNN要经过三次变形：</li>
</ul>
<ol>
<li>最后一个max pooling层替换为RoI pooling层，设置H’和W’与第一个全连接层兼容。</li>
<li>最后一个全连接层和softmax（原本是1000个类）替换为softmax的对K+1个类别的分类层，和bounding box 回归层。</li>
<li>输入修改为两种数据：一组N个图形，R个RoI，batch size和ROI数、图像分辨率都是可变的。</li>
</ol>
<h3 id="Fine-tuning-for-detection"><a href="#Fine-tuning-for-detection" class="headerlink" title="Fine-tuning for detection"></a>Fine-tuning for detection</h3><ul>
<li><p>利用反向传播算法进行训练所有网络的权重是Fast R-CNN很重要的一个能力。</p>
</li>
<li><p>我们提出了一种更有效的训练方法，利用在训练期间的特征共享（feature sharing during training）。</p>
</li>
<li><p>在Fast R-CNN训练中， <strong>随机梯度下降（SGD）小批量分层采样</strong> ，首先通过采样N个图像，然后通过从每个图像采样 <strong>R/N个</strong> RoIs。</p>
</li>
<li><p><strong>关键的是，来自同一图像的RoI在向前和向后传递中 共享计算 和存储。</strong></p>
</li>
<li><p>此外为了分层采样，Fast R-CNN使用了一个<strong>流水线训练过程</strong>，利用一个fine-tuning阶段来联合优化一个softmax分类器和bounding box回归，而非训练一个softmax分类器，SVMs，和regression在三个独立的阶段。</p>
</li>
<li><p>Multi-task loss：</p>
<ul>
<li>两个sibling输出层：<ul>
<li>第一层：输出离散概率分布（针对每个RoIs），$p=(p_0,…,p_K)$，分别对应$K+1$个类。p是在一个全连接层的$K+1$个输出上的softmax。</li>
<li>第二层：输出bounding-box的回归偏移(bounding-box regression offsets)，针对K object classes中的每一个类，计算$t^k=(t^k_x,t^k_y,t^k_w,t^k_h)$，<strong>具体见R-CNN得补充材料，里面有很详细的介绍bounding box regression</strong>。</li>
</ul>
</li>
<li>每一个训练RoIs被标注一个ground truth类$u$，和一个ground truth bounding box 回归目标$v$。</li>
</ul>
<ul>
<li>两个loss，以下分别介绍：<ul>
<li>对于分类loss，是一个N+1路的softmax输出，其中的N是类别个数，1是背景。</li>
<li>对于回归loss，是一个4xN路输出的regressor，也就是说对于每个类别都会训练一个单独的regressor的意思，比较有意思的是，这里regressor的loss不是L2的，而是一个平滑的L1，形式如下：</li>
</ul>
</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfo7oia0ij30bg05n74f.jpg" alt=""></p>
</li>
<li><p>我们利用一个multi-task loss L 在每个被标注的RoI上来联合训练分类器和bounding box regression</p>
</li>
<li><p>Mini-batch sampling：在微调时，每个SGD的mini-batch是随机找两个图片，R为128，因此每个图上取样64个RoI。从object proposal中选25%的RoI，就是和ground-truth交叠至少为0.5的。剩下的作为背景。</p>
</li>
<li><p>Back-propagation through RoI pooling layers：</p>
<ul>
<li><p>RoI pooling层计算损失函数对每个输入变量x的偏导数，如下：</p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfo7owdcpj306q01mwee.jpg" alt=""></p>
<p>y是pooling后的输出单元，x是pooling前的输入单元，如果y由x pooling而来，则将损失L对y的偏导计入累加值，最后累加完R个RoI中的所有输出单元。下面是我理解的x、y、r的关系：</p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfoo7cuv7j30qf0ardgq.jpg" alt="20151208163114338"></p>
</li>
</ul>
</li>
</ul>
<h3 id="Scale-invariance"><a href="#Scale-invariance" class="headerlink" title="Scale invariance"></a>Scale invariance</h3><ul>
<li>这里讨论object的scale问题，就是网络对于object的scale应该是要不敏感的。这里还是引用了SPP的方法，有两种:<ul>
<li>brute force （single scale），也就是简单认为object不需要预先resize到类似的scale再传入网络，直接将image定死为某种scale，直接输入网络来训练就好了，然后期望网络自己能够学习到scale-invariance的表达。</li>
<li>image pyramids （multi scale），也就是要生成一个金字塔，然后对于object，在金字塔上找到一个大小比较接近227x227的投影版本，然后用这个版本去训练网络。</li>
</ul>
</li>
<li>可以看出，2应该比1更加好，作者也在5.2讨论了，2的表现确实比1好，但是好的不算太多，大概是1个mAP左右，但是时间要慢不少，所以作者实际采用的是第一个策略，也就是single scale。</li>
<li>这里，FRCN测试之所以比SPP快，很大原因是因为这里，因为SPP用了2，而FRCN用了1。</li>
</ul>
<h2 id="Fast-R-CNN-detection"><a href="#Fast-R-CNN-detection" class="headerlink" title="Fast R-CNN detection"></a>Fast R-CNN detection</h2><ul>
<li>大型全连接层很容易的可以通过将他们与 <strong>truncated SVD(奇异值分解)</strong> 压缩来加速计算。</li>
</ul>
<h2 id="Main-results"><a href="#Main-results" class="headerlink" title="Main results"></a>Main results</h2><ul>
<li>All Fast R-CNN results in this paper using VGG16 ﬁne-tune layers conv3 1 and up; all experments with models S and M ﬁne-tune layers conv2 and up.</li>
</ul>
<h2 id="Design-evaluation"><a href="#Design-evaluation" class="headerlink" title="Design evaluation"></a>Design evaluation</h2><h3 id="Do-we-need-more-training-data"><a href="#Do-we-need-more-training-data" class="headerlink" title="Do we need more training data?"></a>Do we need more training data?</h3><ul>
<li>在训练期间，作者做过的唯一一个数据增量的方式是水平翻转。 作者也试过将VOC12的数据也作为拓展数据加入到finetune的数据中，结果VOC07的mAP从66.9到了70.0，说明对于网络来说， <strong>数据越多就是越好的。</strong></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;知识点&quot;&gt;&lt;a href=&quot;#知识点&quot; class=&quot;headerlink&quot; title=&quot;知识点&quot;&gt;&lt;/a&gt;知识点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;mAP：detection quality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;本文提出一种基于快速区域的卷积网络方法（快速R-CNN）用于对象检测。&lt;/li&gt;
&lt;li&gt;快速R-CNN采用多项创新技术来提高训练和测试速度，同时提高检测精度。&lt;/li&gt;
&lt;li&gt;采用VGG16的网络：VGG: 16 layers of 3x3 convolution interleaved with max pooling + 3 fully-connected layers&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/blog/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/blog/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/blog/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
