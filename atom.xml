<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>JacobKong&#39;s Blog</title>
  <subtitle>路漫漫其修远兮......</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://jacobkong.github.io/"/>
  <updated>2018-05-28T07:35:27.000Z</updated>
  <id>http://jacobkong.github.io/</id>
  
  <author>
    <name>Jacob Kong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://jacobkong.github.io/posts/0/"/>
    <id>http://jacobkong.github.io/posts/0/</id>
    <published>2018-05-28T16:00:00.000Z</published>
    <updated>2018-05-28T07:35:27.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">title: 行为检测论文笔记：Rethinking the Faster R-CNN Architecture for Temporal Action Localization</div><div class="line">categories: 论文笔记</div><div class="line">tags:</div><div class="line">  - Action Detection</div><div class="line">  - 行为检测</div><div class="line">abbrlink: </div><div class="line">date: 2018-05-29 15:51:24</div></pre></td></tr></table></figure>
<p>这是今年CVPR 2018中接受为数不多的动作时间轴定位论文中的一篇，解决了目前现存方法中的3个问题：（1）Multi-scale的动作片段；（2）Temproal context的利用；（3）Multi-stream 特征融合。方法在THUMOS’ 14数据集上的提议和检测任务上达到目前最好的效果（mAP@tIoU=0.5达到42.8%），在ActivityNet数据及上取得了具有挑战性的效果。</p>
<a id="more"></a>
<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li><p>时间轴行为检测其实和目标检测相类似，因此目前许多行为检测的方法都受启发于目标检测的一些先进方法，比如R-CNN系列，先从整个视频中生成segments proposal，然后用分类器去对这些proposal进行分类。</p>
</li>
<li><p>目前有一些方法将Faster R-CNN迁移到时间轴行为检测中，然而直接迁移过来引入一些挑战，如下：</p>
<ul>
<li><p>如何处理行动持续时间的巨大变化？</p>
<blockquote>
<p>因为行为会有许多时间长短不一的持续时间，从几秒到几分钟的行为片段都有，而Faster R-CNN利用anchor提proposal会在特征的temporal scope和anchor的span之间产生misalignment现象。<strong>我们提出了一个multi-tower网络和利用扩张时间卷积（dilated temporal convolutions）来解决alignment的问题。</strong></p>
</blockquote>
</li>
<li><p>如何利用时间上下文信息？</p>
<blockquote>
<p>动作实例之前和之后的时刻包含关于定位和分类的关键信息（可以说比对象的空间上下文更重要）。Faster R-CNN没有利用时间上下文信息。<strong>我们建议通过扩展提案生成和动作分类中的感受野来明确地编码时间上下文。</strong></p>
</blockquote>
</li>
<li><p>如何最好的去融合multi-stream的特征？</p>
<blockquote>
<p>对于Faster R-CNN探索这种RGB和Flow特征融合方面的工作有限。 我们提出了一个后期融合方案，并且经验性地证明了它在一般的早期融合方案上的优势。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><p>解决Faster R-CNN直接引入到时间轴行为检测中的上述3个挑战,并以此来提升Faster R-CNN在行为检测中的性能.</p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>论文框架如下：</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1frr390sqxsj30ua0a4adx.jpg" alt="mage-20180528153210"></p>
<p>本文提出了TAL-Net，有三个创新的结构改变：</p>
<ul>
<li><p>Receptive Field Alignment</p>
<ul>
<li><p>传统的anchor机制有一个缺点：每个时间点的锚点分类都有相同的单一的感受野。</p>
</li>
<li><p>为了解决这个问题，我们建议将每个锚点的感受野与它的时间跨度对齐。 这是通过两个关键因素实现的：multi-tower网络和扩张时间卷积（dilated temporal convolutions）。</p>
</li>
<li><p>给定一个一维feature map，我们的Segment Proposal Network 由K个temproal ConvNets 组成，每个K网络负责对特定比例的锚段进行分类.最重要的是，每个时间ConvNet都经过精心设计，使得其接受的字段大小与相关的锚点尺度一致。 在每个ConvNet结束时，我们分别应用两个核心大小为1的平行卷积层进行锚定分类和边界回归。</p>
</li>
<li><p>另一问题：如何设计具有可控感受野s的时间卷积？</p>
<ul>
<li><p>方法一：如果s=2L+1，则叠加L层卷积层得到相应的感受野。缺点是层数L随着s线性增加，很容易增加参数数量使网络过拟合。</p>
</li>
<li><p>方法二：在每一层卷积层后添加一个kernel size为2的pooling层，则感受野$s=2^{(L+1)}-1$，此时层数随着s成log变化，但是添加pooling层会减小输出feature map的分辨率，会影响定位准确率。</p>
</li>
<li><p>方法三：使用扩充时间卷积，这种卷积可以在扩充感受野的同时不损失分辨率。在我们的Segment Proposal Network中，每一个temporal ConvNet都只由2个dilated convolutional layers组成。为了获得一个目标感受野s，则第一层的dilated convolutional layers的dilation rate $r_1=s/6, r_2=(s/6)\times2$. </p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1frpy9obbp6j30g70bsjss.jpg" alt=""></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Context Feature Extraction</p>
<ul>
<li><p>时间轴上下文信息十分重要</p>
</li>
<li><p>为了确保上下文特征用于锚定分类和边界回归，感受野必须覆盖时间轴上下文信息区域，可以通过将dilation rate加倍，即$r_1=s/6\times2, r_2=(s/6)\times2\times2$，如下：</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1frpysrksxjj30fq0b53zx.jpg" alt=""></p>
</li>
<li><p>在动作分类阶段，我们要利用SoI pooling来为每个proposal提取一个固定尺寸的feature map</p>
</li>
</ul>
</li>
<li><p>Late Feature Fusion</p>
<ul>
<li>目前许多方法都在使用RGB和光流特征</li>
<li>本文为双流特征提出了一个后融合的机制</li>
<li>们首先使用两个不同的网络分别从RGB帧和叠加的光流中提取两个一维特征映射。 我们通过一个不同的Segment Proposal Network来处理每个feature ma，该网络并行地生成锚定分类和边界回归的逻辑。 我们使用来自两个网络的logits的元素平均值作为最终的逻辑来生成提议。 对于每个提案，我们在两个特征映射上并行执行SoI池，并在每个输出上应用不同的DNN分类器。</li>
</ul>
</li>
</ul>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><ul>
<li><p>基于TensorFlow目标检测API</p>
</li>
<li><p>9个anchor，scales为{1, 2, 3, 4, 5, 6, 8, 11, 16}</p>
</li>
<li><p>NMS阈值为0.7去筛选proposal，保留前300个proposal用于分类</p>
</li>
<li><p>THUMOS’ 14检测结果</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1frq0h25kllj30g20goadr.jpg" alt=""></p>
</li>
<li><p>ActivityNet v1.3在验证集的检测结果</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1frq0m3der6j30ft07d0u2.jpg" alt=""></p>
</li>
</ul>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>相比于R-C3D，本文的方法解决了Multi-scale的问题，利用了上下文信息以及额外的光流信息，解决了目前许多方法中存在的大大小小的缺陷，组合成了一个较为完整的框架，因此在THUMOS’ 14数据集上检测效果达到最好，在ActivityNet数据集上也取得了很有竞争力的结果，但是还是不如SSN的结果。文中分析：THUMOS’ 14是一个更好的用来评估行为定位的数据集，因为其每段视频中包含有更多的行为实例，并且每段视频包含大量的背景活动。</p>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>我认为除了第一点创新：利用dilated temporal convlutional组成感受野可控的multi-tower网络来解决multi-scale问题比较有创新外，另外两点创新其实不算特别有新意。</p>
]]></content>
    
    <summary type="html">
    
      &lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;title: 行为检测论文笔记：Rethinking the Faster R-CNN Architecture for Temporal Action Localization&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;categories: 论文笔记&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;tags:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  - Action Detection&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  - 行为检测&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;abbrlink: &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;date: 2018-05-29 15:51:24&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;这是今年CVPR 2018中接受为数不多的动作时间轴定位论文中的一篇，解决了目前现存方法中的3个问题：（1）Multi-scale的动作片段；（2）Temproal context的利用；（3）Multi-stream 特征融合。方法在THUMOS’ 14数据集上的提议和检测任务上达到目前最好的效果（mAP@tIoU=0.5达到42.8%），在ActivityNet数据及上取得了具有挑战性的效果。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>论文调研：ICCV 2017论文调研</title>
    <link href="http://jacobkong.github.io/posts/679115822/"/>
    <id>http://jacobkong.github.io/posts/679115822/</id>
    <published>2017-11-25T07:51:24.000Z</published>
    <updated>2018-05-28T07:34:35.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Visual-object-tracking"><a href="#Visual-object-tracking" class="headerlink" title="Visual object tracking"></a>Visual object tracking</h2><ul>
<li><h4 id="Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades"><a href="#Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades" class="headerlink" title="Learning Policies for Adaptive Tracking with Deep Feature Cascades"></a>Learning Policies for Adaptive Tracking with Deep Feature Cascades</h4><ul>
<li>Our fundamental insight is to take an adaptive approach, where easy frames are processed with cheap features (such as pixel values), while challenging frames are processed with invariant but expensive deep features.</li>
<li>Formulate the adaptive tracking problem as a decision-making process.</li>
<li>Learn an agent to decide whether to locate objects with high conﬁdence on an early layer, or continue processing subsequent layers of a network.</li>
</ul>
</li>
</ul>
<ul>
<li>Signiﬁcantly reduces the feedforward cost.</li>
<li>Train the agent ofﬂine in a reinforcement learning fashion.</li>
<li>Obviously, the major computational burden comes from the forward pass through the entire network, and can be larger with deeper architectures.</li>
<li>However, when the object is visually distinct or barely moves, early layers are in most scenarios sufﬁcient for precise localization - offering the potential for substantial computational savings.</li>
<li>The agent learns to ﬁnd the target at each layer, and decides if it is conﬁdent enough to output and stop there.</li>
</ul>
<a id="more"></a>
<ul>
<li><h4 id="Tracking-The-Untrackable-Learning-to-Track-Multiple-Cues-with-Long-Term-Dependencies"><a href="#Tracking-The-Untrackable-Learning-to-Track-Multiple-Cues-with-Long-Term-Dependencies" class="headerlink" title="Tracking The Untrackable: Learning to Track Multiple Cues with Long-Term Dependencies"></a>Tracking The Untrackable: Learning to Track Multiple Cues with <strong>Long-Term Dependencies</strong></h4><ul>
<li>Combine cues in a coherent end-to-end fashion over a <strong>long period of time.</strong></li>
<li>Present a structure of Recurrent Neural Networks (RNN) that jointly reasons on multiple cues over a temporal window.</li>
<li>We are able to <strong>correct many data association errors</strong> and recover observations from an occluded state.</li>
</ul>
</li>
<li><h4 id="Tracking-as-Online-Decision-Making-Learning-a-Policy-from-Streaming-Videos-with-Reinforcement-Learning"><a href="#Tracking-as-Online-Decision-Making-Learning-a-Policy-from-Streaming-Videos-with-Reinforcement-Learning" class="headerlink" title="Tracking as Online Decision-Making: Learning a Policy from Streaming Videos with Reinforcement Learning"></a>Tracking as Online Decision-Making: Learning a Policy from Streaming Videos with Reinforcement Learning</h4><ul>
<li>A tracking agent must follow an object despite ambiguous image frames and a limited computational budget.</li>
<li>The agent must decide<ul>
<li>where to look in the upcoming frames</li>
<li>when to reinitialize because it believes the target has been lost</li>
<li>when to update its appearance model for the tracked object</li>
</ul>
</li>
<li>Formulating tracking as a partially observable decision-making process (POMDP).</li>
<li><strong>Sparse rewards</strong> allow us to quickly train on massive datasets.</li>
<li>Challenges:<ul>
<li>First, the limited quantity of annotated video data impedes both training and evaluation.</li>
<li>Second, as vision (re)integrates with robotics, video processing must be done in an online, streaming fashion.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Face-detection"><a href="#Face-detection" class="headerlink" title="Face detection"></a>Face detection</h3><ul>
<li><h4 id="S3FD-Single-Shot-Scale-invariant-Face-Detector"><a href="#S3FD-Single-Shot-Scale-invariant-Face-Detector" class="headerlink" title="S3FD - Single Shot Scale-invariant Face Detector."></a>S3FD - Single Shot Scale-invariant Face Detector.</h4><ul>
<li>Use a single deep neural network, especially for small faces.</li>
<li>Contribution<ul>
<li>提出一个尺度公平的人脸检测框架来处理不同尺度的人脸。我们在各种各样的图层上拼贴anchor，以确保所有人脸的比例尺都具有足够的特征用于检测。基于有效接收域<strong>（effective receptive ﬁeld）</strong>和等比例区间原则<strong>（equal proportion interval principle）</strong>设计anchor</li>
<li>用尺度补偿anchor匹配策略<strong>（ a scale compensation anchor matching strategy）</strong>提高小脸的召回率; </li>
<li>通过最大化背景标签<strong>（ max-out background label）</strong>减少小脸的误报率。</li>
<li>effective receptive ﬁeld: Understanding the effective receptive ﬁeld in deep convolutional neural networks.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Salient-Object-Detection"><a href="#Salient-Object-Detection" class="headerlink" title="Salient Object Detection"></a>Salient Object Detection</h3><ul>
<li><h4 id="Amulet-Aggregating-Multi-level-Convolutional-Features-for-Salient-Object-Detection"><a href="#Amulet-Aggregating-Multi-level-Convolutional-Features-for-Salient-Object-Detection" class="headerlink" title="Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection"></a>Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection</h4><ul>
<li>How to better aggregate multi-level convolutional feature maps for salient object detection is <strong>underexplored</strong>.</li>
<li>Our framework:<ul>
<li>First integrates multi-level feature maps into multiple resolutions, which simultaneously incorporate coarse semantics and ﬁne details. </li>
<li>Then it adaptively learns to combine these feature maps at each resolution and <strong>predict saliency maps with the combined features.</strong> </li>
<li>Finally, the predicted results are efﬁciently fused to generate the <strong>ﬁnal saliency map</strong>.</li>
</ul>
</li>
<li>In addition, edge-aware maps and high-level predictions are embedded into the framework.</li>
</ul>
</li>
<li><h4 id="Learning-Uncertain-Convolutional-Features-for-Accurate-Saliency-Detection"><a href="#Learning-Uncertain-Convolutional-Features-for-Accurate-Saliency-Detection" class="headerlink" title="Learning Uncertain Convolutional Features for Accurate Saliency Detection"></a><a href="http://openaccess.thecvf.com/content_iccv_2017/html/Zhang_Learning_Uncertain_Convolutional_ICCV_2017_paper.html" target="_blank" rel="external">Learning Uncertain Convolutional Features for Accurate Saliency Detection</a></h4><ul>
<li>The key contribution of this work is to learn deep uncertain convolutional features (UCF), which encourage the robustness and accuracy of saliency detection.</li>
<li>我们提出了一种有效的混合上采样方法来减少我们的解码器网络中去卷积算子的棋盘伪影。</li>
<li>We ﬁnd that the actual cause of these artifacts is the upsampling mechanism, which generally utilizes the deconvolution operation.</li>
</ul>
</li>
</ul>
<h3 id="Action-Related"><a href="#Action-Related" class="headerlink" title="Action Related"></a>Action Related</h3><ul>
<li><h4 id="Encouraging-LSTMs-to-Anticipate-Actions-Very-Early"><a href="#Encouraging-LSTMs-to-Anticipate-Actions-Very-Early" class="headerlink" title="Encouraging LSTMs to Anticipate Actions Very Early"></a>Encouraging LSTMs to Anticipate Actions Very Early</h4><ul>
<li>Action anticipation - identify the action from only partially available videos.</li>
<li>To this end, we develop a <strong>multi-stage LSTM architecture</strong> that leverages <strong>context-aware</strong> and <strong>action-aware</strong> features, and introduce <strong>a novel loss function</strong> that encourages the model to predict the correct class as early as possible.</li>
<li>Intuitive: our loss models the intuition that some actions, such as running and high jump, are highly ambiguous after seeing only the ﬁrst few frames, and <strong>false positives</strong> should therefore not be penalized too strongly in the early stages.</li>
<li>We would like to predict a high probability for the correct class <strong>as early as possible</strong>, and thus penalize <strong>false negatives</strong> from the beginning of the sequence.</li>
<li>Contribute a novel multi-stage Long Short Term Memory (LSTM) architecture for action anticipation. This model effectively extracts and jointly exploits context- and action-aware features.</li>
<li>Existing method drawbacks:<ul>
<li>This is in contrast to existing methods that typically extract either <strong>global representations</strong> for the entire image or video sequence thus <strong>not focusing on the action itself, or localize the feature extraction process to the action itself</strong> via dense trajectories  optical ﬂow or actionness, thus <strong>failing to exploit contextual information.</strong></li>
<li>利用光流不允许这些方法在localization过程中明确地利用外观。</li>
<li>Computing optical ﬂow is typically expensive.</li>
</ul>
</li>
<li>In the future, we intend to study new ways to incorporate additional sources of information, such as <strong>dense trajectories</strong> and <strong>human skeletons</strong> in our framework.</li>
</ul>
</li>
<li><h4 id="Unsupervised-Action-Discovery-and-Localization-in-Videos"><a href="#Unsupervised-Action-Discovery-and-Localization-in-Videos" class="headerlink" title="Unsupervised Action Discovery and Localization in Videos"></a>Unsupervised Action Discovery and Localization in Videos</h4><ul>
<li>创新：无监督的action localization。</li>
<li>First to address the problem of unsupervised action localization in videos.</li>
<li><p>We propose a novel approach that:</p>
<ul>
<li>Discovers action class labels</li>
<li>Spatio-temporally localizes actions in videos.</li>
</ul>
</li>
<li><p>Method:</p>
<ul>
<li>It begins by computing local video features to apply <strong>spectral clustering</strong> on a set of unlabeled training videos.<ul>
<li>For each cluster of videos, an <strong>undirected graph</strong> is constructed to extract a dominant set, which are known for <strong>high internal homogeneity</strong> and <strong>in-homogeneity</strong> between vertices outside it.</li>
</ul>
</li>
<li>Next, a <strong>discriminative clustering approach</strong> is applied, by training a classiﬁer for each cluster, to iteratively select videos from the non-dominant set and obtain complete video action classes.<ul>
<li>Once classes are discovered, training videos within each cluster are selected to perform <strong>automatic spatio-temporal annotations</strong>, by ﬁrst over-segmenting videos in each discovered class into <strong>supervoxels</strong>（超体素） and constructing a <strong>directed graph</strong> （有向图）to apply a variant of knapsack problem with temporal constraints. （并构建有向图以应用具有时间约束的背包问题的变体。）</li>
</ul>
</li>
<li>背包优化联合收集超体素的一个子集，通过强制注释的动作进行时空连接，其体积是一个actor的大小。These annotations are used to train SVM action classiﬁers.</li>
</ul>
</li>
<li>在测试过程中，操作使用类似的背包方法来进行localize，在这种方法中将超体素分组在一起，并且使用来自发现的动作类的视频学习的SVM被用于识别这些动作。</li>
<li>However, supervised algorithms have some disadvantages compared to unsupervised approaches, due to the difﬁculty of video annotation.</li>
<li>Contributions：<ul>
<li>Automatic discovery of action class labels using <strong>a new discriminative clustering approach</strong> with dominant sets (Sec. 3).</li>
<li>We propose a novel <strong>Knapsack approach</strong> with <strong>graph-based temporal constraints</strong> to <strong>annotate actions</strong> in training videos</li>
<li>The annotations within each cluster of videos are jointly selected by <strong>Binary Integer Quadratic Programming (BIQP)</strong> optimization to train action classiﬁers.</li>
<li><strong>Structural SVM</strong> is used to learn the pairwise relations of supervoxels within foreground action and foreground-background, which enforces that the supervoxels belonging to the action to be simultaneously selected.</li>
<li>Lastly, we address a new problem of <strong>Unsupervised Action Localization</strong> (Sec. 5.2).</li>
</ul>
</li>
</ul>
</li>
<li><h4 id="Dense-Captioning-Events-in-Videos"><a href="#Dense-Captioning-Events-in-Videos" class="headerlink" title="Dense-Captioning Events in Videos"></a>Dense-Captioning Events in Videos</h4><ul>
<li>We introduce the task of dense-captioning events, which involves both detecting and describing events in a video.</li>
<li>Our model introduces a variant of an existing proposal module that is designed to capture both short as well as long events that span minutes.</li>
<li>To <strong>capture the dependencies between the events in a video</strong>, our model introduces a <strong>new captioning module</strong> that uses <strong>contextual information</strong> from past and future events to jointly describe all events.</li>
<li>While the success of these methods is encouraging, they all share one key limitation: <strong>detail.</strong></li>
<li>We introduce the task of <strong>dense-captioning events</strong>, which requires a model to generate a set of descriptions for multiple events occurring in the video and localize them in time.</li>
<li>However, we observe that densecaptioning events comes with its own set of challenges distinct from the image case.<ul>
<li>One observation is that events in videos can range across multiple time scales and can even overlap.<ul>
<li>Past captioning works have circumvented this problem by encoding the entire video sequence by <strong>mean-pooling</strong> [50] or by using a <strong>recurrent neural network (RNN)</strong> [49].</li>
<li>To overcome this limitation, we extend recent work on generating action proposals [10] to <strong>multi-scale detection of events.</strong></li>
</ul>
</li>
<li>Another key observation is that the events in a given video <strong>are usually related to one another.</strong> <ul>
<li>We introduce a <strong>captioning module</strong> that utilizes the context from all the events from our proposal module to generate each sentence.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><h4 id="Learning-long-term-dependencies-for-action-recognition-with-a-biologically-inspired-deep-network"><a href="#Learning-long-term-dependencies-for-action-recognition-with-a-biologically-inspired-deep-network" class="headerlink" title="Learning long-term dependencies for action recognition with a biologically-inspired deep network"></a>Learning long-term dependencies for action recognition with a biologically-inspired deep network</h4><ul>
<li>How to efﬁciently learn long-term dependencies from sequences still remains a pretty challenging task.</li>
<li>As one of the key models for sequence learning, <strong>recurrent neural network (RNN)</strong> and its variants such as <strong>long short term memory (LSTM)</strong> and <strong>gated recurrent unit (GRU)</strong> are still not powerful enough in practice.<ul>
<li>One possible reason is that they have only feedforward connections, which is different from the biological neural system that is typically composed of <strong>both feedforward and feedback connections.</strong>(既有前传，也有反馈)</li>
</ul>
</li>
<li>Propose <strong>shuttleNet technologically.</strong></li>
<li>The shuttleNet <strong>consists of several processors</strong>, each of which is a GRU while <strong>associated with multiple groups of cells and states.</strong></li>
<li><strong>Attention mechanism</strong> is then employed to select the best information ﬂow pathway.</li>
</ul>
</li>
<li><h4 id="Adaptive-RNN-Tree-for-Large-Scale-Human-Action-Recognition"><a href="#Adaptive-RNN-Tree-for-Large-Scale-Human-Action-Recognition" class="headerlink" title="Adaptive RNN Tree for Large-Scale Human Action Recognition"></a>Adaptive RNN Tree for Large-Scale Human Action Recognition</h4><ul>
<li>We present the RNN Tree (RNN-T), an adaptive learning framework for <strong>skeleton based human action recognition.</strong></li>
<li>Our method categorizes action classes and <strong>uses multiple Recurrent Neural Networks (RNNs)</strong> in a <strong>treelike hierarchy.</strong></li>
<li>在骨架表示中的行为是通过<strong>分层推理</strong>过程来识别的，在这个过程中，单独的RNN将细化的行为类别与增加的置信度</li>
<li>RNN-T effectively addresses two main challenges of large-scale action recognition:<ul>
<li>able to distinguish ﬁne-grained action classes that are intractable using a single network</li>
<li>adaptive to new action classes by augmenting an existing model.</li>
</ul>
</li>
</ul>
</li>
<li><h4 id="Ensemble-Deep-Learning-for-Skeleton-based-Action-Recognition-using-Temporal-Sliding-LSTM-networks"><a href="#Ensemble-Deep-Learning-for-Skeleton-based-Action-Recognition-using-Temporal-Sliding-LSTM-networks" class="headerlink" title="Ensemble Deep Learning for Skeleton-based Action Recognition using Temporal Sliding LSTM networks"></a>Ensemble Deep Learning for Skeleton-based Action Recognition using Temporal Sliding LSTM networks</h4><ul>
<li>Traditional methods generally use relative coordinate systems dependent on some joints, and model <strong>only the long-term dependency</strong>, while excluding <strong>short-term and medium term dependencies.</strong></li>
<li>We transform the skeletons into <strong>another coordinate system</strong> to obtain the robustness to scale, rotation and translation and then extract salient motion features from them.</li>
<li>We propose novel ensemble <strong>Temporal Sliding LSTM (TS-LSTM) networks</strong> for skeleton-based action recognition. The proposed network is composed of multiple parts containing <strong>short-term, medium-term and long-term TS-LSTM networks</strong>.</li>
<li>With a rapid development of 3D data acquisition over the past few decades, lots of researches on <strong>human activity recognition from 3D data</strong> can have been actively performed.</li>
<li>For the modeling of human actions, recent researches show tha<strong>t Long Short-Term Memory (LSTM) networks</strong> are <strong>superior to</strong> temporal pyramids and hidden markov models.</li>
<li>Overall Method：<ul>
<li>Firstly, we transform the coordinates of input skeleton sequences so that the data can be <strong>robust to scale, rotation and translation.</strong> </li>
<li>Secondly, instead of using the simple joint positions, we employ the <strong>motion features</strong> in terms of temporal differences, which help our networks to be focused on the actual skeleton movements. </li>
<li>Thirdly, the motion features are processed with <strong>multi-term LSTMs containing short-term, medium-term and long-term LSTMs</strong>, which allow robustness to variable temporal dynamics. </li>
<li>Finally, the multi-term LSTMs <strong>capture a variety of action dynamics through ensemble.</strong></li>
</ul>
</li>
</ul>
</li>
<li><h4 id="What-Actions-are-Needed-for-Understanding-Human-Actions-in-Videos"><a href="#What-Actions-are-Needed-for-Understanding-Human-Actions-in-Videos" class="headerlink" title="What Actions are Needed for Understanding Human Actions in Videos?"></a>What Actions are Needed for Understanding Human Actions in Videos?</h4><ul>
<li>We analyze the current state of human activity understanding in videos.</li>
<li>The goal of this paper is to examine datasets, evaluation metrics, algorithms, and potential future directions.</li>
<li>The results demonstrate that while there is inherent ambiguity in the temporal extent of activities, current datasets still permit effective benchmarking.</li>
<li>我们发现，当与时间推理相结合时，对物体和姿态的细粒度理解很可能在算法精度上产生实质性的改善。</li>
<li>Some questions:<ul>
<li>What is an activity and how should we represent it? </li>
<li>Do activities have well-deﬁned spatial and temporal extent? </li>
<li>What role do goals and intentions play in deﬁning and understanding activities?</li>
<li>What does the data show about the right categories for recognition in case of activities? </li>
<li>Do existing approaches scale with increasing complexity of activities categories, video data, or temporal relationships between activities? </li>
<li>Are the hypothesized new avenues of studying context, objects, or intentions worthwhile: Do these really help in understanding videos?</li>
</ul>
</li>
<li>This paper provides an in-depth analysis of the <strong>new generation of video datasets</strong>, <strong>human annotators</strong>, <strong>activity categories</strong>, <strong>recognition approaches</strong>, and above all possible new cues for video understanding.</li>
<li>We found that people considered verbs to be relatively more ambiguous.</li>
<li>This suggests that despite boundary ambiguity, current datasets allow us to understand, learn from, and evaluate the temporal extents of activities.</li>
<li>That is, a perfect classiﬁer would automatically do 5 times better than current state-of-the-art [30] on activity localization.</li>
<li>This suggests that focusing our attention on gaining more insight into activity classiﬁcation would naturally yield signiﬁcant improvements in localization accuracy as well.</li>
<li>Having concluded that:<ul>
<li>(1) we should be reasoning about activities as (verb,object) pairs rather than just verb,</li>
<li>(2) temporal boundaries of activities are ambiguous but nevertheless meaningful, and</li>
<li>(3) classiﬁcation of short videos is a reasonable proxy for temporal localization</li>
</ul>
</li>
<li>This suggests that moving forward ﬁne-grained discrimination between activities with similar objects and verbs is needed.</li>
</ul>
</li>
<li><h4 id="Lattice-Long-Short-Term-Memory-for-Human-Action-Recognition"><a href="#Lattice-Long-Short-Term-Memory-for-Human-Action-Recognition" class="headerlink" title="Lattice Long Short-Term Memory for Human Action Recognition"></a>Lattice Long Short-Term Memory for Human Action Recognition</h4><ul>
<li>However, naively applying RNNs to video sequences in a convolutional manner implicitly assumes that motions in videos are <strong>stationary</strong> across different spatial locations. <strong>This assumption is valid for short-term motions but invalid when the duration of the motion is long.</strong></li>
<li>In this work, we propose Lattice-LSTM (L^2STM), which extends LSTM by learning <strong>independent hidden state transitions</strong> of memory cells for individual spatial locations.</li>
<li>Additionally, we introduce a novel multi-modal training procedure for training our network.</li>
<li>An accurate action recognition should:<ul>
<li>(1) have a high capacity for learning and capturing as many motion dynamics as possible</li>
<li>(2) when an action appears in sequential images, the neurons should properly decide what kind of spatio-temporal dynamics should be encoded into the memory for distinguishing actions.</li>
</ul>
</li>
</ul>
</li>
<li><h4 id="Common-Action-Discovery-and-Localization-in-Unconstrained-Videos"><a href="#Common-Action-Discovery-and-Localization-in-Unconstrained-Videos" class="headerlink" title="Common Action Discovery and Localization in Unconstrained Videos"></a>Common Action Discovery and Localization in Unconstrained Videos</h4><ul>
<li>In this work, we tackle the problem of <strong>common action discovery</strong> and <strong>localization</strong> in unconstrained videos, where we do not assume to know the types, numbers or locations of the common actions in the videos.</li>
<li>To perform automatic discovery and localization in such challenging scenarios, we ﬁrst generate action proposals using human prior.</li>
<li>By building an afﬁnity graph among all action proposals, we formulate the common action discovery as <strong>a subgraph density maximization problem</strong> to select the proposals containing common actions.</li>
<li>为了避免在指数级大的解空间中枚举，我们提出了一个有效的多项式时间优化算法。</li>
<li>It solves the problem up to a user speciﬁed error bound with respect to the global optimal solution.</li>
<li>Action discovery的困难：<ul>
<li>首先，由于我们事先不知道在给定的数据集中常见的动作类型或位置，我们必须同时进行发现和定位。 给定一组未标记的视频，我们需要自动识别一组捕获常见操作的时空边界框。</li>
<li>其次，类似的行为也可能由于视点变化，尺度变化或相机运动而出现不同。 自动关联这些常见操作并不是一项简单的任务。</li>
<li>最后，除了常见的动作之外，视频还可能包含动态背景或不常见的动作，因此将这种“noisy motions”与常见动作区分开来是至关重要的。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Pedestrian-Related"><a href="#Pedestrian-Related" class="headerlink" title="Pedestrian Related"></a>Pedestrian Related</h3><ul>
<li><h4 id="HydraPlus-Net-Attentive-Deep-Features-for-Pedestrian-Analysis"><a href="#HydraPlus-Net-Attentive-Deep-Features-for-Pedestrian-Analysis" class="headerlink" title="HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis"></a>HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis</h4><ul>
<li>Learning of comprehensive features of pedestrians for ﬁne-grained tasks remains an open problem.</li>
<li>HydraPlus-Net: multi-directionally feeds the multi-level attention maps to different feature layers.</li>
<li>Advantages: <ul>
<li>(1) the model is capable of capturing <strong>multiple attentions</strong> from low-level to semantic-level</li>
<li>(2) it explores the <strong>multi-scale selectiveness of attentive features</strong> to enrich the ﬁnal feature representations for a pedestrian image.</li>
</ul>
</li>
<li>We demonstrate the effectiveness and generality of the proposed HP-net for pedestrian analysis on two tasks, i.e. <strong>pedestrian attribute recognition</strong> and <strong>person reidentiﬁcation.</strong></li>
<li>However, the learning of feature representation for pedestrian images, as the backbone for all those applications, still confronts critical challenges and needs profound studies.</li>
<li>However, existing arts merely extract global features [13, 24, 30] and are hardly effective to location-aware semantic pattern extraction.</li>
<li><strong>Multidirectional attention (MDA) modules</strong></li>
<li>Reliable 3D <strong>skeleton-based action recognition (SAR)</strong> is now feasible [1].</li>
<li>Although much progress has been achieved, these methods are still facing two challenges.<ul>
<li>We term the ﬁrst one as the <strong>discriminative challenge.</strong></li>
<li>We term the second challenge as <strong>adaptability</strong>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Pose-Estimation"><a href="#Pose-Estimation" class="headerlink" title="Pose Estimation"></a>Pose Estimation</h3><ul>
<li><h4 id="Towards-3D-Human-Pose-Estimation-in-the-Wild-A-Weakly-Supervised-Approach"><a href="#Towards-3D-Human-Pose-Estimation-in-the-Wild-A-Weakly-Supervised-Approach" class="headerlink" title="Towards 3D Human Pose Estimation in the Wild: A Weakly-Supervised Approach"></a><a href="http://openaccess.thecvf.com/content_iccv_2017/html/Zhou_Towards_3D_Human_ICCV_2017_paper.html" target="_blank" rel="external">Towards 3D Human Pose Estimation in the Wild: A Weakly-Supervised Approach</a></h4><ul>
<li>We propose a <strong>weakly-supervised transfer learning</strong> method that uses mixed 2D and 3D labels in a uniﬁed deep neutral network that presents two-stage cascaded structure.</li>
</ul>
</li>
</ul>
<h3 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h3><ul>
<li><h4 id="Flow-Guided-Feature-Aggregation-for-Video-Object-Detection"><a href="#Flow-Guided-Feature-Aggregation-for-Video-Object-Detection" class="headerlink" title="Flow-Guided Feature Aggregation for Video Object Detection"></a>Flow-Guided Feature Aggregation for Video Object Detection</h4><ul>
<li>Video object detection</li>
<li>The accuracy of detection suffers from degenerated object appearances in videos, e.g., motion blur, video defocus, rare poses, etc.</li>
<li>We present ﬂow-guided feature aggregation, an accurate and <strong>end-to-end</strong> learning framework for <strong>video object detection.</strong></li>
<li>It leverages <strong>temporal coherence</strong> on feature level instead.</li>
<li>它通过沿着运动路径聚集附近的特征来改进每帧特征，从而提高了视频识别的准确性。</li>
<li>Fast moving objects.</li>
</ul>
</li>
<li><h4 id="DeNet-Scalable-Real-time-Object-Detection-with-Directed-Sparse-Sampling"><a href="#DeNet-Scalable-Real-time-Object-Detection-with-Directed-Sparse-Sampling" class="headerlink" title="DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling"></a>DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling</h4><ul>
<li>We deﬁne the object detection from imagery problem as estimating a very large but <strong>extremely sparse bounding box</strong> dependent probability distribution. （我们将图像问题中的目标检测定义为估计非常大但极其稀疏的边界框相关概率分布。）</li>
<li>Two novelties:<ul>
<li>a <strong>corner based</strong> region-of-interest estimator</li>
<li>a <strong>deconvolution based</strong> CNN model</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Image-Recognition"><a href="#Image-Recognition" class="headerlink" title="Image Recognition"></a>Image Recognition</h3><ul>
<li><h4 id="Multi-label-Image-Recognition-by-Recurrently-Discovering-Attentional-Regions"><a href="#Multi-label-Image-Recognition-by-Recurrently-Discovering-Attentional-Regions" class="headerlink" title="Multi-label Image Recognition by Recurrently Discovering Attentional Regions"></a>Multi-label Image Recognition by Recurrently Discovering Attentional Regions</h4><ul>
<li>Current solutions for this task usually rely on an extra step of extracting hypothesis regions (i.e., region proposals), resulting in <strong>redundant computation</strong> and <strong>sub-optimal performance.</strong></li>
<li>Developing a recurrent memorized-attention module.</li>
<li>This module consists of two alternately performed components:<ul>
<li>a spatial transformer layer to locate attentional regions from the convolutional feature maps in a region-proposal-free way</li>
<li>an LSTM (Long-Short Term Memory) sub-network to sequentially predict semantic labeling scores on the located regions while capturing the global dependencies of these regions.</li>
</ul>
</li>
<li>Despite acknowledged successes, these methods take the redundant computational cost of extracting region proposals and usually over-simplify the contextual dependencies among foreground objects, leading to a sub-optimal performance in complex scenarios.</li>
</ul>
</li>
</ul>
<p>  ​<br>  ​<br>  ​<br>  ​    </p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Visual-object-tracking&quot;&gt;&lt;a href=&quot;#Visual-object-tracking&quot; class=&quot;headerlink&quot; title=&quot;Visual object tracking&quot;&gt;&lt;/a&gt;Visual object tracking&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;h4 id=&quot;Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades&quot;&gt;&lt;a href=&quot;#Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades&quot; class=&quot;headerlink&quot; title=&quot;Learning Policies for Adaptive Tracking with Deep Feature Cascades&quot;&gt;&lt;/a&gt;Learning Policies for Adaptive Tracking with Deep Feature Cascades&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Our fundamental insight is to take an adaptive approach, where easy frames are processed with cheap features (such as pixel values), while challenging frames are processed with invariant but expensive deep features.&lt;/li&gt;
&lt;li&gt;Formulate the adaptive tracking problem as a decision-making process.&lt;/li&gt;
&lt;li&gt;Learn an agent to decide whether to locate objects with high conﬁdence on an early layer, or continue processing subsequent layers of a network.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;Signiﬁcantly reduces the feedforward cost.&lt;/li&gt;
&lt;li&gt;Train the agent ofﬂine in a reinforcement learning fashion.&lt;/li&gt;
&lt;li&gt;Obviously, the major computational burden comes from the forward pass through the entire network, and can be larger with deeper architectures.&lt;/li&gt;
&lt;li&gt;However, when the object is visually distinct or barely moves, early layers are in most scenarios sufﬁcient for precise localization - offering the potential for substantial computational savings.&lt;/li&gt;
&lt;li&gt;The agent learns to ﬁnd the target at each layer, and decides if it is conﬁdent enough to output and stop there.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文调研" scheme="http://jacobkong.github.io/tags/%E8%AE%BA%E6%96%87%E8%B0%83%E7%A0%94/"/>
    
  </entry>
  
  <entry>
    <title>行为识别论文笔记：行为分类深度模型的总结.md</title>
    <link href="http://jacobkong.github.io/posts/679115822/"/>
    <id>http://jacobkong.github.io/posts/679115822/</id>
    <published>2017-11-24T07:51:24.000Z</published>
    <updated>2017-11-29T08:42:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>本次主要总结了目前常见一些经典的基于深度学习的行为分类模型。其中的主要内容来自于论文《Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset》中的Related Work部分的总结。</p>
<a id="more"></a>
<p>虽然近年来图像表示体系结构的发展已经迅速成熟，但视频的前端运行架构仍然不够清晰。当前视频体系结构中的一些主要差异在于convolutional and layers operators是使用2D（基于图像的）还是3D（基于视频的）kernels; 无论网络输入是RGB视频或者还是包含预先计算的光流，在2D ConvNets的情况下，对于信息如何跨帧传播，这可以通过使用诸如LSTM之类的temporally-recurrent layers或者随着时间的推移进行特征聚合来完成。</p>
<p>图1显示了我们评估的五种体系结构的图形概述。<br><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fly3z4y2x1j316f0cxaf1.jpg" alt="图 1. 各种视频架构。其中K表示一个视频中全部的帧数，N表示一段视频相邻帧子集"></p>
<h3 id="模型1：ConvNet-LSTM"><a href="#模型1：ConvNet-LSTM" class="headerlink" title="模型1：ConvNet+LSTM"></a>模型1：ConvNet+LSTM</h3><p>图像分类网络的高性能使得我们尝试只需很少的改变就能将其重用于视频中。 通过使用它们针对每个独立帧提取特征，然后集中在整个视频中来提取预测结果来实现这一目标。 这是bag of words图像建模方法的精神; 但是在实践中使用方便的同时，还存在完全忽略时间结构的问题（例如，模型不能很好的区分开门和关门）。</p>
<p>理论上，更令人满意的方法是向模型添加一个recurrent layer，例如可以对状态进行编码的LSTM，并捕获<strong>时间顺序</strong>和<strong>长程依赖性</strong>。 本文在有512个隐藏单元的Inception-V1的最后的平均池化层之后防止了一个有着BN的LSTM层， 一个全连接层被添加到分类器的顶部。</p>
<p>该模型对所有时间步骤的输出上使用交叉熵损失进行训练。 在测试过程中，我们只考虑最后一帧的输出。</p>
<h3 id="模型2：3D-ConvNets"><a href="#模型2：3D-ConvNets" class="headerlink" title="模型2：3D ConvNets"></a>模型2：3D ConvNets</h3><p>3D ConvNets似乎是一种更自然的视频建模方法，其就像标准的卷积网络一样，但是具有时空卷积核。<strong>它们有一个非常重要的特征</strong>：它们直接创建时空数据的分层表示。这些模型的一个问题是，由于附加的内核维度，它们比2D ConvNets<strong>有更多的参数</strong>，这使得它们<strong>更难以训练</strong>。另外，它们似乎排除了ImageNet预训练的好处，因此以前的工作定义了相对较浅的架构，并且都是train from scratch。基准测试的结果具有提升的前景，但是与最新的技术水平相比还不具有竞争性，使得这种类型的模型成为评估我们大型数据集的好选择。</p>
<p>在本文中，我们实现了一个C3D模型的小变体，它在顶部有8个卷积层，5个池化层和2个完全连接的层。模型的输入与原始实现相同，使用112×112像素共16帧。与原始C3D不同的是，我们在所有卷积和全连接层之后使用了batch normalization。另一个区别在于对于第一个池化层，我们使用stride=2而不是stride=1，这减少了内存占用，并允许更大批量 - 这对于批量标准化非常重要。使用这一步，我们能够使用标准的K40 GPU在每个GPU上每批处理15个视频。</p>
<h3 id="模型3：Two-Stream-Networks"><a href="#模型3：Two-Stream-Networks" class="headerlink" title="模型3：Two-Stream Networks"></a>模型3：Two-Stream Networks</h3><p>来自ConvNets最后层的特征，LSTM可以模拟高层次的变化，但是可能无法捕获在许多情况下非常关键的精细的low-level动作。训练也是昂贵的，因为它需要通过多帧来展开网络以便反向传播。</p>
<p>Simonyan和Zisserman介绍了一种不同的非常实用的方法，在通过两个副本ImageNet预先训练的ConvNet后，通过对来自单个RGB帧的预测和10个外部计算的光流帧的预测进行平均，来对视频的短时间快照进行建模。光流 stream有一个自适应的输入卷积层，输入通道的数量是光流帧的两倍（因为流量有两个通道，水平和垂直），在测试时，多个快照从视频中采样，并对动作预测进行平均。这被证明在现有的基准测试中得到了非常高的性能，同时非常有效地进行训练和测试。</p>
<p>最近的一个扩展[8]将最后一个网络卷积层之后的空间流和光流融合起来，显示出对HMDB的一些改进，同时需要较少的测试时间增量（快照采样）。我们的实现大致使用了Inception-V1。网络的输入是5个连续的RGB帧，相隔10帧，以及相应的光流片段。 Inception-V1（5×7×7特征网格，对应于时间x和y维度）的最后一个平均汇聚层之前的空间和运动特征通过具有512个输出通道的3×3×3的3D卷积层，然后是3×3×3的3D最大池层，并通过最终的完全连接层。这些新层的权重用高斯噪声初始化。</p>
<p>两种模型（原始双流和3D融合版本）都是端对端训练（包括原始模型中的双流平均流程）。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次主要总结了目前常见一些经典的基于深度学习的行为分类模型。其中的主要内容来自于论文《Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset》中的Related Work部分的总结。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Action Recognition" scheme="http://jacobkong.github.io/tags/Action-Recognition/"/>
    
      <category term="行为识别" scheme="http://jacobkong.github.io/tags/%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：DSSD</title>
    <link href="http://jacobkong.github.io/posts/2938514597/"/>
    <id>http://jacobkong.github.io/posts/2938514597/</id>
    <published>2017-03-31T07:51:24.000Z</published>
    <updated>2017-04-01T02:16:57.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>本文的主要贡献在于在当前最好的通用目标检测器中加入了额外的上下文信息。</li>
<li>为实现这一目的：我们通过将<strong>ResNet-101</strong>与<strong>SSD</strong>结合。然后，我们用<strong>deconvolution layers</strong>来丰富了SSD + Residual-101，以便在物体检测中引入额外的large-scale的上下文，并提高准确性，<strong>特别是对于小物体</strong>，从而称之为<strong>DSSD</strong>。</li>
<li>我们通过仔细的加入额外的<strong>learned transformations阶段</strong>，具体来说是一个用于在deconvolution中前向传递连接的模块，以及一个新的输出模型，使得这个新的方法变得可行，并为之后的研究提供一个潜在的道路。</li>
<li>我们的DSSD具有513×513的输入，在VOC2007测试中达到81.5％de的mAP，VOC2012测试为80.0％de的mAP，COCO为33.2％的mAP，<strong>在每个数据集上优于最先进的R-FCN</strong> 。</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>最近的一些目标检测方法回归到了<strong>滑动窗口技术</strong>，这种技术随着更强大的整合了深度学习的机器学习框架而回归。</li>
<li>Faster RCNN -&gt; YOLO -&gt; SSD.</li>
<li>回顾最近的这些优秀的目标检测框架，要想提高检测准确率，一个很明显的目标就是：利用更好的特征网络并且添加更多的上下文，特别是对于小物体，另外还要提高边界框预测过程的空间分辨率。</li>
<li>在目标检测之外，最近有一个集成上下文的工作，利用所谓的<strong>“encoder-decoder”网络</strong>。该网络中间的bottleneck layer用于编码关于输入图像的信息，然后逐渐地更大的层将其解码到整个图像的map中。所形成的wide，narrow，wide的网络结构通常被称为沙漏。</li>
<li>但是有必要仔细构建用于集成反卷积的组合模块和输出模块，以在训练期间隔绝ResNet-101层，从而允许有效的学习。</li>
</ul>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul>
<li>SPPnet, Fast R-CNN, Faster R-CNN, R-FCN, YOLO：使用卷积网络的最上面的层来进行不同尺度的物体检测。</li>
<li>通过在ConvNet中开发多层来提高检测精度的方法有多重。<ul>
<li>第一种方法：组合了ConvNet不同层的特征图，并使用组合特征图进行预测。</li>
<li>ION利用L2 normalization来结合多个VGGNet和池化层的特征来进行目标检测。<ul>
<li>HyperNet也是使用类似于ION的方法。</li>
<li>但是这种结合多层特征的方法不仅增加内存，而且降低了模型的速度。</li>
</ul>
</li>
<li>第二种方法：使用ConvNet中的不同层来预测不同尺度的对象。<ul>
<li>因为不同层中的节点具有不同的接收域，所以自然会<strong>从具有大型接收场的层预测大对象，并使用具有小接收场的层来预测小物体。</strong></li>
<li>SSD将不同尺度的默认框扩展到ConvNet中的多个层，并强制执行每一层专注于预测一定规模的对象。</li>
<li>S-CNN [2]在ConvNet的多层应用去卷积，以在使用层去学习region proposal和pool feature之前<strong>增加feature maps的分辨率。</strong></li>
<li>然而，为了很好地检测小物体，这些方法需要从具有小的接收场和密集特征图的浅层中使用一些信息，这可能导致在检测小对象性能较低，因为浅层具有较少的关于对象的语义信息。</li>
<li>通过使用deconvolution layers和skip connections,，我们可以在密集（去卷积）特征图中注入更多的信息，从而有助于预测小物体。</li>
</ul>
</li>
<li>另外还有一个工作方法，尽量去包括预测的上下文信息。<ul>
<li>Multi-Region CNN</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Deconvolutional-DSSD-model-Single-Shot-Detection"><a href="#Deconvolutional-DSSD-model-Single-Shot-Detection" class="headerlink" title="Deconvolutional (DSSD) model Single Shot Detection"></a>Deconvolutional (DSSD) model Single Shot Detection</h3><h3 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h3><p><img src="https://ww1.sinaimg.cn/large/006tNc79gy1fe6cf9kja4j30wd0beta0.jpg" alt=""></p>
<ul>
<li>SSD构建于base network之上，添加了一些逐渐见效的卷积层，如上图蓝色部分。</li>
<li>每个添加的层和一些较早的基本网络层用于预测某些预定义的边界框的分数和偏移。 </li>
<li>这些预测由3x3x＃个通道维数的滤波器执行，一个滤波器用于产生每个类别分数，一个用于回归边界框的每个维度。 </li>
<li>它使用非最大抑制（NMS）对预测进行后处理，以获得最终检测结果。 </li>
</ul>
<h3 id="Using-Residual-101-in-place-of-VGG"><a href="#Using-Residual-101-in-place-of-VGG" class="headerlink" title="Using Residual-101 in place of VGG"></a>Using Residual-101 in place of VGG</h3><p>将Base Network从VGG16换为ResNet-101并未提升结果，但是添加额外的<strong>prediction module</strong>会显著地提成性能。</p>
<h3 id="prediction-module"><a href="#prediction-module" class="headerlink" title="prediction module"></a>prediction module</h3><p><img src="https://ww1.sinaimg.cn/large/006tNc79gy1fe6cpqgt1rj311z0gxtbt.jpg" alt=""></p>
<ul>
<li>在原始SSD中，目标函数直接应用于所选择的特征图，并且由于梯度的大幅度，使用L2标准化层用于conv4 3层。</li>
<li>MS-CNN指出，改进每个任务的子网可以提高准确性，按照这个原则，我们为每个预测层添加一个残差块，如图2变体（c）所示。</li>
<li>我们还尝试了原始SSD方法（a）和具有跳过连接（b）的残余块的版本以及两个顺序的残余块（d）。 我们注意到，<strong>ResNet-101和预测模块似乎显著优于对于较高分辨率输入图像没有预测模块的VGG。</strong></li>
</ul>
<h3 id="Deconvolutional-SSD"><a href="#Deconvolutional-SSD" class="headerlink" title="Deconvolutional SSD"></a>Deconvolutional SSD</h3><p><img src="https://ww2.sinaimg.cn/large/006tNc79gy1fe6cyu3qi7j311j0f9aca.jpg" alt=""></p>
<ul>
<li>为了在检测中包含更多的高层次上下文，我们将prediction module转移到在原始SSD设置之后放置的一系列去卷积层中，有效地制作了非对称沙漏网络结构。</li>
<li>添加额外的去卷积层，以连续增加feature maps layers的分辨率。为了加强特征，我们采用了沙漏模型中“<strong>跳跃连接</strong>”的想法。</li>
<li>尽管沙漏模型在编码器和解码器阶段均包含对称层，但由于两个原因，我们使解码器阶段非常浅。</li>
</ul>
<h3 id="Deconvolution-Module"><a href="#Deconvolution-Module" class="headerlink" title="Deconvolution Module"></a>Deconvolution Module</h3><p><img src="https://ww3.sinaimg.cn/large/006tNc79gy1fe6d85jld2j30ht0g9wfq.jpg" alt=""></p>
<ul>
<li>为了帮助整合早期特征图和去卷积层的信息，我们引入了一个去卷积模块，如图3所示。</li>
<li>首先，在每个卷积层之后添加BN层。</li>
<li>第二，我们使用学习的去卷积层代替双线性上采样。</li>
<li>最后，我们测试不同的组合方法：element-wise sum and element-wise product。</li>
</ul>
<h3 id="Traning"><a href="#Traning" class="headerlink" title="Traning"></a>Traning</h3><ul>
<li>我们遵循与SSD相同的训练政策。</li>
<li>在原始SSD模型中，长宽比为2和3的boxes从实验中证明是有用的。为了了解训练数据（PASCAL VOC 2007和2012年 trainval）中边界框的纵横比，我们以training box运行K-means聚类，以方格平方根为特征。我们从两个集群开始，如果错误可以提高20％以上，就会增加集群的数量。经过试验因此，我们决定在每个预测层添加一个宽高比1.6，并使用（1.6, 2.0, 3.0）。</li>
</ul>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ul>
<li>PASCAL VOC2007 test detection results.</li>
</ul>
<p><img src="https://ww2.sinaimg.cn/large/006tNc79gy1fe6ykpyluhj30s30bgwlo.jpg" alt=""></p>
<ul>
<li>PASCAL 2012 test detection results.</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tNc79gy1fe6yl98p6tj30rp080dka.jpg" alt=""></p>
<ul>
<li>COCO test-dev2015 detection results.</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tNc79gy1fe6ylysfswj30s809ldjq.jpg" alt=""></p>
<ul>
<li>Comparison of Speed &amp; Accuracy on PASCAL VOC2007 test.</li>
</ul>
<p><img src="https://ww1.sinaimg.cn/large/006tNc79gy1fe6ymnt5vsj30rp0cnn1k.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;本文的主要贡献在于在当前最好的通用目标检测器中加入了额外的上下文信息。&lt;/li&gt;
&lt;li&gt;为实现这一目的：我们通过将&lt;strong&gt;ResNet-101&lt;/strong&gt;与&lt;strong&gt;SSD&lt;/strong&gt;结合。然后，我们用&lt;strong&gt;deconvolution layers&lt;/strong&gt;来丰富了SSD + Residual-101，以便在物体检测中引入额外的large-scale的上下文，并提高准确性，&lt;strong&gt;特别是对于小物体&lt;/strong&gt;，从而称之为&lt;strong&gt;DSSD&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;我们通过仔细的加入额外的&lt;strong&gt;learned transformations阶段&lt;/strong&gt;，具体来说是一个用于在deconvolution中前向传递连接的模块，以及一个新的输出模型，使得这个新的方法变得可行，并为之后的研究提供一个潜在的道路。&lt;/li&gt;
&lt;li&gt;我们的DSSD具有513×513的输入，在VOC2007测试中达到81.5％de的mAP，VOC2012测试为80.0％de的mAP，COCO为33.2％的mAP，&lt;strong&gt;在每个数据集上优于最先进的R-FCN&lt;/strong&gt; 。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：Deep Residual Learning for Image Recognition</title>
    <link href="http://jacobkong.github.io/posts/3085218970/"/>
    <id>http://jacobkong.github.io/posts/3085218970/</id>
    <published>2017-03-23T07:51:24.000Z</published>
    <updated>2017-03-28T08:02:39.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>本文是何凯明大神的又一篇CVPR最佳论文。</li>
<li>网络越深越难训练，所以我们提出一个residual learning framework从而减轻网络的训练，该网络比以前使用的网络要深得多。</li>
<li>我们明确地将参考层的输入来作为学习残差函数，而不是学习无参考的函数（unreferenced functions）。</li>
<li>我们提供全面的经验证据，表明这些残留网络更容易优化，并可以从显着增加的深度中获得准确性。</li>
<li>这些残留网络的集合在ImageNet测试集上达到3.57％的误差。 该结果在ILSVRC 2015分类任务中荣获第一名。</li>
<li><strong>深度对于许多CV领域的任务都十分重要的。</strong>由于我们网络很深，我们在COCO对象检测数据集上获得了28％的相对改进。我们还荣获了ImageNet检测，ImageNet定位，COCO检测和COCO分割任务的第一名。</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><p>深层网络自然地将低/中/高层特征和分类器以端到端多层方式进行集成，并且特征的“级别”可以通过堆叠层数（深度）来丰富。<strong>网络的深度有着十分重要的作用。</strong></p>
</li>
<li><p>随着网络深度的增加，带来一个问题：<strong>学习更好的网络是否和堆叠更多的层一样简单？</strong>回答这个问题的障碍是：<strong>逐渐消失的梯度问题</strong>。</p>
</li>
<li><p>当较深的网络能够开始收敛时，暴露了一个退化问题：<strong>随着网络深度的增加，精度饱和，然后迅速下降。</strong>这种下降不是由于过拟合，添加多层会导致更高的训练错误。</p>
</li>
<li><p>从浅到深的一个解决方案：</p>
<ul>
<li>附加层：设置为“恒等”（identity）</li>
<li>原始层：由一个已经学会的较浅模型复制得来。</li>
<li>这种解决方案的存在表明，较深的模型不应该产生比较浅的模型更高的训练误差。至少具有相同的训练误差。</li>
</ul>
</li>
<li><p>优化难题：<strong>随着网络层数不断加深，求解器不能找到解决途径。</strong></p>
</li>
<li><p>为了解决这个问题，本文提出了<strong>深度残差学习框架</strong>。</p>
</li>
<li><p><strong>平原网络</strong>：</p>
<p><img src="https://ww3.sinaimg.cn/large/006tNc79gy1fdyd28cydnj30k706wt99.jpg" alt=""></p>
<p>H(x)是任意一种理想的映射</p>
<p>平原网络希望第2层权重层能够<strong>与H(x)拟合</strong>。</p>
</li>
<li><p><strong>残差网络</strong>：</p>
<p><img src="https://ww3.sinaimg.cn/large/006tNc79gy1fdyd3gt5c5j30ki076dgr.jpg" alt=""></p>
<p>H(x)是任意一种理想的映射</p>
<p>残差网络希望第2类权重层能够与<strong>F(x)拟合使得H(x) = F(x) + x</strong></p>
</li>
<li><p>F(x)是一个残差映射w.r.t 恒</p>
<ul>
<li>如果说恒等是理想，很容易将权重值设定为0；</li>
<li>如果理想化映射更接近于恒等映射，便更容易发现微小波动。</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tNc79gy1fdyd6l6pwhj30kl074my2.jpg" alt=""></p>
</li>
<li><p>我们假设优化残差映射比优化原始的，无参考映射(unreferenced mapping)更容易。在极端情况下，如果一个identity mapping是最佳的，那么将残差推到零比通过一堆非线性层的identity mapping更容易。</p>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;本文是何凯明大神的又一篇CVPR最佳论文。&lt;/li&gt;
&lt;li&gt;网络越深越难训练，所以我们提出一个residual learning framework从而减轻网络的训练，该网络比以前使用的网络要深得多。&lt;/li&gt;
&lt;li&gt;我们明确地将参考层的输入来作为学习残差函数，而不是学习无参考的函数（unreferenced functions）。&lt;/li&gt;
&lt;li&gt;我们提供全面的经验证据，表明这些残留网络更容易优化，并可以从显着增加的深度中获得准确性。&lt;/li&gt;
&lt;li&gt;这些残留网络的集合在ImageNet测试集上达到3.57％的误差。 该结果在ILSVRC 2015分类任务中荣获第一名。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;深度对于许多CV领域的任务都十分重要的。&lt;/strong&gt;由于我们网络很深，我们在COCO对象检测数据集上获得了28％的相对改进。我们还荣获了ImageNet检测，ImageNet定位，COCO检测和COCO分割任务的第一名。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>目标检测论文笔记：R-FCN</title>
    <link href="http://jacobkong.github.io/posts/3678248031/"/>
    <id>http://jacobkong.github.io/posts/3678248031/</id>
    <published>2017-02-27T07:51:24.000Z</published>
    <updated>2018-05-28T07:35:11.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>提出了一个region-based, fully convolutional的网络来准确高效的进行物体检测。</li>
<li>不同于Fast/Faster R-CNN，其应用了计算成本很高的每个区域子网络数百次，本论文的region-based detector是完全卷积化的，几乎一张图像上所有的计算都是共享的。</li>
<li>为了实现这一目标，我们提出position-sensitive score maps，以解决在图像分类的平移不变性（translation-invariance）和物体检测中的平移可变性（translation-variance）之间的困境。</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>最近流行的用于目标检测的深度学习框架依据RoI层的不同可以分为两大subnetworks：<ul>
<li>一类是独立于RoIs的、共享的、fully convolutional的subnetwork。</li>
<li>另一类是RoI-wise的subnetwork，不共享计算。</li>
</ul>
</li>
<li>在图像分类网络中，一个convolutional subnetwork会以一个sptial pooling layer跟随着几个fully-connected  layer最为结尾，所以图像分类中的sptial pooling layer自然转化为目标检测中的RoI pooling layer。</li>
<li>ResNet和GoogleLeNets都被设计成fully convolutional的。</li>
<li>在ResNet论文中，Faster R-CNN中的RoI pooling layer被不自然的插入到两个卷积层集之间，带来了准确率的提升，<strong>但是速度由于unshared per-RoI计算降低。</strong></li>
<li>对于<strong>图像分类</strong>任务来说：更倾向于平移不变性。对于<strong>图像检测</strong>任务来说：更倾向于<strong>平移变换性</strong>。</li>
<li>假设图像分类网络中更深层的卷积层对translation不敏感，所以为了解决translation invariance和translation variance之间的困难，ResNet将RoI pooling layer插入到了卷积神经网络之间。这个区域特定的操作打破了平移不变性，并且在不同区域之间进行评估时，RoI之后的卷积层不再是平移不变的。</li>
<li>为了将translation variance结合到FCN中，我们通过使用一组专用卷积层作为FCN输出来构造一组位置敏感得分图（position-sensitive score maps）。每一个得分图将相对于相对空间位置（例如，“在对象的左边”）的位置信息进行编码。在这个FCN之上，我们附加一个位置敏感的RoI池层（position-sensitive RoI pooling layer），从这些得分图中获取信息，没有跟随的权重的（卷积/ fc）层。</li>
</ul>
<h2 id="Our-approach"><a href="#Our-approach" class="headerlink" title="Our approach"></a>Our approach</h2><ul>
<li><p>本论文的方法参考R-CNN，也是使用two-stage的目标检测策略。</p>
<ul>
<li>region proposal</li>
<li>region classification</li>
</ul>
</li>
<li><p>虽然不依赖于region proposal的目标检测方法确实存在，如SSD何YOLO，但是region-based system依旧在几个基准上保持领先的准确性。</p>
</li>
<li><p>Overall architecture of R-FCN:</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgy1fd58i0j1icj30ql0dcmzl.jpg" alt=""></p>
<p>用RPN来提出candidate RoIs，然后这些RoIs被应用到score maps，在RPN和R-FCN之间共享特征。</p>
<p>给定一个RoI，R-FCN架构对RoI进行分类（分为物体类别或者背景）。所有可学习权值的层都是卷积层，并且是在整张图片上计算得到的权重。<strong>最后卷积层为每个类别产生一个$k^2$个position-sensitive score maps</strong>，因此具有带有C个对象类别（背景为+1）的$k^2(C + 1)$通道的输出层。</p>
<p><img src="https://ww3.sinaimg.cn/large/006tNc79gy1fdvwup92g7j302p06e0sp.jpg" alt=""></p>
<p><strong>每一个category有一个$k^2$的score map</strong>，对于这里来说k=3，所以最后RoI pooling层产生3x3x(C+1)维的feature map。</p>
</li>
<li><p>RPN以一个position-sensitive RoI pooling layer结束，该层聚合最后卷积层的输出并产生每个RoI的分数。我们的位置敏感的RoI pooling layer进行选择性合并，each of the k × k bin aggregates responses from only one score map out of the bank of k × k score maps。利用端到端训练，这个RoI层管理最后的卷积层以学习专门的position-sensitive score maps。 </p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcly1fd597p3an8j30vs0en40p.jpg" alt=""></p>
</li>
</ul>
<h3 id="Backbone-architecture"><a href="#Backbone-architecture" class="headerlink" title="Backbone architecture"></a>Backbone architecture</h3><ul>
<li>本论文R-FCN基于ResNet-101。</li>
<li>ResNet-101具有100个卷积层，后面是global average pooling和一个1000-class的fc层。我们移去了average pooling layer and the fc layer，仅使用convolutional layer来计算feature maps。</li>
<li>我们使用ResNet-101，在ImageNet上进行预训练，ResNet-101中的最后一个卷积块是2048-d，并且我们附加随机初始化的1024-d 1×1卷积层以减小尺寸。然后，我们应用$k^2(C + 1)$通道卷积层来生成分数图，如下所述。</li>
</ul>
<h3 id="Position-sensitive-score-maps-amp-Position-sensitive-RoI-pooling"><a href="#Position-sensitive-score-maps-amp-Position-sensitive-RoI-pooling" class="headerlink" title="Position-sensitive score maps &amp; Position-sensitive RoI pooling."></a>Position-sensitive score maps &amp; Position-sensitive RoI pooling.</h3><ul>
<li><p>为了将位置信息显式编码到每个RoI中，我们将RoI矩形划分为k x k个bins。</p>
</li>
<li><p>最后的卷积层为每个类别产生的$k^2$个分数图。在第(i, j)个bin内，我们定义了一个位置敏感的RoI池化操作，从而只在第(i, j)个score map上进行池化：</p>
<script type="math/tex; mode=display">
r_c(i, j|\theta)=\sum_{(x,y)\in bin(i,j)}{z_{i,j,c}(x+x_0,y+y_0|\theta)/n}</script><ul>
<li>其中$r_c(i, j)$表示从c-th类别中得到的(i,j)-th bin的池化响应。</li>
<li>$z_{i,j,c}$是$k^2(C+1)$个score maps中的一个score map。</li>
<li>$(x_0,y_0)$表示一个RoI的左上角。</li>
<li>$n$表示这个bin中的像素的数量。</li>
<li>$\theta$表示这个网络中所有的可学习权重。</li>
<li>该pooling属于AVE，也可以用MAX。</li>
</ul>
</li>
<li><p>对每个类别k*k的score map进行平均，最后每个RoI得到一个C+1维的向量。然后求loss，它们用于评估训练期间的交叉熵损失和推理期间的RoIs排名。</p>
</li>
<li><p>bounding boxes regression。对每个RoI产生一个$4k^2$维的向量。然后通过average voting将其聚合成4维向量。</p>
</li>
<li><p>在RoI层之后没有可学习的层次，实现了几乎无成本地区的计算、加速训练和推理。</p>
</li>
</ul>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ul>
<li><p>利用提前计算好的region proposal，很容易来端到端的R-FCN架构训练。</p>
</li>
<li><p>loss function:</p>
<script type="math/tex; mode=display">
L(s,t_{x,y,w,h})=L_{cls}(s_{c^*})+\lambda[c^*>0]L_{reg}(t,t^*)</script></li>
<li><p>本框架很容易在训练的时候使用online hard example mining (OHEM)。</p>
<ul>
<li>我们的per-RoI计算可以进行几乎cost-free的example mining。</li>
<li>在前向传播中：假设每张图片N个proposals。我们计算所有N个proposal的loss，排序，选择最高的B个RoIs。然后在选中的proposal上进行反向传播。</li>
</ul>
</li>
<li><p>decay：0.0005</p>
</li>
<li><p>momentum：0.9</p>
</li>
<li><p>single-scale training。</p>
</li>
<li><p>B=128</p>
</li>
<li><p>lr = 0.001 ~20k, 0.0001 ~ 10k</p>
</li>
<li><p>同Faster R-CNN一趟，使用4步alternating training，在训练RPN和训练R-FCN之间。</p>
</li>
</ul>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul>
<li>为公平期间，在300个RoIs上进行评估，结果之后用NMS进行处理，IoU阈值0.3</li>
</ul>
<h3 id="A-trous-and-stride"><a href="#A-trous-and-stride" class="headerlink" title="À trous and stride"></a>À trous and stride</h3><ul>
<li>将ResNet-10的有效stride从32减为16像素，提高了score map的分辨率。</li>
<li>conv4阶段之前（stride=16）的所有层都没有改变。</li>
<li>第一个conv5块儿的stride从2改为1，并且conv5阶段的卷积核都被改为hole algorithm，（Algorithme à trous）以补偿减少的步幅。</li>
<li>为了公平比较，RPN在conv4之上进行计算。从而RPN不被à trous影响。</li>
</ul>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>R-CNN已经说明了带深度网络的区域候选的有效性。R-CNN计算那些关于裁剪不正常的覆盖区域的卷积网络，并且计算在区域直接是不共享的。SPPnet，Fast R-CNN和Faster R-CNN是半卷积的（semi-convolutional），在卷积子网络中是计算共享的，在另一个子网络是各自计算独立的区域。</p>
<p>物体检测器可以被认为是全卷积模型。OverFeat 检测物体通过在convolutional feature maps上进行多尺度的窗口滑动。 在某些情况下，可以将单精度的滑动窗口改造成一个单层的卷积层。在Faster R-CNN中的RPN组件是一个全卷积检测器，用来预测是一个关于多尺寸的参考边框的实际边框。原始的RPN是class-agnostic（class无关的）。但是对应的class-specific是可应用的。</p>
<p>另一个用于物体检测的是fc layer（fully-connected）用来基于整幅图片的完整物体检测。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;提出了一个region-based, fully convolutional的网络来准确高效的进行物体检测。&lt;/li&gt;
&lt;li&gt;不同于Fast/Faster R-CNN，其应用了计算成本很高的每个区域子网络数百次，本论文的region-based detector是完全卷积化的，几乎一张图像上所有的计算都是共享的。&lt;/li&gt;
&lt;li&gt;为了实现这一目标，我们提出position-sensitive score maps，以解决在图像分类的平移不变性（translation-invariance）和物体检测中的平移可变性（translation-variance）之间的困境。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：SSD</title>
    <link href="http://jacobkong.github.io/posts/3118967289/"/>
    <id>http://jacobkong.github.io/posts/3118967289/</id>
    <published>2017-02-13T09:53:54.000Z</published>
    <updated>2017-02-16T02:30:35.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><ul>
<li>Jaccard overlap, Jaccard similarity:<br>Jaccard coefficient:<script type="math/tex; mode=display">
J(A,B)=\frac{|A\cap B|}{|A\cup B|}</script>A,B分别代表符合某种条件的集合：两个集合交集的大小/两个集合并集的大小，交集=并集意味着2个集合完全重合。<br><strong>所以Jaccard overlap其实就是IoU。</strong><a id="more"></a>
</li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>SSD: 利用单个深度神经网络的目标检测方法。将边界框的输出空间离散化为一组默认框，在每个feature map位置上有着不同的宽高比和尺度。</li>
<li>在预测的时候，网络针对每个默认框中的每个存在的对象类别产生分数，并且对框的进行调整以更好地匹配对象形状。</li>
<li><strong>在多尺度图像处理方面</strong>，网络组合来自具有不同分辨率的多个feature map的预测，以自然地处理各种尺寸的对象。</li>
<li>相比于基于object proposal的方法，SSD是简单地，因为它能够<strong>完全消除</strong>proposal generation和后续的<strong>像素或者特征重冲采样阶段</strong>，所有的计算都封装在单独的网络中。</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>目前的目标检测系统是以下方法的变体：假设边界框（bounding box），对每个框进行像素或特征重取样，采用高质量分类器。</li>
<li>评估速度方法：<strong>SPF (seconds per frame)</strong>.</li>
<li>提出第一个基于深度网络的不需要为BB进行resample pixels or features的目标检测器，并能够同样达到高准确率。</li>
<li>本论文的贡献（具体看论文）：<ul>
<li>引入了SSD。</li>
<li>SSD的核心。</li>
<li>为了实现高检测准确率，引入了在不同尺度和横纵比的feature maps上进行预测。</li>
<li>End-to-end training 以及高准确率，机试在低分辨率图片。</li>
<li>在PASCAL VOC、COCO和ILSVRC上进行试验，具有很强的竞争力。</li>
</ul>
</li>
</ul>
<h2 id="The-Single-Shot-Detector-SSD"><a href="#The-Single-Shot-Detector-SSD" class="headerlink" title="The Single Shot Detector (SSD)"></a>The Single Shot Detector (SSD)</h2><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><ul>
<li>基于前向卷积神经网络，产生固定尺寸的BB集，以及这些BB中存在物体的分数，之后跟随者一个非极大值抑制步骤来产生最终检测。</li>
<li><p>网络前面的几层是基于标准的用于产生高质量图像分类的架构，我们成为<strong>基础网络</strong>。我们给网络然后添加了<strong>辅助的结构</strong>来产生检测结构。辅助网络具备以下关键特征：</p>
<ul>
<li><p><strong>用于检测的多尺寸特征图。</strong>在基础网络后面<strong>添加额外几个卷积层</strong>，在尺寸上逐层递减，从而能够在不同尺寸上检测。（Overfeat和YOLO都只是在单独尺寸的feature map上进行操作。）</p>
</li>
<li><p><strong>用来预测的卷积预测器</strong>（Convolutional predictors）。</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgy1fcq4bw59jtj30j109i3zs.jpg" alt=""></p>
</li>
<li><p><strong>默认的boxes和aspect ratios。</strong>我们将一组默认边界框与每个feature map单元关联，用于网络顶部的多个特征映射。在每个feature map单元格中，我们预测相对于单元格中的默认框形状的偏移，以及指示每个框中存在类实例的每类分数。（本论文中的default boxes类似于Faster R-CNN中的anchor boxes，然而我们将他用于不同分辨率的几个feature maps中）</p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgy1fcp21vqfpgj30le0ey0xb.jpg" alt=""></p>
</li>
</ul>
</li>
</ul>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ul>
<li><p>训练SSD和训练使用region proposals的典型检测器之间的<strong>关键区别是</strong>：ground truth信息需要分配给固定的检测器输出集合中的特定输出。</p>
</li>
<li><p>训练涉及到：</p>
<ul>
<li>choosing the set of default boxes and scales for detection。</li>
<li>hard negative mining。</li>
<li>data augmentation strategies（数据增加策略）。</li>
</ul>
</li>
</ul>
<h4 id="Matching-strategy"><a href="#Matching-strategy" class="headerlink" title="Matching strategy"></a><strong>Matching strategy</strong></h4><p>在训练过程中，对于每一个ground truth，我们都从默认框中选择每个不同的位置、aspect ratio、scale的bounding boxes。首先匹配最好的jaccard overlap的default box（类似于MultiBox），但与MultiBox不同的是，我们然后匹配default box与任何ground truth，只要jaccard overlap高于阈值（0.5）。<strong>这样简化了学习问题。</strong></p>
<h4 id="Training-objective"><a href="#Training-objective" class="headerlink" title="Training objective"></a><strong>Training objective</strong></h4><p>整体的代价函数是localization loss和confidence loss之和：</p>
<script type="math/tex; mode=display">
L(x,c,l,g)=\frac{1}{N}(L_{conf}(x,c)+\alpha L_{loc}(x,l,g))</script><ul>
<li><p>N是匹配的default boxes的数量，N=0时，loss=0。</p>
</li>
<li><p>localization loss是predicted box和ground truth box之间的<strong>Smooth L1 loss</strong>（类似于Faster R-CNN）。我们预测default box的中心$(cx,cy)$，以及宽度$(w)$和长度$(h)$。</p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcjw1fcqyi2ogdnj30bj03l3yr.jpg" alt=""></p>
</li>
<li><p>confidence loss是多个类confidence$(c)$之间的<strong>softmax loss</strong>。</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgy1fcqyjitcw7j30eh01kjrg.jpg" alt=""></p>
</li>
<li><p>权值$\alpha$通过交叉验证设为1。</p>
</li>
</ul>
<h4 id="Choosing-scales-and-aspect-ratios-for-default-boxes"><a href="#Choosing-scales-and-aspect-ratios-for-default-boxes" class="headerlink" title="Choosing scales and aspect ratios for default boxes"></a>Choosing scales and aspect ratios for default boxes</h4><ul>
<li><p>不同于将照片处理为不同尺寸再结合结果的方法，本论文通过利用单个神经网络中不同层的feature maps，可以达到同样的效果，同时可以在所有尺寸中共享权值。</p>
</li>
<li><p><strong>利用较低层的feature maps可以提高semantic segmentation质量，应为较低层往往可以捕捉到更精细的细节。</strong></p>
</li>
<li><p>我们同时使用较低和较高层的feature maps来进行检测。</p>
</li>
<li><p>网络中不同层的feature maps有着不同的接受域的尺寸。</p>
</li>
<li><p>假设我们想要使用m个feature maps用来检测，则每个feature map的default boxes的scale可以这样计算：</p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcjw1fcr0sa2183j308k0190sn.jpg" alt=""></p>
</li>
<li><p>通过结合在许多feature maps上所有位置上的所有的有着不同scale和aspect ratio的default boxes，我们可以产生对不同物体大小和形状的各种预测。</p>
</li>
<li><p>如下图中，狗在8x8的feature map中没有匹配的default box，因此在训练中会被作为负样本，但是在4x4的feature map中有着匹配的feature map。</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcjw1fcr148r96xj30e70563zd.jpg" alt=""></p>
</li>
</ul>
<h4 id="Hard-negative-mining"><a href="#Hard-negative-mining" class="headerlink" title="Hard negative mining"></a>Hard negative mining</h4><ul>
<li>通过匹配阶段后，default boxes中会产生大量的negatives，尤其是当可能的default boxes数量非常大时。<strong>这导致positive和negative时间严重的不平衡。</strong></li>
<li>我们将negative examples的default boxes通过其最高的confidence loss进行排序，然后选择较高的几个，使negative examples和positive examples之间的比例保持在3:1之间。<strong>这样会更快的优化和更稳定的训练。</strong></li>
</ul>
<h2 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h2><ul>
<li>所有的实验都是基于VGG16。</li>
<li>将fc6和fc7转化为卷积层，从fc6和fc7中取样子参数。</li>
<li>将pool5从2x2-s2转化为3x3-s1。</li>
<li>使用a trous algorithm来填补“holes”。</li>
<li>移去了所有的dropout层和fc8层。</li>
<li>用SGD进行微调。</li>
<li>学习率$10^{-3}$，动量0.9，weight decay是0.0005，batch size是32.</li>
</ul>
<h3 id="PASCAL-VOC2007"><a href="#PASCAL-VOC2007" class="headerlink" title="PASCAL VOC2007"></a>PASCAL VOC2007</h3><ul>
<li><strong>“xavier” method</strong>来初始化新加入的层的参数。</li>
<li>通过detection analysis tool分析后，显示SSD有着<strong>更少的localization错误</strong>，因为其能够直接去学习regress物体的形状，并分类，而非使用两个互相解耦的步骤。</li>
<li>然而，SSD对于相似的物体会有更多的混淆，特别是animals，一部分原因。</li>
<li>SSD对bounding boxes尺寸是十分敏感的。提升输入尺寸可能会提升小物体检测，但依旧有许多空间提升。</li>
</ul>
<h3 id="Model-analysis"><a href="#Model-analysis" class="headerlink" title="Model analysis"></a>Model analysis</h3><ul>
<li>Data augmentation很重要。</li>
<li>更多的default boxes的形状可能会更好。</li>
<li>Atrous is faster：如果不使用更改后的VGG-16，虽然结果一样，但是速度回降低20%。</li>
<li>不同分辨率中多个输出层会更好。<strong>SSD的主要贡献</strong>是在不同输出层上使用不同尺度的默认框。</li>
</ul>
<h3 id="PASCAL-VOC2012"><a href="#PASCAL-VOC2012" class="headerlink" title="PASCAL VOC2012"></a>PASCAL VOC2012</h3><ul>
<li>和PASCAL VOC2007一样的实验设置。</li>
<li>在2012 trainval+2007 trainval+2007 test上进行训练，在2012 test上进行测试。</li>
</ul>
<h3 id="COCO"><a href="#COCO" class="headerlink" title="COCO"></a>COCO</h3><ul>
<li>COCO中的物体比PASCAL VOC中的物体小，所以我们在所有层上使用更小的default boxes</li>
</ul>
<h3 id="Data-Augmentation-for-Small-Object-Accuracy"><a href="#Data-Augmentation-for-Small-Object-Accuracy" class="headerlink" title="Data Augmentation for Small Object Accuracy"></a>Data Augmentation for Small Object Accuracy</h3><ul>
<li>对SSD来说，对小物体分类的任务相对Faster R-CNN来说会更难。</li>
<li>Data augmentation对于特高性能是十分显著的，尤其是小数据及。</li>
<li>改进SSD的另一种方法是设计更好的平铺默认框（tiling of default boxes），使其位置和尺度更好地与特征图上每个位置的接收场对准。</li>
</ul>
<h3 id="Inference-time"><a href="#Inference-time" class="headerlink" title="Inference time"></a>Inference time</h3><ul>
<li>考虑到从我们的方法生成的大量框，有必要在推理期间有效地执行非最大抑制（nms）。</li>
<li>通过使用0.01的限制阈值，我们可以过滤大多数bounding boxes。 然后我们应用nms，每个类别的jaccard重叠0.45，并保持每个图像前200个检测。</li>
<li>80%的前向传递时间被花费在了base network，所以使用一个更快的base network可以提高速度。</li>
</ul>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul>
<li>有两种已建立的用于图像中的对象检测的方法类别，<strong>一种基于滑动窗口，另一种基于region proposal classification。</strong></li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>我们模型的一个<strong>关键特性</strong>是使用多尺度卷积边界框输出附加到网络顶部的多个特征图。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;知识点&quot;&gt;&lt;a href=&quot;#知识点&quot; class=&quot;headerlink&quot; title=&quot;知识点&quot;&gt;&lt;/a&gt;知识点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Jaccard overlap, Jaccard similarity:&lt;br&gt;Jaccard coefficient:&lt;script type=&quot;math/tex; mode=display&quot;&gt;
J(A,B)=\frac{|A\cap B|}{|A\cup B|}&lt;/script&gt;A,B分别代表符合某种条件的集合：两个集合交集的大小/两个集合并集的大小，交集=并集意味着2个集合完全重合。&lt;br&gt;&lt;strong&gt;所以Jaccard overlap其实就是IoU。&lt;/strong&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：YOLO9000</title>
    <link href="http://jacobkong.github.io/posts/2102833929/"/>
    <id>http://jacobkong.github.io/posts/2102833929/</id>
    <published>2017-02-08T03:56:20.000Z</published>
    <updated>2018-05-18T04:04:53.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>YOLO9000: a state-of-the-art, real-time 的目标检测系统，可以检测超过9000种的物体分类。</li>
<li>本论文提出两个模型，<strong>YOLOv2和YOLO9000</strong>。</li>
<li>YOLOv2：<ul>
<li>是对YOLO改进后的提升模型。</li>
<li>利用新颖的，多尺度训练的方法，YOLOv2模型可以在多种尺度上运行，在速度与准确性上更容易去trade off。</li>
</ul>
</li>
<li>YOLO9000：<ul>
<li>是提出的一种联合在检测和分类数据集上训练的模型，<strong>这种联合训练的方法使得YOLO9000能够为没有标签的检测数据目标类预测</strong>。</li>
<li>可以检测超过9000个类。</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>目前，许多检测方法依旧约束在很小的物体集上。</li>
<li>目前，目标检测数据集相比于用于分类和标注的数据集来说，是有限制的。<ul>
<li>最常见的检测数据集包含数十到数十万的图像，具有几十到几百个标签，比如Pascal、CoCo、ImageNet。 </li>
<li>分类数据集具有数以百万计的图像，具有数万或数十万种类别，如ImageNet。</li>
</ul>
</li>
<li>目标检测数据集永远不会达到和分类数据集一样的等级。</li>
<li>本论文提出一种方法，利用分类数据集来作为检测数据集，将两种截然不同的数据集结合。</li>
<li>本论文提出一个在目标检测和分类数据集上联合训练的方法。<strong>此方法利用标记的检测图像学习精确定位对象，而它使用分类图像增加其词汇和鲁棒性。</strong></li>
</ul>
<h2 id="Better"><a href="#Better" class="headerlink" title="Better"></a>Better</h2><ul>
<li><p>YOLO产生很多的定位错误。而且YOLO相比于region proposal-based方法有着相对较低的recall（查全率）。<strong>所以主要任务是在保持分类准确率的前提下，提高recall和减少定位错误。</strong></p>
</li>
<li><p>我们从过去的工作中融合了我们自己的各种新想法，以提高YOLO的性能。 结果的摘要可以在表中找到：</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcjw1fcizngrvjoj30eq05paas.jpg" alt=""></p>
</li>
</ul>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a><strong>Batch Normalization</strong></h3><ul>
<li>得到2%的mAP的提升，使用Batch Normalization，我们可以从模型中删除dropout，而不会出现过度缺陷。</li>
</ul>
<h3 id="High-Resolution-Classiﬁer"><a href="#High-Resolution-Classiﬁer" class="headerlink" title="High Resolution Classiﬁer"></a><strong>High Resolution Classiﬁer</strong></h3><ul>
<li>对于YOLOv2，我们首先在ImageNet对全部448×448分辨率图像上进行10epochs的微调来调整分类网络。  然后我们在检测时调整resulting network。 这种高分辨率分类网络使我们增加了近4％的mAP。</li>
</ul>
<h3 id="Convolutional-With-Anchor-Boxes"><a href="#Convolutional-With-Anchor-Boxes" class="headerlink" title="Convolutional With Anchor Boxes"></a><strong>Convolutional With Anchor Boxes</strong></h3><ul>
<li>YOLO利用卷积特征提取器最顶端的全连接层来直接预测BB的坐标。而Faster R-CNN是利用首选的priors来预测BB。</li>
<li>预测BB的偏移而不是坐标可以简化问题，并使网络更容易学习。本论文从YOLO中移去了全连接层，并且利用anchor box来预测BB。</li>
<li>我们移去了pooling层，使得网络的卷积层的输出有更高的像素。</li>
<li>同时将网络缩减到在416*416像素的图片上操作。 我们这样做是因为我们想要特征图中具有奇数个位置，因此存在单个中心单元。</li>
<li>当我们移动到anchor boxes时，我们也将class prediction机制与空间位置解耦，而是为每个anchor box预测的类和对象。 同YOLO一样，objectness prediction仍然预测ground truth和所提出的框的IOU，并且class predictions预测该类的条件概率，假定存在对象。(没太懂)</li>
</ul>
<h3 id="Dimension-Clusters"><a href="#Dimension-Clusters" class="headerlink" title="Dimension Clusters"></a><strong>Dimension Clusters</strong></h3><ul>
<li><p>将YOLO与anchor boxes结合有两个问题，<strong>第一个是anchor box的长宽是认为选定的。</strong></p>
</li>
<li><p>我们不是手动选择先验（priors），而是在训练集边界框上运行k-means聚类，以自动找到好的先验。</p>
<ul>
<li><p>我们真正想要的是导致良好的IOU分数的priors，这是独立于盒子的大小。 因此，对于我们的distance metric，我们使用：</p>
<script type="math/tex; mode=display">
d(box, centroid) = 1-IOU(box,centroid)</script></li>
<li><p>我们选择<strong>k = 5</strong>作为模型复杂性和高召回率之间的良好权衡。这样非常不同于相比于人工选择的boxes。更多的又高又瘦的boxes。</p>
</li>
</ul>
</li>
</ul>
<h3 id="Direct-location-prediction"><a href="#Direct-location-prediction" class="headerlink" title="Direct location prediction"></a><strong>Direct location prediction</strong></h3><ul>
<li><p>将YOLO与anchor boxes结合有两个问题，<strong>第二个模型不稳定，特别是在早期迭代中。</strong></p>
</li>
<li><p>并非预测偏移，我们遵循YOLO的方法并预测相对于网格单元的位置的位置坐标。 这将ground truth限制在0和1之间。我们使用<strong>逻辑激活</strong>来约束网络的预测落在该范围内。</p>
</li>
<li><p>网络为每一个BB预测5个坐标：$t_x, t_y, t_w, t_h, t_o$.</p>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcjw1fcjb6g0tdpj309203r0st.jpg" alt="">  <img src="https://ww1.sinaimg.cn/large/006tKfTcjw1fcjbl2jwc6j30bj09k3z4.jpg" alt=""></p>
</li>
<li><p>结合Dimension Clusters和Direct location prediction，YOLO提升5%的mAP。</p>
</li>
</ul>
<h3 id="Fine-Grained-Features"><a href="#Fine-Grained-Features" class="headerlink" title="Fine-Grained Features"></a><strong>Fine-Grained Features</strong></h3><ul>
<li>修改后的YOLO在13<em>13的feature map上进行检测。 虽然这对于大对象是足够的，但是它可以从用于定位较小对象的<em>*细粒度特征</em></em>中受益。</li>
<li>添加一个传递层，将分辨率从前面的层变为从26 x 26分辨率。</li>
</ul>
<h3 id="Multi-Scale-Training"><a href="#Multi-Scale-Training" class="headerlink" title="Multi-Scale Training"></a><strong>Multi-Scale Training</strong></h3><ul>
<li>我们希望YOLOv2可以足够鲁邦在不同尺寸的images上进行训练。</li>
<li>并非使用固定的输入图像尺寸，我们在每几次迭代后改变网络。每10batches，我们的网络随机选择一个新的图像尺寸。</li>
</ul>
<h2 id="Faster"><a href="#Faster" class="headerlink" title="Faster"></a>Faster</h2><ul>
<li>大多数检测框架依赖VGG-16作为基本特征提取器。VGG-16是一个强大、准确的分类网络，但是也很复杂。</li>
<li>YOLO框架使用的基于Googlenet架构的修改后的网络。比VGG-16快速，但是准确性比VGG-16稍差。</li>
</ul>
<h3 id="Darknet-19"><a href="#Darknet-19" class="headerlink" title="Darknet-19"></a>Darknet-19</h3><ul>
<li><p>供YOLOv2使用的新的分类模型。</p>
</li>
<li><p>最终模型叫做darknet-19，有着19个卷积层和5个maxpooling层。</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcjw1fck21bicicj308j0byabg.jpg" alt=""></p>
</li>
<li><p>Darknet-19处理每张图片只需要5.58 billion的操作。</p>
</li>
</ul>
<h3 id="Training-for-classification"><a href="#Training-for-classification" class="headerlink" title="Training for classification"></a>Training for classification</h3><ul>
<li>在标准的ImageNet 1000类的数据集上利用随机梯度下降训练160 epochs。开始学习率为0.1，polynomial rate decay 是4，weight decay是0.0005，动量是0.9。</li>
<li>在训练期间，我们使用标准的数据增加技巧，包括随机裁剪，旋转，以及色调，饱和度和曝光偏移。</li>
<li>在224x224分辨率的图像上进行预训练，然后在448x448分辨率的图像上进行微调。</li>
</ul>
<h3 id="Training-for-detection"><a href="#Training-for-detection" class="headerlink" title="Training for detection"></a>Training for detection</h3><ul>
<li>我们通过去除最后的卷积层来修改这个网络，并且替代地增加具有1024个滤波器的三个3×3卷积层，每个跟随着具有我们需要检测所需的输出数量的最后的1×1卷积层。</li>
<li><strong>passthrough层</strong>的添加：使网络能够使用fine grain feature。</li>
</ul>
<h2 id="Stronger"><a href="#Stronger" class="headerlink" title="Stronger"></a>Stronger</h2><ul>
<li>本论文提出一种机制，用来将分类和检测数据结合起来再一起训练。<ul>
<li>在训练过程中，当看到用于检测的被标注的图片，我们会使用基于YOLOv2的代价函数进行反向传播。</li>
<li>在训练过程中，当看到分类图片，我们只从框架中用来分类部分来传递损失。</li>
</ul>
</li>
<li>这种方法的challenge：<ul>
<li>检测数据集中的标签是大分类，而分类数据集的标签是小分类，<strong>所以我们需要找一个方法来融合这些标签</strong>。</li>
<li>用来分类的许多方法都是使用softmax层来计算最后的概率分布，<strong>使用softmax层会假设类之间是互斥的</strong>，但是如何用本方法融合数据集，类之间本身不是互斥的。</li>
</ul>
</li>
<li>我们所以使用multi-label模型来结合数据集，不假设类之间互斥。<strong>这种方法忽略了我们已知的数据的结构。</strong></li>
</ul>
<h3 id="Hierarchical-classification"><a href="#Hierarchical-classification" class="headerlink" title="Hierarchical classification"></a>Hierarchical classification</h3><ul>
<li>ImageNet标签是从<strong>WordNet</strong>中得来，<strong>一种结构化概念和标签之间如何联系的语言数据库</strong>。</li>
<li>WordNet是<strong>连接图结构</strong>，而非树。我们相反并不实用整个图结构，我们将问题简化成从ImageNet的概念中构建有结构的树。</li>
<li>WordTree</li>
</ul>
<h3 id="Dataset-combination-with-WordTree"><a href="#Dataset-combination-with-WordTree" class="headerlink" title="Dataset combination with WordTree"></a>Dataset combination with WordTree</h3><ul>
<li><p>我们可以使用WordTree来介个数据集。</p>
</li>
<li><p>将数据集中分类映射成树中的下义词。</p>
</li>
<li><p>举例：将ImageNet和COCO数据集结合：</p>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcgy1fckj905lbrj30bm0co75r.jpg" alt=""></p>
</li>
<li><p>WordNet十分多样化，所以我们可以利用这种技术到大多数数据集。</p>
</li>
</ul>
<h3 id="Joint-classiﬁcation-and-detection"><a href="#Joint-classiﬁcation-and-detection" class="headerlink" title="Joint classiﬁcation and detection"></a>Joint classiﬁcation and detection</h3><ul>
<li>将COCO数据集和ImageNet数据集结合，训练处一个特别大规模的检测器。</li>
<li>对应的WordTree有9418个类。</li>
<li>ImageNet是一个更大的数据集，因此我们通过对COCO进行过采样来平衡数据集，使ImageNet只以4：1的倍数来增大。</li>
<li>当我们的网络看见一张用来检测的图片，我们正常反向传播loss。对于分类loss，我们只在该label对应层次之上反向传播loss。<strong>比如：如果标签是“dog”，我们会在树中的“German Shepherd”和“Golden Retriever”中进一步预测错误，因为我们没有这些信息。</strong></li>
<li>当我们的网络看见一张用来分来的照片，我们只反向传递分类loss。</li>
<li>使用这种联合训练，YOLO 9000使用COCO中的检测数据学习找到图像中的对象，并使用ImageNet中的数据学习分类各种各样的对象。</li>
<li>在ImageNet上利用YOLO9000来做detection，从而进行评估。ImageNet和COCO只有44个相同的类分类，意味着YOLO9000在利用部分监督来进行检测。</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>本论文提出两个模型，<strong>YOLOv2和YOLO9000</strong>。<ul>
<li>YOLOv2：是对YOLO改进后的提升模型。更快更先进。此外，它可以以各种图像大小运行，以提供速度和精度之间的权衡。</li>
<li>YOLO9000：是提出的一种联合在检测和分类数据集上训练的模型，可以为没有任何标注检测标签的数据进行检测。可以检测超过9000个类。使用WordTree技术来组合不同来源的数据。</li>
</ul>
</li>
<li>我们创造出许多目标检测之外的技术：<ul>
<li>WordTree representation.</li>
<li>Dataset combination.</li>
<li>Multi-scale training.</li>
</ul>
</li>
<li>下一步工作：我们希望利用相似的技术来进行weakly supervised image segmentation.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;YOLO9000: a state-of-the-art, real-time 的目标检测系统，可以检测超过9000种的物体分类。&lt;/li&gt;
&lt;li&gt;本论文提出两个模型，&lt;strong&gt;YOLOv2和YOLO9000&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;YOLOv2：&lt;ul&gt;
&lt;li&gt;是对YOLO改进后的提升模型。&lt;/li&gt;
&lt;li&gt;利用新颖的，多尺度训练的方法，YOLOv2模型可以在多种尺度上运行，在速度与准确性上更容易去trade off。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;YOLO9000：&lt;ul&gt;
&lt;li&gt;是提出的一种联合在检测和分类数据集上训练的模型，&lt;strong&gt;这种联合训练的方法使得YOLO9000能够为没有标签的检测数据目标类预测&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;可以检测超过9000个类。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：YOLO</title>
    <link href="http://jacobkong.github.io/posts/2094641206/"/>
    <id>http://jacobkong.github.io/posts/2094641206/</id>
    <published>2017-02-02T11:49:53.000Z</published>
    <updated>2017-02-08T03:57:58.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>之前的物体检测的方法是使用分类器来进行检测。</li>
<li>相反，本论文将对象检测作为空间分离的边界框和相关类概率的回归问题。</li>
<li>本论文的YOLO模型能达到45fps的实时图像处理效果。</li>
<li>Fast YOLO：小型的网络版本，可达到155fps。</li>
<li>与目前的检测系统相比，YOLO会产生更多的定位错误，但是会更少的去在背景中产生false positive。</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><p>DPM: use a sliding window approach where the classiﬁer is run at evenly spaced locations over the entire image.</p>
</li>
<li><p>R-CNN: use region proposal methods to ﬁrst generate potential bounding boxes in an image and then run a classiﬁer on these proposed boxes. 具有<strong>slow</strong>和<strong>hard to optimize</strong>的缺点。</p>
</li>
<li><p>本论文将目标检测问题重新组织成<strong>single regression problem</strong>. 从图像像素转为<strong>bounding box coordinates</strong>和<strong>class probabilities</strong>.</p>
</li>
<li><p>YOLO框架：</p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgy1fcfslf8g7bj30bs06b756.jpg" alt=""></p>
<ul>
<li>A <strong>single convolutional network</strong> simultaneously predicts multiple bounding boxes and class probabilities for those boxes.</li>
<li>YOLO trains on full images and directly optimizes detection performance.</li>
</ul>
</li>
<li><p>YOLO模型的优势：</p>
<ul>
<li>First, YOLO is extremely <strong>fast</strong>.<ul>
<li>regression problem.</li>
<li>no batch processing on a Titan X.</li>
</ul>
</li>
<li>Second, YOLO reasons globally about the image when making predictions.<ul>
<li>YOLO makes <strong>less than half</strong> the number of background errors compared to Fast R-CNN.</li>
</ul>
</li>
<li>Third, YOLO learns <strong>generalizable representations</strong> of objects.</li>
</ul>
</li>
<li><p>YOLO在准确性方面依旧落后与其他先进的检测系统，但是可以快速的标注图片中的物体，特别是小物体。</p>
</li>
</ul>
<h2 id="Unified-Detection"><a href="#Unified-Detection" class="headerlink" title="Unified Detection"></a>Unified Detection</h2><ul>
<li><p>本论文将物体检测中单独的组件统一到一个单一的神经网络中。网络利用整个图像的各个特征来预测每一个BB。而且同时为一张图片中所有的类预测所用的BB。</p>
</li>
<li><p>YOLO可以<strong>end-to-end</strong>来训练，而且能在保持高平均准确率的同时达到<strong>实时要求</strong>。</p>
</li>
<li><p>系统将输入图片分为$S*S$的网格单元。如果物体的中心落入某个格子，那么这个格子将会用来检测这个物体。</p>
</li>
<li><p>每个网格单元会预测<strong>B</strong>个bounding box以及这些框的置信值。</p>
</li>
<li><p>每个bounding box会有5个预测值：$x,y,w,h$和置信值confidence，$confidence = Pr(Object)*IOU^{truth}_{pred}$.</p>
</li>
<li><p>每个网格单元也预测<strong>C</strong>个条件类概率，$Pr(Class_i|Object)$，<strong>在一个网格单元包含一个物体的前提下，它属于某个类的概率</strong>。我们只为每个网格单元预测一组类概率，而不考虑框B的数量。</p>
</li>
<li><p>在测试的时候，通过如下公式来给出对某一个box来说某一类的confidence score：</p>
<script type="math/tex; mode=display">
Pr(Class_{i}|Object)*Pr(Object)*IOU^{truth}_{pred}=Pr(Class_{i})*IOU^{truth}_{pred}</script></li>
<li><p>Model示例：</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcjw1fcgkqg2fdyj309608lgmg.jpg" alt=""></p>
<p>每个grid cell预测B个bounding boxes，每个框的confidence和C个类概率。</p>
</li>
</ul>
<h3 id="Network-Design"><a href="#Network-Design" class="headerlink" title="Network Design"></a>Network Design</h3><ul>
<li><p>YOLO网络结构图：</p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgy1fcgkv58b9lj30oc0aota4.jpg" alt=""></p>
</li>
<li><p>起初的<strong>卷积层</strong>用来从图像中提取特征。</p>
</li>
<li><p>全连接层用来预测输出的概率和坐标。</p>
</li>
<li><p>24个卷积层，之后跟着2个全连接层</p>
</li>
<li><p>最终输出是7 x 7 x 30的张量。</p>
</li>
<li><p>Fast YOLO和YOLO之间所有的训练和测试参数一样。</p>
</li>
<li><p>在ImageNet上进行卷积层的预训练。</p>
</li>
</ul>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ul>
<li><p>在ImageNet上预训练卷积层。预训练前20层卷积层，之后跟随者一个average-pooling layer和一个fully connected layer.</p>
</li>
<li><p>将预训练的模型用来检测，<strong>论文Ren et al.显示给与训练好的模型添加卷积和连接层能够提高性能。</strong>所以添加了<strong>额外的4个卷积层和2个全连接层</strong>，其权值随机初始化。</p>
</li>
<li><p>将像素从224x224提升到448x448。</p>
</li>
<li><p>最后一层同时预测class probabilities和bounding box coordinates. 其中涉及到BB的长宽规范化。</p>
</li>
<li><p>由于sum-squared error的缺点，增加边界框坐标预测的损失，并减少对不包含对象的框的置信度预测的损失。</p>
</li>
<li><p>large boxes中的偏差matter less than 与small boxes中的偏差。</p>
</li>
<li><p>YOLO为每一个网格单元预测多个BB，但是在测试期间，我们只想每一个物体有一个BB预测框来做响应，我们选择具有最高IOU的BB来作为响应框。</p>
</li>
<li><p>总的<strong>loss function</strong>:</p>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcgy1fch1eth6atj30do08hdgg.jpg" alt=""></p>
</li>
<li><p>135 epochs</p>
</li>
<li><p>batch size：64</p>
</li>
<li><p>动量：0.9</p>
</li>
<li><p>decay：0.0005</p>
</li>
<li><p>为防止过拟合，我们使用dropout和extensive data augmentation技术。</p>
</li>
</ul>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul>
<li>在测试图像中预测检测只需要一个网络评估，与一般的classifier-based methods不同。</li>
<li>Non-maximal suppression可以用来修复multiple detections。</li>
</ul>
<h2 id="Comparison-to-Other-Detection-Systems"><a href="#Comparison-to-Other-Detection-Systems" class="headerlink" title="Comparison to Other Detection Systems"></a>Comparison to Other Detection Systems</h2><ul>
<li>检测流水线往往开始于提取健壮特征集（Haar, SIFT, HOG, convolutional features）,然后分类器或者定位器用来识别特征空间的物体，这些分类器或者定位器往往在整个图像上或者在图像的子区域中滑动窗口。</li>
<li>与DPM的比较。</li>
<li>与R-CNN的比较。每个图片值预测98个bounding boxes。</li>
<li>与其他快速检测器的比较。相比于单类检测器，YOLO可以同时检测多种物体。</li>
<li>与Deep MultiBox的比较。YOLO是一个完整的检测系统。</li>
<li>与OverFeat的比较。OverFeat是一个disjoint的系统，OverFeat优化定位，而非检测性能。需要大量的后处理。</li>
<li>与MultiGrasp的比较。执行比目标检测更简单的任务。</li>
</ul>
<h3 id=""><a href="#" class="headerlink" title=" "></a> </h3>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;之前的物体检测的方法是使用分类器来进行检测。&lt;/li&gt;
&lt;li&gt;相反，本论文将对象检测作为空间分离的边界框和相关类概率的回归问题。&lt;/li&gt;
&lt;li&gt;本论文的YOLO模型能达到45fps的实时图像处理效果。&lt;/li&gt;
&lt;li&gt;Fast YOLO：小型的网络版本，可达到155fps。&lt;/li&gt;
&lt;li&gt;与目前的检测系统相比，YOLO会产生更多的定位错误，但是会更少的去在背景中产生false positive。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习实践经验：用Faster R-CNN训练Caltech数据集——训练检测</title>
    <link href="http://jacobkong.github.io/posts/464905881/"/>
    <id>http://jacobkong.github.io/posts/464905881/</id>
    <published>2017-01-16T22:32:24.000Z</published>
    <updated>2017-02-25T16:22:18.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前面已经介绍了如何准备数据集，以及如何修改数据集读写接口来操作数据集，接下来我来说明一下怎么来训练网络和之后的检测过程。</p>
<a id="more"></a>
<h2 id="修改模型文件"><a href="#修改模型文件" class="headerlink" title="修改模型文件"></a>修改模型文件</h2><p>faster rcnn有两种各种训练方式:</p>
<ul>
<li>Alternative training(alt-opt)</li>
<li>Approximate joint training(end-to-end)</li>
</ul>
<p>两种方法有什么不同，可以参考我<a href="http://jacobkong.github.io/posts/3802700508/">这篇博客</a>，推荐使用第二种，因为第二种使用的显存更小，而且训练会更快，同时准确率差不多，两种方式需要修改的代码是不一样的，同时faster rcnn提供了三种训练模型，小型的ZF model，中型的VGG_CNN_M_1024和大型的VGG16,论文中说VGG16效果比其他两个好，但是同时占用更大的GPU显存(~11GB)</p>
<p>我使用的是<strong>VGG model + alternative training</strong>，需要检测的类别只有一类，加上背景所以总共是两类(background + person)。</p>
<p>下面修改模型文件：</p>
<ol>
<li><p>py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/stage1_fast_rcnn_train.pt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &apos;data&apos;  </div><div class="line">  type: &apos;Python&apos;  </div><div class="line">  top: &apos;data&apos;  </div><div class="line">  top: &apos;rois&apos;  </div><div class="line">  top: &apos;labels&apos;  </div><div class="line">  top: &apos;bbox_targets&apos;  </div><div class="line">  top: &apos;bbox_inside_weights&apos;  </div><div class="line">  top: &apos;bbox_outside_weights&apos;  </div><div class="line">  python_param &#123;  </div><div class="line">    module: &apos;roi_data_layer.layer&apos;  </div><div class="line">    layer: &apos;RoIDataLayer&apos;  </div><div class="line">    param_str: &quot;&apos;num_classes&apos;: 2&quot; #按训练集类别改，该值为类别数+1  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &quot;cls_score&quot;  </div><div class="line">  type: &quot;InnerProduct&quot;  </div><div class="line">  bottom: &quot;fc7&quot;  </div><div class="line">  top: &quot;cls_score&quot;  </div><div class="line">  param &#123; </div><div class="line">  lr_mult: 1.0</div><div class="line">  &#125;  </div><div class="line">  param &#123;</div><div class="line">  lr_mult: 2.0 </div><div class="line">  &#125;  </div><div class="line">  inner_product_param &#123;  </div><div class="line">    num_output: 2 #按训练集类别改，该值为类别数+1  </div><div class="line">    weight_filler &#123;  </div><div class="line">      type: &quot;gaussian&quot;  </div><div class="line">      std: 0.01  </div><div class="line">    &#125;  </div><div class="line">    bias_filler &#123;  </div><div class="line">      type: &quot;constant&quot;  </div><div class="line">      value: 0  </div><div class="line">    &#125;  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &quot;bbox_pred&quot;  </div><div class="line">  type: &quot;InnerProduct&quot;  </div><div class="line">  bottom: &quot;fc7&quot;  </div><div class="line">  top: &quot;bbox_pred&quot;  </div><div class="line">  param &#123; </div><div class="line">  lr_mult: 1.0 </div><div class="line">  &#125;  </div><div class="line">  param &#123; </div><div class="line">  lr_mult: 2.0 </div><div class="line">  &#125;  </div><div class="line">  inner_product_param &#123;  </div><div class="line">    num_output: 8 #按训练集类别改，该值为（类别数+1）*4，四个顶点坐标  </div><div class="line">    weight_filler &#123;  </div><div class="line">      type: &quot;gaussian&quot;  </div><div class="line">      std: 0.001  </div><div class="line">    &#125;  </div><div class="line">    bias_filler &#123;  </div><div class="line">      type: &quot;constant&quot;  </div><div class="line">      value: 0  </div><div class="line">    &#125;  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/stage1_rpn_train.pt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &apos;input-data&apos;  </div><div class="line">  type: &apos;Python&apos;  </div><div class="line">  top: &apos;data&apos;  </div><div class="line">  top: &apos;im_info&apos;  </div><div class="line">  top: &apos;gt_boxes&apos;  </div><div class="line">  python_param &#123;  </div><div class="line">    module: &apos;roi_data_layer.layer&apos;  </div><div class="line">    layer: &apos;RoIDataLayer&apos;  </div><div class="line">    param_str: &quot;&apos;num_classes&apos;: 2&quot; #按训练集类别改，该值为类别数+1  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/stage2_fast_rcnn_train.pt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &apos;data&apos;  </div><div class="line">  type: &apos;Python&apos;  </div><div class="line">  top: &apos;data&apos;  </div><div class="line">  top: &apos;rois&apos;  </div><div class="line">  top: &apos;labels&apos;  </div><div class="line">  top: &apos;bbox_targets&apos;  </div><div class="line">  top: &apos;bbox_inside_weights&apos;  </div><div class="line">  top: &apos;bbox_outside_weights&apos;  </div><div class="line">  python_param &#123;  </div><div class="line">    module: &apos;roi_data_layer.layer&apos;  </div><div class="line">    layer: &apos;RoIDataLayer&apos;  </div><div class="line">    param_str: &quot;&apos;num_classes&apos;: 2&quot; #按训练集类别改，该值为类别数+1  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &quot;cls_score&quot;  </div><div class="line">  type: &quot;InnerProduct&quot;  </div><div class="line">  bottom: &quot;fc7&quot;  </div><div class="line">  top: &quot;cls_score&quot;  </div><div class="line">  param &#123; </div><div class="line">  lr_mult: 1.0 </div><div class="line">  &#125;  </div><div class="line">  param &#123; </div><div class="line">  lr_mult: 2.0 </div><div class="line">  &#125;  </div><div class="line">  inner_product_param &#123;  </div><div class="line">    num_output: 2 #按训练集类别改，该值为类别数+1  </div><div class="line">    weight_filler &#123;  </div><div class="line">      type: &quot;gaussian&quot;  </div><div class="line">      std: 0.01  </div><div class="line">    &#125;  </div><div class="line">    bias_filler &#123;  </div><div class="line">      type: &quot;constant&quot;  </div><div class="line">      value: 0  </div><div class="line">    &#125;  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &quot;bbox_pred&quot;  </div><div class="line">  type: &quot;InnerProduct&quot;  </div><div class="line">  bottom: &quot;fc7&quot;  </div><div class="line">  top: &quot;bbox_pred&quot;  </div><div class="line">  param &#123; </div><div class="line">  lr_mult: 1.0</div><div class="line">  &#125;  </div><div class="line">  param &#123; </div><div class="line">  lr_mult: 2.0 </div><div class="line">  &#125;  </div><div class="line">  inner_product_param &#123;  </div><div class="line">    num_output: 8 #按训练集类别改，该值为（类别数+1）*4,四个顶点坐标  </div><div class="line">    weight_filler &#123;  </div><div class="line">      type: &quot;gaussian&quot;  </div><div class="line">      std: 0.001  </div><div class="line">    &#125;  </div><div class="line">    bias_filler &#123;  </div><div class="line">      type: &quot;constant&quot;  </div><div class="line">      value: 0  </div><div class="line">    &#125;  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/stage2_rpn_train.pt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &apos;input-data&apos;  </div><div class="line">  type: &apos;Python&apos;  </div><div class="line">  top: &apos;data&apos;  </div><div class="line">  top: &apos;im_info&apos;  </div><div class="line">  top: &apos;gt_boxes&apos;  </div><div class="line">  python_param &#123;  </div><div class="line">    module: &apos;roi_data_layer.layer&apos;  </div><div class="line">    layer: &apos;RoIDataLayer&apos;  </div><div class="line">    param_str: &quot;&apos;num_classes&apos;: 2&quot; #按训练集类别改，该值为类别数+1  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
<li><p>py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">layer &#123;  </div><div class="line">  name: &quot;cls_score&quot;  </div><div class="line">  type: &quot;InnerProduct&quot;  </div><div class="line">  bottom: &quot;fc7&quot;  </div><div class="line">  top: &quot;cls_score&quot;  </div><div class="line">  inner_product_param &#123;  </div><div class="line">    num_output: 2 #按训练集类别改，该值为类别数+1  </div><div class="line">  &#125;  </div><div class="line">&#125;  </div><div class="line"></div><div class="line">layer &#123;  </div><div class="line">  name: &quot;bbox_pred&quot;  </div><div class="line">  type: &quot;InnerProduct&quot;  </div><div class="line">  bottom: &quot;fc7&quot;  </div><div class="line">  top: &quot;bbox_pred&quot;  </div><div class="line">  inner_product_param &#123;  </div><div class="line">    num_output: 84 #按训练集类别改，该值为（类别数+1）*4,四个顶点坐标</div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="训练测试"><a href="#训练测试" class="headerlink" title="训练测试"></a>训练测试</h2><p>训练前还需要注意几个地方：</p>
<ol>
<li><p>cache问题：</p>
<p>假如你之前训练了官方的VOC2007的数据集或其他的数据集，是会产生cache的问题的，建议在重新训练新的数据之前将其删除。</p>
<ul>
<li><code>py-faster-rcnn/output</code></li>
<li><code>py-faster-rcnn/data/cache</code></li>
</ul>
</li>
<li><p>训练参数</p>
<p>参数放在如下文件:</p>
<p><code>py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/stage_fast_rcnn_solver*.pt</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">base_lr: 0.001</div><div class="line">lr_policy: &apos;step&apos;</div><div class="line">step_size: 30000</div><div class="line">display: 20</div><div class="line">....</div></pre></td></tr></table></figure>
<p>迭代次数在文件py-faster-rcnn/tools/train_faster_rcnn_alt_opt.py中进行修改:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">max_iters = [80000, 40000, 80000, 40000]</div></pre></td></tr></table></figure>
<p>分别对应rpn第1阶段，fast rcnn第1阶段，rpn第2阶段，fast rcnn第2阶段的迭代次数，自己修改即可，不过注意这里的值不要小于上面的solver里面的step_size的大小，大家自己修改吧</p>
</li>
</ol>
<h3 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h3><p>首先修改<code>experiments/scripts/faster_rcnn_alt_opt.sh</code>成如下，修改地方已标注：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"><span class="comment"># Usage:</span></div><div class="line"><span class="comment"># ./experiments/scripts/faster_rcnn_alt_opt.sh GPU NET DATASET [options args to &#123;train,test&#125;_net.py]</span></div><div class="line"><span class="comment"># DATASET is only pascal_voc for now</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Example:</span></div><div class="line"><span class="comment"># ./experiments/scripts/faster_rcnn_alt_opt.sh 0 VGG_CNN_M_1024 pascal_voc \</span></div><div class="line"><span class="comment">#   --set EXP_DIR foobar RNG_SEED 42 TRAIN.SCALES "[400, 500, 600, 700]"</span></div><div class="line"></div><div class="line"><span class="built_in">set</span> -x</div><div class="line"><span class="built_in">set</span> <span class="_">-e</span></div><div class="line"></div><div class="line"><span class="built_in">export</span> PYTHONUNBUFFERED=<span class="string">"True"</span></div><div class="line"></div><div class="line">GPU_ID=<span class="variable">$1</span></div><div class="line">NET=<span class="variable">$2</span></div><div class="line">NET_lc=<span class="variable">$&#123;NET,,&#125;</span></div><div class="line">DATASET=<span class="variable">$3</span></div><div class="line"></div><div class="line">array=( <span class="variable">$@</span> )</div><div class="line">len=<span class="variable">$&#123;#array[@]&#125;</span></div><div class="line">EXTRA_ARGS=<span class="variable">$&#123;array[@]:3:$len&#125;</span></div><div class="line">EXTRA_ARGS_SLUG=<span class="variable">$&#123;EXTRA_ARGS// /_&#125;</span></div><div class="line"></div><div class="line"><span class="keyword">case</span> <span class="variable">$DATASET</span> <span class="keyword">in</span></div><div class="line">  caltech)                     <span class="comment"># 这里将pascal_voc改为caltech</span></div><div class="line">    TRAIN_IMDB=<span class="string">"caltech_train"</span> <span class="comment"># 改为与factor.py中命名的name格式相同，为caltech_train</span></div><div class="line">    TEST_IMDB=<span class="string">"caltech_test"</span>   <span class="comment"># 改为与factor.py中命名的name格式相同，为caltech_test</span></div><div class="line">    PT_DIR=<span class="string">"caltech"</span>           <span class="comment"># 这里将pascal_voc改为caltech</span></div><div class="line">    ITERS=40000</div><div class="line">    ;;</div><div class="line">  coco)</div><div class="line">    <span class="built_in">echo</span> <span class="string">"Not implemented: use experiments/scripts/faster_rcnn_end2end.sh for coco"</span></div><div class="line">    <span class="built_in">exit</span></div><div class="line">    ;;</div><div class="line">  *)</div><div class="line">    <span class="built_in">echo</span> <span class="string">"No dataset given"</span></div><div class="line">    <span class="built_in">exit</span></div><div class="line">    ;;</div><div class="line"><span class="keyword">esac</span></div><div class="line"></div><div class="line">LOG=<span class="string">"experiments/logs/faster_rcnn_alt_opt_<span class="variable">$&#123;NET&#125;</span>_<span class="variable">$&#123;EXTRA_ARGS_SLUG&#125;</span>.txt.`date +'%Y-%m-%d_%H-%M-%S'`"</span></div><div class="line"><span class="built_in">exec</span> &amp;&gt; &gt;(tee <span class="_">-a</span> <span class="string">"<span class="variable">$LOG</span>"</span>)</div><div class="line"><span class="built_in">echo</span> Logging output to <span class="string">"<span class="variable">$LOG</span>"</span></div><div class="line"></div><div class="line">time ./tools/train_faster_rcnn_alt_opt.py --gpu <span class="variable">$&#123;GPU_ID&#125;</span> \</div><div class="line">  --net_name <span class="variable">$&#123;NET&#125;</span> \</div><div class="line">  --weights data/imagenet_models/<span class="variable">$&#123;NET&#125;</span>.v2.caffemodel \</div><div class="line">  --imdb <span class="variable">$&#123;TRAIN_IMDB&#125;</span> \</div><div class="line">  --cfg experiments/cfgs/faster_rcnn_alt_opt.yml \</div><div class="line">  <span class="variable">$&#123;EXTRA_ARGS&#125;</span></div><div class="line"></div><div class="line"><span class="built_in">set</span> +x</div><div class="line">NET_FINAL=`grep <span class="string">"Final model:"</span> <span class="variable">$&#123;LOG&#125;</span> | awk <span class="string">'&#123;print $3&#125;'</span>`</div><div class="line"><span class="built_in">set</span> -x</div><div class="line"></div><div class="line">time ./tools/test_net.py --gpu <span class="variable">$&#123;GPU_ID&#125;</span> \</div><div class="line">  --def models/<span class="variable">$&#123;PT_DIR&#125;</span>/<span class="variable">$&#123;NET&#125;</span>/faster_rcnn_alt_opt/faster_rcnn_test.pt \</div><div class="line">  --net <span class="variable">$&#123;NET_FINAL&#125;</span> \</div><div class="line">  <span class="comment">#--net output/faster_rcnn_alt_opt/train/ZF_faster_rcnn_final.caffemodel \</span></div><div class="line">  --imdb <span class="variable">$&#123;TEST_IMDB&#125;</span> \</div><div class="line">  --cfg experiments/cfgs/faster_rcnn_alt_opt.yml \</div><div class="line">  <span class="variable">$&#123;EXTRA_ARGS&#125;</span></div></pre></td></tr></table></figure>
<p>调用如下命令进行训练及测试，从上面代码可以看出，该shell文件在训练完后会接着进行测试，但是我的测试集没有标注，所以测试的时候会报错，但是由于Caltech数据集的测试结果有专门的评估代码，所以我不用faster r-cnn提供的代码进行测试，而是直接进行检测生成坐标，用专门的评估代码进行检测。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> py-faster-rcnn</div><div class="line">./experiments/scripts/faster_rcnn_alt_opt.sh 0 VGG16 caltech</div></pre></td></tr></table></figure>
<ul>
<li>参数1：指定gpu_id。</li>
<li>参数2：指定网络模型参数。</li>
<li>参数3：数据集名称，目前只能为<code>pascal_voc</code>。</li>
</ul>
<p>在训练过程中，会调用<code>py_faster_rcnn/tools/train_faster_rcnn_alt_opt.py</code>文件开始训练网络。</p>
<h3 id="可能会出现的Bugs"><a href="#可能会出现的Bugs" class="headerlink" title="可能会出现的Bugs"></a>可能会出现的Bugs</h3><h4 id="AssertionError-assert-boxes-2-gt-boxes-0-all"><a href="#AssertionError-assert-boxes-2-gt-boxes-0-all" class="headerlink" title="AssertionError: assert (boxes[:, 2] &gt;= boxes[:, 0]).all()"></a>AssertionError: assert (boxes[:, 2] &gt;= boxes[:, 0]).all()</h4><h5 id="问题重现"><a href="#问题重现" class="headerlink" title="问题重现"></a>问题重现</h5><p>在训练过程中可能会出现如下报错：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">File &quot;/py-faster-rcnn/tools/../lib/datasets/imdb.py&quot;, line 108, in </div><div class="line">append_flipped_images </div><div class="line">	assert (boxes[:, 2] &gt;= boxes[:, 0]).all() </div><div class="line">AssertionError</div></pre></td></tr></table></figure>
<h5 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h5><p>检查自己数据发现，左上角坐标 (x, y) 可能为0，或标定区域溢出图片（即坐标为负数），而faster rcnn会对Xmin,Ymin,Xmax,Ymax进行减一操作，如果Xmin为0，减一后变为65535，从而在左右翻转图片时导致如上错误发生。</p>
<h5 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h5><ol>
<li><p>修改<code>lib/datasets/imdb.py</code>中的<code>append_flipped_images()</code>函数：</p>
<p>数据整理，在一行代码为 <code>boxes[:, 2] = widths[i] - oldx1 - 1</code>下加入代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> b <span class="keyword">in</span> range(len(boxes)):</div><div class="line">	<span class="keyword">if</span> boxes[b][<span class="number">2</span>]&lt; boxes[b][<span class="number">0</span>]:</div><div class="line">		boxes[b][<span class="number">0</span>] = <span class="number">0</span></div></pre></td></tr></table></figure>
</li>
<li><p>修改<code>lib/datasets/caltech.py</code>，<code>_load_pascal_annotation()</code>函数，将对Xmin,Ymin,Xmax,Ymax减一去掉，变为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Load object bounding boxes into a data frame.</span></div><div class="line">        <span class="keyword">for</span> ix, obj <span class="keyword">in</span> enumerate(objs):</div><div class="line">            bbox = obj.find(<span class="string">'bndbox'</span>)</div><div class="line">            <span class="comment"># Make pixel indexes 0-based</span></div><div class="line">            <span class="comment"># 这里我把‘-1’全部删除掉了，防止有的数据是0开始，然后‘-1’导致变为负数，产生AssertError错误</span></div><div class="line">            x1 = float(bbox.find(<span class="string">'xmin'</span>).text)</div><div class="line">            y1 = float(bbox.find(<span class="string">'ymin'</span>).text)</div><div class="line">            x2 = float(bbox.find(<span class="string">'xmax'</span>).text)</div><div class="line">            y2 = float(bbox.find(<span class="string">'ymax'</span>).text)</div><div class="line">            cls = self._class_to_ind[obj.find(<span class="string">'name'</span>).text.lower().strip()]</div><div class="line">            boxes[ix, :] = [x1, y1, x2, y2]</div><div class="line">            gt_classes[ix] = cls</div><div class="line">            overlaps[ix, cls] = <span class="number">1.0</span></div><div class="line">            seg_areas[ix] = (x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>)</div></pre></td></tr></table></figure>
</li>
<li><p>（可选）如果1和2可以解决问题，就没必要用方法3。修改<code>lib/fast_rcnn/config.py</code>，不使图片实现翻转，如下改为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># Use horizontally-flipped images during training? </div><div class="line">__C.TRAIN.USE_FLIPPED = False</div></pre></td></tr></table></figure>
</li>
</ol>
<p>如果如上三种方法都无法解决该问题，那么肯定是你的数据集坐标出现小于等于0的数，你<strong>应该一一排查</strong>。</p>
<h4 id="训练fast-rcnn时出现loss-nan的情况。"><a href="#训练fast-rcnn时出现loss-nan的情况。" class="headerlink" title="训练fast rcnn时出现loss=nan的情况。"></a>训练fast rcnn时出现loss=nan的情况。</h4><h5 id="问题重现-1"><a href="#问题重现-1" class="headerlink" title="问题重现"></a>问题重现</h5><p><img src="https://ww1.sinaimg.cn/large/006tNbRwly1fcuq2kgkwgj30v10hddsn.jpg" alt=""></p>
<h5 id="问题分析-1"><a href="#问题分析-1" class="headerlink" title="问题分析"></a>问题分析</h5><p>这是由于模型不收敛，导致loss迅速增长。</p>
<p>而我出现以上现象的原因主要是因为我在出现AssertionError的时候直接使用了第三种方法导致的。也就是禁用图片翻转。</p>
<h5 id="问题解决-1"><a href="#问题解决-1" class="headerlink" title="问题解决"></a>问题解决</h5><p>启用图片翻转。</p>
<h3 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h3><p>训练后的模型放在<code>output/faster_rcnn_alt_opt/train/VGG16_faster_rcnn_final.caffemodel</code>，该模型可以用于之后的检测。</p>
<h2 id="检测"><a href="#检测" class="headerlink" title="检测"></a>检测</h2><h3 id="检测步骤"><a href="#检测步骤" class="headerlink" title="检测步骤"></a>检测步骤</h3><p>经过以上训练后，就可以用得到的模型来进行检测了。检测所参考的代码是<code>tools/demo.py</code>，具体步骤如下：</p>
<ol>
<li>将<code>output/faster_rcnn_alt_opt/train/VGG16_faster_rcnn_final.caffemodel</code>，拷贝到<code>data/faster_rcnn_models</code>下，命名为<code>VGG16_Caltech_faster_rcnn__final.caffemodel</code></li>
<li>进入<code>tools/</code>文件夹中，拷贝<code>demo.py</code>为<code>demo_caltech.py</code>。</li>
<li>修改demo_caltech.py代码如下：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"></div><div class="line"><span class="comment"># --------------------------------------------------------</span></div><div class="line"><span class="comment"># Faster R-CNN</span></div><div class="line"><span class="comment"># Copyright (c) 2015 Microsoft</span></div><div class="line"><span class="comment"># Licensed under The MIT License [see LICENSE for details]</span></div><div class="line"><span class="comment"># Written by Ross Girshick</span></div><div class="line"><span class="comment"># --------------------------------------------------------</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> matplotlib</div><div class="line">matplotlib.use(<span class="string">'Agg'</span>);</div><div class="line"><span class="string">"""</span></div><div class="line">Demo script showing detections in sample images.</div><div class="line"></div><div class="line">See README.md for installation instructions before running.</div><div class="line">"""</div><div class="line"></div><div class="line"><span class="keyword">import</span> _init_paths</div><div class="line"><span class="keyword">from</span> fast_rcnn.config <span class="keyword">import</span> cfg</div><div class="line"><span class="keyword">from</span> fast_rcnn.test <span class="keyword">import</span> im_detect</div><div class="line"><span class="keyword">from</span> fast_rcnn.nms_wrapper <span class="keyword">import</span> nms</div><div class="line"><span class="keyword">from</span> utils.timer <span class="keyword">import</span> Timer</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</div><div class="line"><span class="keyword">import</span> caffe, os, sys, cv2</div><div class="line"><span class="keyword">import</span> argparse</div><div class="line"></div><div class="line">CLASSES = (<span class="string">'__background__'</span>, <span class="comment"># 这里改为自己的类别</span></div><div class="line">           <span class="string">'person'</span>)</div><div class="line"></div><div class="line">NETS = &#123;<span class="string">'vgg16'</span>: (<span class="string">'VGG16'</span>,</div><div class="line">                  <span class="string">'VGG16_Caltech_faster_rcnn_final.caffemodel'</span>), <span class="comment">#这里需要修改为训练后得到的模型的名称</span></div><div class="line">        <span class="string">'zf'</span>: (<span class="string">'ZF'</span>,</div><div class="line">                  <span class="string">'ZF_Caltech_faster_rcnn_final.caffemodel'</span>)&#125; <span class="comment">#这里需要修改为训练后得到的模型的名称</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">vis_detections</span><span class="params">(im, image_name, class_name, dets, thresh=<span class="number">0.5</span>)</span>:</span></div><div class="line">    <span class="string">"""Draw detected bounding boxes."""</span></div><div class="line">    inds = np.where(dets[:, <span class="number">-1</span>] &gt;= thresh)[<span class="number">0</span>]</div><div class="line">    <span class="keyword">if</span> len(inds) == <span class="number">0</span>:</div><div class="line">        <span class="keyword">return</span></div><div class="line"></div><div class="line">    im = im[:, :, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>)]</div><div class="line">    fig, ax = plt.subplots(figsize=(<span class="number">12</span>, <span class="number">12</span>))</div><div class="line">    ax.imshow(im, aspect=<span class="string">'equal'</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> inds:</div><div class="line">        bbox = dets[i, :<span class="number">4</span>]</div><div class="line">        score = dets[i, <span class="number">-1</span>]</div><div class="line"></div><div class="line">        ax.add_patch(</div><div class="line">            plt.Rectangle((bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]),</div><div class="line">                          bbox[<span class="number">2</span>] - bbox[<span class="number">0</span>],</div><div class="line">                          bbox[<span class="number">3</span>] - bbox[<span class="number">1</span>], fill=<span class="keyword">False</span>,</div><div class="line">                          edgecolor=<span class="string">'red'</span>, linewidth=<span class="number">3.5</span>)</div><div class="line">            )</div><div class="line">        ax.text(bbox[<span class="number">0</span>], bbox[<span class="number">1</span>] - <span class="number">2</span>,</div><div class="line">                <span class="string">'&#123;:s&#125; &#123;:.3f&#125;'</span>.format(class_name, score),</div><div class="line">                bbox=dict(facecolor=<span class="string">'blue'</span>, alpha=<span class="number">0.5</span>),</div><div class="line">                fontsize=<span class="number">14</span>, color=<span class="string">'white'</span>)</div><div class="line"></div><div class="line">    ax.set_title((<span class="string">'&#123;&#125; detections with '</span></div><div class="line">                  <span class="string">'p(&#123;&#125; | box) &gt;= &#123;:.1f&#125;'</span>).format(class_name, class_name,</div><div class="line">                                                  thresh),</div><div class="line">                  fontsize=<span class="number">14</span>)</div><div class="line">    plt.axis(<span class="string">'off'</span>)</div><div class="line">    plt.tight_layout()</div><div class="line">    plt.draw()</div><div class="line">    plt.savefig(<span class="string">'/home/jk/py-faster-rcnn/output/faster_rcnn_alt_opt/test/'</span>+image_name) <span class="comment">#将检测后的图片保存到相应的路径</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span><span class="params">(net, image_name)</span>:</span></div><div class="line">    <span class="string">"""Detect object classes in an image using pre-computed object proposals."""</span></div><div class="line"></div><div class="line">    <span class="comment"># Load the demo image</span></div><div class="line">    im_file = os.path.join(cfg.DATA_DIR, <span class="string">'VOCdevkit/Caltech/JPEGImages'</span>, image_name)</div><div class="line">    im = cv2.imread(im_file)</div><div class="line"></div><div class="line">    <span class="comment"># Detect all object classes and regress object bounds</span></div><div class="line">    timer = Timer()</div><div class="line">    timer.tic()</div><div class="line">    scores, boxes = im_detect(net, im)</div><div class="line">    timer.toc()</div><div class="line">    <span class="keyword">print</span> (<span class="string">'Detection took &#123;:.3f&#125;s for '</span></div><div class="line">           <span class="string">'&#123;:d&#125; object proposals'</span>).format(timer.total_time, boxes.shape[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Visualize detections for each class</span></div><div class="line">    CONF_THRESH = <span class="number">0.85</span> <span class="comment"># 设置权值，越低检测出的框越多</span></div><div class="line">    NMS_THRESH = <span class="number">0.3</span></div><div class="line">    <span class="keyword">for</span> cls_ind, cls <span class="keyword">in</span> enumerate(CLASSES[<span class="number">1</span>:]):</div><div class="line">        cls_ind += <span class="number">1</span> <span class="comment"># because we skipped background</span></div><div class="line">        cls_boxes = boxes[:, <span class="number">4</span>*cls_ind:<span class="number">4</span>*(cls_ind + <span class="number">1</span>)]</div><div class="line">        cls_scores = scores[:, cls_ind]</div><div class="line">        dets = np.hstack((cls_boxes,</div><div class="line">                          cls_scores[:, np.newaxis])).astype(np.float32)</div><div class="line">        keep = nms(dets, NMS_THRESH)</div><div class="line">        dets = dets[keep, :]</div><div class="line">        vis_detections(im, image_name, cls, dets, thresh=CONF_THRESH)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_args</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""Parse input arguments."""</span></div><div class="line">    parser = argparse.ArgumentParser(description=<span class="string">'Faster R-CNN demo'</span>)</div><div class="line">    parser.add_argument(<span class="string">'--gpu'</span>, dest=<span class="string">'gpu_id'</span>, help=<span class="string">'GPU device id to use [0]'</span>,</div><div class="line">                        default=<span class="number">0</span>, type=int)</div><div class="line">    parser.add_argument(<span class="string">'--cpu'</span>, dest=<span class="string">'cpu_mode'</span>,</div><div class="line">                        help=<span class="string">'Use CPU mode (overrides --gpu)'</span>,</div><div class="line">                        action=<span class="string">'store_true'</span>)</div><div class="line">    parser.add_argument(<span class="string">'--net'</span>, dest=<span class="string">'demo_net'</span>, help=<span class="string">'Network to use [vgg16]'</span>,</div><div class="line">                        choices=NETS.keys(), default=<span class="string">'vgg16'</span>)</div><div class="line"></div><div class="line">    args = parser.parse_args()</div><div class="line"></div><div class="line">    <span class="keyword">return</span> args</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    cfg.TEST.HAS_RPN = <span class="keyword">True</span>  <span class="comment"># Use RPN for proposals</span></div><div class="line"></div><div class="line">    args = parse_args()</div><div class="line"></div><div class="line">    prototxt = os.path.join(cfg.MODELS_DIR, NETS[args.demo_net][<span class="number">0</span>],</div><div class="line">                            <span class="string">'faster_rcnn_alt_opt'</span>, <span class="string">'faster_rcnn_test.pt'</span>)</div><div class="line">    caffemodel = os.path.join(cfg.DATA_DIR, <span class="string">'faster_rcnn_models'</span>,</div><div class="line">                              NETS[args.demo_net][<span class="number">1</span>])</div><div class="line"></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(caffemodel):</div><div class="line">        <span class="keyword">raise</span> IOError((<span class="string">'&#123;:s&#125; not found.\nDid you run ./data/script/'</span></div><div class="line">                       <span class="string">'fetch_faster_rcnn_models.sh?'</span>).format(caffemodel))</div><div class="line"></div><div class="line">    <span class="keyword">if</span> args.cpu_mode:</div><div class="line">        caffe.set_mode_cpu()</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        caffe.set_mode_gpu()</div><div class="line">        caffe.set_device(args.gpu_id)</div><div class="line">        cfg.GPU_ID = args.gpu_id</div><div class="line">    net = caffe.Net(prototxt, caffemodel, caffe.TEST)</div><div class="line"></div><div class="line">    <span class="keyword">print</span> <span class="string">'\n\nLoaded network &#123;:s&#125;'</span>.format(caffemodel)</div><div class="line"></div><div class="line">    <span class="comment"># Warmup on a dummy image</span></div><div class="line">    im = <span class="number">128</span> * np.ones((<span class="number">300</span>, <span class="number">500</span>, <span class="number">3</span>), dtype=np.uint8)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">2</span>):</div><div class="line">        _, _= im_detect(net, im)</div><div class="line"></div><div class="line">    testfile_path = <span class="string">'/home/jk/py-faster-rcnn/data/VOCdevkit/Caltech/ImageSets/Main/test.txt'</span></div><div class="line">    <span class="keyword">with</span> open(testfile_path) <span class="keyword">as</span> f:</div><div class="line">        im_names = [x.strip()+<span class="string">'.jpg'</span> <span class="keyword">for</span> x <span class="keyword">in</span> f.readlines()] <span class="comment"># 从test.txt文件中读取图片文件名，找到相应的图片进行检测。也可以使用如下的方法，把项检测的图片存到tools/demo/文件夹下进行读取检测</span></div><div class="line"></div><div class="line">    <span class="comment">#im_names = ['set06_V002_I00023.jpg', 'set06_V002_I00072.jpg', 'set06_V002_I00097.jpg',</span></div><div class="line">    <span class="comment">#            'set06_V002_I00151.jpg', 'set07_V010_I00247.jpg']</span></div><div class="line">    <span class="keyword">for</span> im_name <span class="keyword">in</span> im_names:</div><div class="line">        <span class="keyword">print</span> <span class="string">'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'</span></div><div class="line">        <span class="keyword">print</span> <span class="string">'Demo for data/demo/&#123;&#125;'</span>.format(im_name)</div><div class="line">        demo(net, im_name)</div><div class="line"></div><div class="line">    plt.show()</div></pre></td></tr></table></figure>
<p>在命令行中输入一下命令进行检测：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python tools/demo_caltech.py</div></pre></td></tr></table></figure>
<h3 id="检测结果"><a href="#检测结果" class="headerlink" title="检测结果"></a>检测结果</h3><p>放几张检测后的结果图，感觉检测效果并不是很好，很多把背景当成行人的错误：</p>
<p><img src="https://ww2.sinaimg.cn/large/006y8lValy1fd36adqacrj30hr0h8qe6.jpg" alt=""></p>
<p><img src="https://ww4.sinaimg.cn/large/006y8lValy1fd36apj94fj30ho0h7wpo.jpg" alt=""></p>
<p><img src="https://ww1.sinaimg.cn/large/006y8lValy1fd36aznxnlj30hq0h1k2b.jpg" alt=""></p>
<p><img src="https://ww3.sinaimg.cn/large/006y8lValy1fd36bnalbwj30hk0h749k.jpg" alt=""></p>
<h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol>
<li><strong><a href="http://www.itdadao.com/articles/c15a253094p0.html" target="_blank" rel="external">使用Faster-Rcnn进行目标检测(实践篇)</a></strong></li>
<li><a href="https://github.com/zeyuanxy/fast-rcnn/blob/master/help/train/README.md" target="_blank" rel="external"><strong>Train Fast-RCNN on Another Dataset</strong></a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;前面已经介绍了如何准备数据集，以及如何修改数据集读写接口来操作数据集，接下来我来说明一下怎么来训练网络和之后的检测过程。&lt;/p&gt;
    
    </summary>
    
      <category term="经验" scheme="http://jacobkong.github.io/categories/%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习实践经验：用Faster R-CNN训练Caltech数据集——修改读写接口</title>
    <link href="http://jacobkong.github.io/posts/4113466123/"/>
    <id>http://jacobkong.github.io/posts/4113466123/</id>
    <published>2017-01-16T22:32:24.000Z</published>
    <updated>2017-02-18T11:17:14.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这部分主要讲如何修改Faster R-CNN的代码，来训练自己的数据集，首先确保你已经编译安装了py-faster-rcnn，并且准备好了数据集，具体可参考我<a href="http://jacobkong.github.io/posts/2093106769/">上一篇文章</a>。</p>
<a id="more"></a>
<h2 id="py-faster-rcnn文件结构"><a href="#py-faster-rcnn文件结构" class="headerlink" title="py-faster-rcnn文件结构"></a>py-faster-rcnn文件结构</h2><ul>
<li>caffe-fast-rcnn<br>这里是caffe框架目录，用来进行caffe编译安装</li>
<li>data<br>用来存放pre trained模型，比如ImageNet上的，要训练的数据集以及读取文件的cache缓存。</li>
<li>experiments<br>存放配置文件，运行的log文件，另外这个目录下有scripts 用来获取imagenet的模型，以及作者训练好的fast rcnn模型，以及相应的pascal-voc数据集</li>
<li>lib<br>用来存放一些python接口文件，如其下的datasets主要负责数据库读取，config负责cnn一些训练的配置选项</li>
<li>matlab<br>放置matlab与python的接口，用matlab来调用实现detection</li>
<li>models<br>里面存放了三个模型文件，小型网络的ZF，大型网络VGG16，中型网络VGG_CNN_M_1024</li>
<li>output<br>这里存放的是训练完成后的输出目录，默认会在default文件夹下</li>
<li>tools<br>里面存放的是训练和测试的Python文件</li>
</ul>
<h2 id="修改训练代码"><a href="#修改训练代码" class="headerlink" title="修改训练代码"></a>修改训练代码</h2><h3 id="所要操作文件结构介绍"><a href="#所要操作文件结构介绍" class="headerlink" title="所要操作文件结构介绍"></a>所要操作文件结构介绍</h3><p>所有需要修改的训练代码都放到了<code>py-faster-rcnn/lib</code>文件夹下，我们进入文件夹，里面主要用到的文件夹有：</p>
<ul>
<li>datasets：该目录下主要存放读写数据接口。</li>
<li>fast-rcnn：该目录下主要存放的是python的训练和测试脚本，以及训练的配置文件。</li>
<li>roi_data_layer：该目录下主要存放一些ROI处理操作文件。</li>
<li>utils：该目录下主要存放一些通用操作比如非极大值nms，以及计算bounding box的重叠率等常用功能。</li>
</ul>
<p>读写数据接口都放在<code>datasets/</code>文件夹下，我们进入文件夹，里面主要文件有：</p>
<ul>
<li>factory.py：这是个工厂类，用类生成imdb类并且返回数据库共网络训练和测试使用。</li>
<li>imdb.py：这是数据库读写类的基类，分装了许多db的操作，但是具体的一些文件读写需要继承继续读写</li>
<li>pascal_voc.py：这是imdb的子类，里面定义许多函数用来进行所有的数据读写操作。</li>
</ul>
<p>从上面可以看出，我们主要对<code>pascal_voc.py</code>文件进行修改。</p>
<h3 id="pascal-voc-py文件代码分析"><a href="#pascal-voc-py文件代码分析" class="headerlink" title="pascal_voc.py文件代码分析"></a>pascal_voc.py文件代码分析</h3><p>我们主要是基于<code>pasca_voc.py</code>这个文件进行修改，里面有几个重要的函数需要介绍：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, image_set, devkit_path=None)</span>:</span> <span class="comment"># 这个是初始化函数，它对应着的是pascal_voc的数据集访问格式。</span></div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">image_path_at</span><span class="params">(self, i)</span>:</span> <span class="comment"># 根据第i个图像样本返回其对应的path，其调用image_path_from_index(self, index):作为其具体实现。</span></div><div class="line">        </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">image_path_from_index</span><span class="params">(self, index)</span>:</span> <span class="comment"># 实现了 image_path的具体功能</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_load_image_set_index</span><span class="params">(self)</span>:</span> <span class="comment"># 加载了样本的list文件，根据ImageSet/Main/文件夹下的文件进行image_index的加载。</span></div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_default_path</span><span class="params">(self)</span>:</span> <span class="comment"># 获得数据集地址</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gt_roidb</span><span class="params">(self)</span>:</span> <span class="comment"># 读取并返回ground_truth的db</span></div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rpn_roidb</span><span class="params">(self)</span>:</span> <span class="comment"># 加载rpn产生的roi，调用_load_rpn_roidb(self, gt_roidb):函数作为其具体实现</span></div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_load_rpn_roidb</span><span class="params">(self, gt_roidb)</span>:</span> <span class="comment"># 加载rpn_file</span></div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_load_pascal_annotation</span><span class="params">(self, index)</span>:</span> <span class="comment"># 这个函数是读取gt的具体实现</span></div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_write_voc_results_file</span><span class="params">(self, all_boxes)</span>:</span> <span class="comment"># 将voc的检测结果写入到文件</span></div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_do_python_eval</span><span class="params">(self, output_dir = <span class="string">'output'</span>)</span>:</span> <span class="comment"># 根据python的evluation接口来做结果的分析</span></div></pre></td></tr></table></figure>
<h3 id="修改pascal-voc-py文件"><a href="#修改pascal-voc-py文件" class="headerlink" title="修改pascal_voc.py文件"></a>修改pascal_voc.py文件</h3><p>要想对自己的数据集进行读取，我们主要是进行<code>pascal_voc.py</code>文件的修改，但是为了不破坏源文件，我们可以将<code>pascal_voc.py</code>进行拷贝复制，从而进行修改。这里我将<code>pascal_voc.py</code>文件拷贝成<code>caltech.py</code>文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cp pascal_voc.py caltech.py</div></pre></td></tr></table></figure>
<p>下面我们对<code>caltech.py</code>文件进行修改，在这里我会一一列举每个我修改过的函数。这里按照文件中的顺序排列。。</p>
<h4 id="init函数修改"><a href="#init函数修改" class="headerlink" title="init函数修改"></a><strong>init</strong>函数修改</h4><p>这里是原始的pascal_voc的init函数，在这里，由于我们自己的数据集往往比voc的数据集要更简单的一些，在作者额代码里面用了很多的路径拼接，我们不用去迎合他的格式，将这些操作简单化即可。</p>
<h5 id="原始的函数"><a href="#原始的函数" class="headerlink" title="原始的函数"></a>原始的函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, image_set, year, devkit_path=None)</span>:</span></div><div class="line">        imdb.__init__(self, <span class="string">'voc_'</span> + year + <span class="string">'_'</span> + image_set)</div><div class="line">        self._year = year</div><div class="line">        self._image_set = image_set</div><div class="line">        self._devkit_path = self._get_default_path() <span class="keyword">if</span> devkit_path <span class="keyword">is</span> <span class="keyword">None</span> \</div><div class="line">                            <span class="keyword">else</span> devkit_path</div><div class="line">        self._data_path = os.path.join(self._devkit_path, <span class="string">'VOC'</span> + self._year)</div><div class="line">        self._classes = (<span class="string">'__background__'</span>, <span class="comment"># always index 0</span></div><div class="line">                         <span class="string">'aeroplane'</span>, <span class="string">'bicycle'</span>, <span class="string">'bird'</span>, <span class="string">'boat'</span>,</div><div class="line">                         <span class="string">'bottle'</span>, <span class="string">'bus'</span>, <span class="string">'car'</span>, <span class="string">'cat'</span>, <span class="string">'chair'</span>,</div><div class="line">                         <span class="string">'cow'</span>, <span class="string">'diningtable'</span>, <span class="string">'dog'</span>, <span class="string">'horse'</span>,</div><div class="line">                         <span class="string">'motorbike'</span>, <span class="string">'person'</span>, <span class="string">'pottedplant'</span>,</div><div class="line">                         <span class="string">'sheep'</span>, <span class="string">'sofa'</span>, <span class="string">'train'</span>, <span class="string">'tvmonitor'</span>)</div><div class="line">        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))</div><div class="line">        self._image_ext = <span class="string">'.jpg'</span></div><div class="line">        self._image_index = self._load_image_set_index()</div><div class="line">        <span class="comment"># Default to roidb handler</span></div><div class="line">        self._roidb_handler = self.selective_search_roidb</div><div class="line">        self._salt = str(uuid.uuid4())</div><div class="line">        self._comp_id = <span class="string">'comp4'</span></div><div class="line"></div><div class="line">        <span class="comment"># PASCAL specific config options</span></div><div class="line">        self.config = &#123;<span class="string">'cleanup'</span>     : <span class="keyword">True</span>,</div><div class="line">                       <span class="string">'use_salt'</span>    : <span class="keyword">True</span>,</div><div class="line">                       <span class="string">'use_diff'</span>    : <span class="keyword">False</span>,</div><div class="line">                       <span class="string">'matlab_eval'</span> : <span class="keyword">False</span>,</div><div class="line">                       <span class="string">'rpn_file'</span>    : <span class="keyword">None</span>,</div><div class="line">                       <span class="string">'min_size'</span>    : <span class="number">2</span>&#125;</div><div class="line"></div><div class="line">        <span class="keyword">assert</span> os.path.exists(self._devkit_path), \</div><div class="line">                <span class="string">'VOCdevkit path does not exist: &#123;&#125;'</span>.format(self._devkit_path)</div><div class="line">        <span class="keyword">assert</span> os.path.exists(self._data_path), \</div><div class="line">                <span class="string">'Path does not exist: &#123;&#125;'</span>.format(self._data_path)</div></pre></td></tr></table></figure>
<h5 id="修改后的函数"><a href="#修改后的函数" class="headerlink" title="修改后的函数"></a>修改后的函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, image_set, devkit_path=None)</span>:</span><span class="comment"># initial function，把year删除</span></div><div class="line">        imdb.__init__(self, image_set) <span class="comment"># imageset is train.txt or test.txt</span></div><div class="line">        self._image_set = image_set</div><div class="line">        self._devkit_path = devkit_path <span class="comment"># devkit_path = '~/py-faster-rcnn/data/VOCdevkit'</span></div><div class="line">        self._data_path = os.path.join(self._devkit_path, <span class="string">'Caltech'</span>) <span class="comment"># _data_path = '~/py-faster-rcnn/data/VOCdevkit/Caltech'</span></div><div class="line">        self._classes = (<span class="string">'__background__'</span>, <span class="comment"># always index 0</span></div><div class="line">                         <span class="string">'person'</span>) <span class="comment"># 我只有‘background’和‘person’两类</span></div><div class="line">        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))</div><div class="line">        self._image_ext = <span class="string">'.jpg'</span></div><div class="line">        self._image_index = self._load_image_set_index()</div><div class="line">        <span class="comment"># Default to roidb handler</span></div><div class="line">        self._roidb_handler = self.selective_search_roidb</div><div class="line">        self._salt = str(uuid.uuid4())</div><div class="line">        self._comp_id = <span class="string">'comp4'</span></div><div class="line"></div><div class="line">        <span class="comment"># PASCAL specific config options</span></div><div class="line">        self.config = &#123;<span class="string">'cleanup'</span>     : <span class="keyword">True</span>,</div><div class="line">                       <span class="string">'use_salt'</span>    : <span class="keyword">True</span>,</div><div class="line">                       <span class="string">'use_diff'</span>    : <span class="keyword">True</span>, <span class="comment"># 我把use_diff改为true了，因为我的数据集xml文件中没有&lt;difficult&gt;标签，否则之后训练会报错</span></div><div class="line">                       <span class="string">'matlab_eval'</span> : <span class="keyword">False</span>,</div><div class="line">                       <span class="string">'rpn_file'</span>    : <span class="keyword">None</span>,</div><div class="line">                       <span class="string">'min_size'</span>    : <span class="number">2</span>&#125;</div><div class="line"></div><div class="line">        <span class="keyword">assert</span> os.path.exists(self._devkit_path), \</div><div class="line">                <span class="string">'VOCdevkit path does not exist: &#123;&#125;'</span>.format(self._devkit_path)</div><div class="line">        <span class="keyword">assert</span> os.path.exists(self._data_path), \</div><div class="line">                <span class="string">'Path does not exist: &#123;&#125;'</span>.format(self._data_path)</div></pre></td></tr></table></figure>
<h4 id="load-image-set-index函数修改"><a href="#load-image-set-index函数修改" class="headerlink" title="_load_image_set_index函数修改"></a>_load_image_set_index函数修改</h4><h5 id="原始的函数-1"><a href="#原始的函数-1" class="headerlink" title="原始的函数"></a>原始的函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_load_image_set_index</span><span class="params">(self)</span>:</span></div><div class="line">      <span class="string">"""</span></div><div class="line">          Load the indexes listed in this dataset's image set file.</div><div class="line">          """</div><div class="line">      <span class="comment"># Example path to image set file:</span></div><div class="line">      <span class="comment"># self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt</span></div><div class="line">      image_set_file = os.path.join(self._data_path, <span class="string">'ImageSets'</span>, <span class="string">'Main'</span>,</div><div class="line">                                    self._image_set + <span class="string">'.txt'</span>)</div><div class="line">      <span class="keyword">assert</span> os.path.exists(image_set_file), \</div><div class="line">      <span class="string">'Path does not exist: &#123;&#125;'</span>.format(image_set_file)</div><div class="line">      <span class="keyword">with</span> open(image_set_file) <span class="keyword">as</span> f:</div><div class="line">          image_index = [x.strip() <span class="keyword">for</span> x <span class="keyword">in</span> f.readlines()]</div><div class="line">          <span class="keyword">return</span> image_index</div></pre></td></tr></table></figure>
<h5 id="修改后的函数-1"><a href="#修改后的函数-1" class="headerlink" title="修改后的函数"></a>修改后的函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_load_image_set_index</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Load the indexes listed in this dataset's image set file.</div><div class="line">        """</div><div class="line">        <span class="comment"># Example path to image set file:</span></div><div class="line">        <span class="comment"># self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt</span></div><div class="line">        <span class="comment"># /home/jk/py-faster-rcnn/data/VOCdevkit/Caltech/ImageSets/Main/train.txt</span></div><div class="line">        image_set_file = os.path.join(self._data_path, <span class="string">'ImageSets'</span>, <span class="string">'Main'</span>,</div><div class="line">                                      self._image_set + <span class="string">'.txt'</span>)</div><div class="line">        <span class="keyword">assert</span> os.path.exists(image_set_file), \</div><div class="line">                <span class="string">'Path does not exist: &#123;&#125;'</span>.format(image_set_file)</div><div class="line">        <span class="keyword">with</span> open(image_set_file) <span class="keyword">as</span> f:</div><div class="line">            image_index = [x.strip() <span class="keyword">for</span> x <span class="keyword">in</span> f.readlines()]</div><div class="line">        <span class="keyword">return</span> image_index</div></pre></td></tr></table></figure>
<p>其实没改，只是加了一行注释，从而更好理解路径问题。</p>
<h4 id="get-default-path函数修改"><a href="#get-default-path函数修改" class="headerlink" title="_get_default_path函数修改"></a>_get_default_path函数修改</h4><p>直接注释即可</p>
<h4 id="load-pascal-annotation函数修改"><a href="#load-pascal-annotation函数修改" class="headerlink" title="_load_pascal_annotation函数修改"></a>_load_pascal_annotation函数修改</h4><h5 id="原始的函数-2"><a href="#原始的函数-2" class="headerlink" title="原始的函数"></a>原始的函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_load_pascal_annotation</span><span class="params">(self, index)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Load image and bounding boxes info from XML file in the PASCAL VOC</div><div class="line">        format.</div><div class="line">        """</div><div class="line">        filename = os.path.join(self._data_path, <span class="string">'Annotations'</span>, index + <span class="string">'.xml'</span>)</div><div class="line">        tree = ET.parse(filename)</div><div class="line">        objs = tree.findall(<span class="string">'object'</span>)</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.config[<span class="string">'use_diff'</span>]:</div><div class="line">            <span class="comment"># Exclude the samples labeled as difficult</span></div><div class="line">            non_diff_objs = [</div><div class="line">                obj <span class="keyword">for</span> obj <span class="keyword">in</span> objs <span class="keyword">if</span> int(obj.find(<span class="string">'difficult'</span>).text) == <span class="number">0</span>]</div><div class="line">            <span class="comment"># if len(non_diff_objs) != len(objs):</span></div><div class="line">            <span class="comment">#     print 'Removed &#123;&#125; difficult objects'.format(</span></div><div class="line">            <span class="comment">#         len(objs) - len(non_diff_objs))</span></div><div class="line">            objs = non_diff_objs</div><div class="line">        num_objs = len(objs)</div><div class="line"></div><div class="line">        boxes = np.zeros((num_objs, <span class="number">4</span>), dtype=np.uint16)</div><div class="line">        gt_classes = np.zeros((num_objs), dtype=np.int32)</div><div class="line">        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)</div><div class="line">        <span class="comment"># "Seg" area for pascal is just the box area</span></div><div class="line">        seg_areas = np.zeros((num_objs), dtype=np.float32)</div><div class="line"></div><div class="line">        <span class="comment"># Load object bounding boxes into a data frame.</span></div><div class="line">        <span class="keyword">for</span> ix, obj <span class="keyword">in</span> enumerate(objs):</div><div class="line">            bbox = obj.find(<span class="string">'bndbox'</span>)</div><div class="line">            <span class="comment"># Make pixel indexes 0-based</span></div><div class="line">            x1 = float(bbox.find(<span class="string">'xmin'</span>).text) - <span class="number">1</span></div><div class="line">            y1 = float(bbox.find(<span class="string">'ymin'</span>).text) - <span class="number">1</span></div><div class="line">            x2 = float(bbox.find(<span class="string">'xmax'</span>).text) - <span class="number">1</span></div><div class="line">            y2 = float(bbox.find(<span class="string">'ymax'</span>).text) - <span class="number">1</span></div><div class="line">            cls = self._class_to_ind[obj.find(<span class="string">'name'</span>).text.lower().strip()]</div><div class="line">            boxes[ix, :] = [x1, y1, x2, y2]</div><div class="line">            gt_classes[ix] = cls</div><div class="line">            overlaps[ix, cls] = <span class="number">1.0</span></div><div class="line">            seg_areas[ix] = (x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>)</div><div class="line"></div><div class="line">        overlaps = scipy.sparse.csr_matrix(overlaps)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> &#123;<span class="string">'boxes'</span> : boxes,</div><div class="line">                <span class="string">'gt_classes'</span>: gt_classes,</div><div class="line">                <span class="string">'gt_overlaps'</span> : overlaps,</div><div class="line">                <span class="string">'flipped'</span> : <span class="keyword">False</span>,</div><div class="line">                <span class="string">'seg_areas'</span> : seg_areas&#125;</div></pre></td></tr></table></figure>
<h5 id="修改后的函数-2"><a href="#修改后的函数-2" class="headerlink" title="修改后的函数"></a>修改后的函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_load_pascal_annotation</span><span class="params">(self, index)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Load image and bounding boxes info from XML file in the PASCAL VOC</div><div class="line">        format.</div><div class="line">        """</div><div class="line">        filename = os.path.join(self._data_path, <span class="string">'Annotations'</span>, index + <span class="string">'.xml'</span>)</div><div class="line">        tree = ET.parse(filename)</div><div class="line">        objs = tree.findall(<span class="string">'object'</span>)</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.config[<span class="string">'use_diff'</span>]:</div><div class="line">            <span class="comment"># Exclude the samples labeled as difficult</span></div><div class="line">            non_diff_objs = [</div><div class="line">                obj <span class="keyword">for</span> obj <span class="keyword">in</span> objs <span class="keyword">if</span> int(obj.find(<span class="string">'difficult'</span>).text) == <span class="number">0</span>]</div><div class="line">            <span class="comment"># if len(non_diff_objs) != len(objs):</span></div><div class="line">            <span class="comment">#     print 'Removed &#123;&#125; difficult objects'.format(</span></div><div class="line">            <span class="comment">#         len(objs) - len(non_diff_objs))</span></div><div class="line">            objs = non_diff_objs</div><div class="line">        num_objs = len(objs)</div><div class="line"></div><div class="line">        boxes = np.zeros((num_objs, <span class="number">4</span>), dtype=np.uint16)</div><div class="line">        gt_classes = np.zeros((num_objs), dtype=np.int32)</div><div class="line">        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)</div><div class="line">        <span class="comment"># "Seg" area for pascal is just the box area</span></div><div class="line">        seg_areas = np.zeros((num_objs), dtype=np.float32)</div><div class="line"></div><div class="line">        <span class="comment"># Load object bounding boxes into a data frame.</span></div><div class="line">        <span class="keyword">for</span> ix, obj <span class="keyword">in</span> enumerate(objs):</div><div class="line">            bbox = obj.find(<span class="string">'bndbox'</span>)</div><div class="line">            <span class="comment"># Make pixel indexes 0-based</span></div><div class="line">            <span class="comment"># 这里我把‘-1’全部删除掉了，防止有的数据是0开始，然后‘-1’导致变为负数，产生AssertError错误</span></div><div class="line">            x1 = float(bbox.find(<span class="string">'xmin'</span>).text)</div><div class="line">            y1 = float(bbox.find(<span class="string">'ymin'</span>).text)</div><div class="line">            x2 = float(bbox.find(<span class="string">'xmax'</span>).text)</div><div class="line">            y2 = float(bbox.find(<span class="string">'ymax'</span>).text)</div><div class="line">            cls = self._class_to_ind[obj.find(<span class="string">'name'</span>).text.lower().strip()]</div><div class="line">            boxes[ix, :] = [x1, y1, x2, y2]</div><div class="line">            gt_classes[ix] = cls</div><div class="line">            overlaps[ix, cls] = <span class="number">1.0</span></div><div class="line">            seg_areas[ix] = (x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>)</div><div class="line"></div><div class="line">        overlaps = scipy.sparse.csr_matrix(overlaps)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> &#123;<span class="string">'boxes'</span> : boxes,</div><div class="line">                <span class="string">'gt_classes'</span>: gt_classes,</div><div class="line">                <span class="string">'gt_overlaps'</span> : overlaps,</div><div class="line">                <span class="string">'flipped'</span> : <span class="keyword">False</span>,</div><div class="line">                <span class="string">'seg_areas'</span> : seg_areas&#125;</div></pre></td></tr></table></figure>
<h4 id="main函数修改"><a href="#main函数修改" class="headerlink" title="main函数修改"></a>main函数修改</h4><p>原始的函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="keyword">from</span> datasets.pascal_voc <span class="keyword">import</span> pascal_voc</div><div class="line">    d = pascal_voc(<span class="string">'trainval'</span>, <span class="string">'2007'</span>)</div><div class="line">    res = d.roidb</div><div class="line">    <span class="keyword">from</span> IPython <span class="keyword">import</span> embed; embed()</div></pre></td></tr></table></figure>
<p>修改后的函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="keyword">from</span> datasets.caltech <span class="keyword">import</span> caltech <span class="comment"># 导入caltech包</span></div><div class="line">    d = caltech(<span class="string">'train'</span>, <span class="string">'/home/jk/py-faster-rcnn/data/VOCdevkit'</span>)<span class="comment">#调用构造函数，传入imageset和路径</span></div><div class="line">    res = d.roidb</div><div class="line">    <span class="keyword">from</span> IPython <span class="keyword">import</span> embed; embed()</div></pre></td></tr></table></figure>
<p>至此读取接口修改完毕，该文件中的其他函数并未修改。</p>
<h3 id="修改factory-py文件"><a href="#修改factory-py文件" class="headerlink" title="修改factory.py文件"></a>修改factory.py文件</h3><p>当网络训练时会调用factory里面的get方法获得相应的imdb，首先在文件头import 把pascal_voc改成caltech</p>
<p>在这个文件作者生成了多个数据库的路径，我们自己数据库只要给定根路径即可，修改主要有以下4个</p>
<ul>
<li>函数之后有两个多级的for循环，也将其注释</li>
<li>直接定义<code>devkit</code>。</li>
<li>利用创建自己的训练和测试的imdb set，这里的name的格式为<code>caltech_{}</code>。</li>
</ul>
<h4 id="原始的代码"><a href="#原始的代码" class="headerlink" title="原始的代码"></a>原始的代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># --------------------------------------------------------</span></div><div class="line"><span class="comment"># Fast R-CNN</span></div><div class="line"><span class="comment"># Copyright (c) 2015 Microsoft</span></div><div class="line"><span class="comment"># Licensed under The MIT License [see LICENSE for details]</span></div><div class="line"><span class="comment"># Written by Ross Girshick</span></div><div class="line"><span class="comment"># --------------------------------------------------------</span></div><div class="line"></div><div class="line"><span class="string">"""Factory method for easily getting imdbs by name."""</span></div><div class="line"></div><div class="line">__sets = &#123;&#125;</div><div class="line"></div><div class="line"><span class="keyword">from</span> datasets.pascal_voc <span class="keyword">import</span> pascal_voc</div><div class="line"><span class="keyword">from</span> datasets.coco <span class="keyword">import</span> coco</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># Set up voc_&lt;year&gt;_&lt;split&gt; using selective search "fast" mode</span></div><div class="line"><span class="keyword">for</span> year <span class="keyword">in</span> [<span class="string">'2007'</span>, <span class="string">'2012'</span>]:</div><div class="line">    <span class="keyword">for</span> split <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>, <span class="string">'trainval'</span>, <span class="string">'test'</span>]:</div><div class="line">        name = <span class="string">'voc_&#123;&#125;_&#123;&#125;'</span>.format(year, split)</div><div class="line">        __sets[name] = (<span class="keyword">lambda</span> split=split, year=year: pascal_voc(split, year))</div><div class="line"></div><div class="line"><span class="comment"># Set up coco_2014_&lt;split&gt;</span></div><div class="line"><span class="keyword">for</span> year <span class="keyword">in</span> [<span class="string">'2014'</span>]:</div><div class="line">    <span class="keyword">for</span> split <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'val'</span>, <span class="string">'minival'</span>, <span class="string">'valminusminival'</span>]:</div><div class="line">        name = <span class="string">'coco_&#123;&#125;_&#123;&#125;'</span>.format(year, split)</div><div class="line">        __sets[name] = (<span class="keyword">lambda</span> split=split, year=year: coco(split, year))</div><div class="line"></div><div class="line"><span class="comment"># Set up coco_2015_&lt;split&gt;</span></div><div class="line"><span class="keyword">for</span> year <span class="keyword">in</span> [<span class="string">'2015'</span>]:</div><div class="line">    <span class="keyword">for</span> split <span class="keyword">in</span> [<span class="string">'test'</span>, <span class="string">'test-dev'</span>]:</div><div class="line">        name = <span class="string">'coco_&#123;&#125;_&#123;&#125;'</span>.format(year, split)</div><div class="line">        __sets[name] = (<span class="keyword">lambda</span> split=split, year=year: coco(split, year))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_imdb</span><span class="params">(name)</span>:</span></div><div class="line">    <span class="string">"""Get an imdb (image database) by name."""</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> __sets.has_key(name):</div><div class="line">        <span class="keyword">raise</span> KeyError(<span class="string">'Unknown dataset: &#123;&#125;'</span>.format(name))</div><div class="line">    <span class="keyword">return</span> __sets[name]()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">list_imdbs</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""List all registered imdbs."""</span></div><div class="line">    <span class="keyword">return</span> __sets.keys()</div></pre></td></tr></table></figure>
<h4 id="修改后的文件"><a href="#修改后的文件" class="headerlink" title="修改后的文件"></a>修改后的文件</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># --------------------------------------------------------</span></div><div class="line"><span class="comment"># Fast R-CNN</span></div><div class="line"><span class="comment"># Copyright (c) 2015 Microsoft</span></div><div class="line"><span class="comment"># Licensed under The MIT License [see LICENSE for details]</span></div><div class="line"><span class="comment"># Written by Ross Girshick</span></div><div class="line"><span class="comment"># --------------------------------------------------------</span></div><div class="line"></div><div class="line"><span class="string">"""Factory method for easily getting imdbs by name."""</span></div><div class="line"></div><div class="line">__sets = &#123;&#125;</div><div class="line"></div><div class="line"><span class="keyword">from</span> datasets.caltech <span class="keyword">import</span> caltech <span class="comment"># 导入caltech包</span></div><div class="line"><span class="comment">#from datasets.coco import coco</span></div><div class="line"><span class="comment">#import numpy as np</span></div><div class="line"></div><div class="line">devkit = <span class="string">'/home/jk/py-faster-rcnn/data/VOCdevkit'</span></div><div class="line"><span class="comment"># Set up voc_&lt;year&gt;_&lt;split&gt; using selective search "fast" mode</span></div><div class="line"><span class="comment">#for year in ['2007', '2012']:</span></div><div class="line"><span class="comment">#    for split in ['train', 'val', 'trainval', 'test']:</span></div><div class="line"><span class="comment">#        name = 'voc_&#123;&#125;_&#123;&#125;'.format(year, split)</span></div><div class="line"><span class="comment">#        __sets[name] = (lambda split=split, year=year: pascal_voc(split, year))</span></div><div class="line"></div><div class="line"><span class="comment"># Set up coco_2014_&lt;split&gt;</span></div><div class="line"><span class="comment">#for year in ['2014']:</span></div><div class="line"><span class="comment">#    for split in ['train', 'val', 'minival', 'valminusminival']:</span></div><div class="line"><span class="comment">#        name = 'coco_&#123;&#125;_&#123;&#125;'.format(year, split)</span></div><div class="line"><span class="comment">#        __sets[name] = (lambda split=split, year=year: coco(split, year))</span></div><div class="line"></div><div class="line"><span class="comment"># Set up coco_2015_&lt;split&gt;</span></div><div class="line"><span class="comment">#for year in ['2015']:</span></div><div class="line"><span class="comment">#    for split in ['test', 'test-dev']:</span></div><div class="line"><span class="comment">#        name = 'coco_&#123;&#125;_&#123;&#125;'.format(year, split)</span></div><div class="line"><span class="comment">#        __sets[name] = (lambda split=split, year=year: coco(split, year))</span></div><div class="line"></div><div class="line"><span class="comment"># Set up caltech_&lt;split&gt;</span></div><div class="line"><span class="keyword">for</span> split <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'test'</span>]:</div><div class="line">    name = <span class="string">'caltech_&#123;&#125;'</span>.format(split)</div><div class="line">    __sets[name] = (<span class="keyword">lambda</span> imageset=split, devkit=devkit: caltech(imageset, devkit))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_imdb</span><span class="params">(name)</span>:</span></div><div class="line">    <span class="string">"""Get an imdb (image database) by name."""</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> __sets.has_key(name):</div><div class="line">        <span class="keyword">raise</span> KeyError(<span class="string">'Unknown dataset: &#123;&#125;'</span>.format(name))</div><div class="line">    <span class="keyword">return</span> __sets[name]()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">list_imdbs</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""List all registered imdbs."""</span></div><div class="line">    <span class="keyword">return</span> __sets.keys()</div></pre></td></tr></table></figure>
<h3 id="修改init-py文件"><a href="#修改init-py文件" class="headerlink" title="修改init.py文件"></a>修改<strong>init</strong>.py文件</h3><p>在行首添加上 <code>from .caltech import caltech</code></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>坐标的顺序我再说一次，要左上右下，并且x1必须要小于x2，这个是基本，反了会在坐标水平变换的时候会出错，坐标从0开始，如果已经是0，则不需要再-1。</li>
<li>训练图像的大小不要太大，否则生成的OP也会太多，速度太慢，图像样本大小最好调整到500，600左右，然后再提取OP</li>
<li>如果读取并生成pkl文件之后，实际数据内容或者顺序还有问题，记得要把data/cache/下面的pkl文件给删掉。</li>
</ul>
<h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol>
<li><a href="http://www.cnblogs.com/louyihang-loves-baiyan/p/4903231.html" target="_blank" rel="external"><strong>Fast RCNN训练自己的数据集 （2修改读写接口）</strong></a></li>
<li><a href="http://www.cnblogs.com/CarryPotMan/p/5390336.html" target="_blank" rel="external"><strong>Faster R-CNN教程</strong></a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;这部分主要讲如何修改Faster R-CNN的代码，来训练自己的数据集，首先确保你已经编译安装了py-faster-rcnn，并且准备好了数据集，具体可参考我&lt;a href=&quot;http://jacobkong.github.io/posts/2093106769/&quot;&gt;上一篇文章&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="经验" scheme="http://jacobkong.github.io/categories/%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习实践经验：用Faster R-CNN训练行人检测数据集Caltech——准备工作</title>
    <link href="http://jacobkong.github.io/posts/2093106769/"/>
    <id>http://jacobkong.github.io/posts/2093106769/</id>
    <published>2017-01-15T22:32:24.000Z</published>
    <updated>2017-02-28T06:55:04.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Faster R-CNN是Ross Girshick大神在Fast R-CNN基础上提出的又一个更加快速、更高mAP的用于目标检测的深度学习框架，它对Fast R-CNN进行的最主要的优化就是在Region Proposal阶段，引入了Region Proposal Network (RPN)来进行Region Proposal，同时可以达到和检测网络共享整个图片的卷积网络特征的目标，使得region proposal几乎是<strong>cost free</strong>的。</p>
<p>关于Faster R-CNN的详细介绍，可以参考我<a href="http://jacobkong.github.io/posts/3802700508/">上一篇博客</a>。</p>
<p>Faster R-CNN的代码是开源的，有两个版本：<a href="https://github.com/ShaoqingRen/faster_rcnn" target="_blank" rel="external">MATLAB版本(<strong>faster_rcnn</strong>)</a>，<a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="external">Python版本(<strong>py-faster-rcnn</strong>)</a>。</p>
<p>这里我主要使用的是Python版本，Python版本在测试期间会比MATLAB版本慢10%，因为Python layers中的一些操作是在CPU中执行的，但是准确率应该是差不多的。</p>
<a id="more"></a>
<h2 id="准备工作1——py-faster-rcnn的编译安装测试"><a href="#准备工作1——py-faster-rcnn的编译安装测试" class="headerlink" title="准备工作1——py-faster-rcnn的编译安装测试"></a>准备工作1——py-faster-rcnn的编译安装测试</h2><h3 id="py-faster-rcnn的编译安装"><a href="#py-faster-rcnn的编译安装" class="headerlink" title="py-faster-rcnn的编译安装"></a>py-faster-rcnn的编译安装</h3><ol>
<li><p>克隆Faster R-CNN仓库：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> --recursive https://github.com/rbgirshick/py-faster-rcnn.git</div></pre></td></tr></table></figure>
<p>一定要加上<code>--recursive</code>标志，假设克隆后的文件夹名字叫<code>py-faster-rcnn</code></p>
</li>
<li><p>编译Cython模块：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> py-faster-rcnn/lib</div><div class="line">make</div></pre></td></tr></table></figure>
</li>
<li><p>编译里面的Caffe和pycaffe：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> py-faster-rcnn/caffe-fast-rcnn</div><div class="line"><span class="comment"># 按照编译Caffe的方法，进行编译</span></div><div class="line"><span class="comment"># 注意Makefile.config的修改，这里不再赘述Caffe的安装</span></div><div class="line"><span class="comment"># 编译</span></div><div class="line">make -j8 &amp;&amp; make pycaffe</div></pre></td></tr></table></figure>
</li>
<li><p>这里贴上我的<code>Makefile.config</code>文件代码，根据你的情况进行相应修改</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## Refer to http://caffe.berkeleyvision.org/installation.html</span></div><div class="line"><span class="comment"># Contributions simplifying and improving our build system are welcome!</span></div><div class="line"></div><div class="line"><span class="comment"># cuDNN acceleration switch (uncomment to build with cuDNN).</span></div><div class="line">USE_CUDNN := 1</div><div class="line"></div><div class="line"><span class="comment"># CPU-only switch (uncomment to build without GPU support).</span></div><div class="line"><span class="comment"># CPU_ONLY := 1</span></div><div class="line"></div><div class="line"><span class="comment"># uncomment to disable IO dependencies and corresponding data layers</span></div><div class="line"><span class="comment"># USE_OPENCV := 0</span></div><div class="line"><span class="comment"># USE_LEVELDB := 0</span></div><div class="line"><span class="comment"># USE_LMDB := 0</span></div><div class="line"></div><div class="line"><span class="comment"># uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)</span></div><div class="line"><span class="comment"># You should not set this flag if you will be reading LMDBs with any</span></div><div class="line"><span class="comment"># possibility of simultaneous read and write</span></div><div class="line"><span class="comment"># ALLOW_LMDB_NOLOCK := 1</span></div><div class="line"></div><div class="line"><span class="comment"># Uncomment if you're using OpenCV 3</span></div><div class="line">OPENCV_VERSION := 3</div><div class="line"></div><div class="line"><span class="comment"># To customize your choice of compiler, uncomment and set the following.</span></div><div class="line"><span class="comment"># N.B. the default for Linux is g++ and the default for OSX is clang++</span></div><div class="line"><span class="comment"># CUSTOM_CXX := g++</span></div><div class="line"></div><div class="line"><span class="comment"># CUDA directory contains bin/ and lib/ directories that we need.</span></div><div class="line">CUDA_DIR := /usr/<span class="built_in">local</span>/cuda</div><div class="line"><span class="comment"># On Ubuntu 14.04, if cuda tools are installed via</span></div><div class="line"><span class="comment"># "sudo apt-get install nvidia-cuda-toolkit" then use this instead:</span></div><div class="line"><span class="comment"># CUDA_DIR := /usr</span></div><div class="line"></div><div class="line"><span class="comment"># CUDA architecture setting: going with all of them.</span></div><div class="line"><span class="comment"># For CUDA &lt; 6.0, comment the *_50 lines for compatibility.</span></div><div class="line">CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \</div><div class="line">-gencode arch=compute_20,code=sm_21 \</div><div class="line">-gencode arch=compute_30,code=sm_30 \</div><div class="line">-gencode arch=compute_35,code=sm_35 \</div><div class="line">-gencode arch=compute_50,code=sm_50 \</div><div class="line">-gencode arch=compute_50,code=compute_50</div><div class="line"></div><div class="line"><span class="comment"># BLAS choice:</span></div><div class="line"><span class="comment"># atlas for ATLAS (default)</span></div><div class="line"><span class="comment"># mkl for MKL</span></div><div class="line"><span class="comment"># open for OpenBlas</span></div><div class="line">BLAS :=mkl</div><div class="line"><span class="comment"># Custom (MKL/ATLAS/OpenBLAS) include and lib directories.</span></div><div class="line"><span class="comment"># Leave commented to accept the defaults for your choice of BLAS</span></div><div class="line"><span class="comment"># (which should work)!</span></div><div class="line"><span class="comment"># BLAS_INCLUDE := /path/to/your/blas</span></div><div class="line"><span class="comment"># BLAS_LIB := /path/to/your/blas</span></div><div class="line"></div><div class="line"><span class="comment"># Homebrew puts openblas in a directory that is not on the standard search path</span></div><div class="line"><span class="comment"># BLAS_INCLUDE := $(shell brew --prefix openblas)/include</span></div><div class="line"><span class="comment"># BLAS_LIB := $(shell brew --prefix openblas)/lib</span></div><div class="line"></div><div class="line"><span class="comment"># This is required only if you will compile the matlab interface.</span></div><div class="line"><span class="comment"># MATLAB directory should contain the mex binary in /bin.</span></div><div class="line">MATLAB_DIR := /usr/<span class="built_in">local</span>/MATLAB/R2016b</div><div class="line"><span class="comment"># MATLAB_DIR := /Applications/MATLAB_R2012b.app</span></div><div class="line"></div><div class="line"><span class="comment"># <span class="doctag">NOTE:</span> this is required only if you will compile the python interface.</span></div><div class="line"><span class="comment"># We need to be able to find Python.h and numpy/arrayobject.h.</span></div><div class="line"><span class="comment"># PYTHON_INCLUDE := /usr/include/python2.7 \</span></div><div class="line">/usr/lib/python2.7/dist-packages/numpy/core/include</div><div class="line"><span class="comment"># Anaconda Python distribution is quite popular. Include path:</span></div><div class="line"><span class="comment"># Verify anaconda location, sometimes it's in root.</span></div><div class="line">ANACONDA_HOME := $(HOME)/anaconda</div><div class="line">PYTHON_INCLUDE := $(ANACONDA_HOME)/include \</div><div class="line">$(ANACONDA_HOME)/include/python2.7 \</div><div class="line">$(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \</div><div class="line">$ /usr/include/python2.7</div><div class="line"><span class="comment"># Uncomment to use Python 3 (default is Python 2)</span></div><div class="line"><span class="comment"># PYTHON_LIBRARIES := boost_python3 python3.5m</span></div><div class="line"><span class="comment"># PYTHON_INCLUDE := /usr/include/python3.5m \</span></div><div class="line"><span class="comment"># /usr/lib/python3.5/dist-packages/numpy/core/include</span></div><div class="line"></div><div class="line"><span class="comment"># We need to be able to find libpythonX.X.so or .dylib.</span></div><div class="line"><span class="comment"># PYTHON_LIB := /usr/lib</span></div><div class="line">PYTHON_LIB := $(ANACONDA_HOME)/lib</div><div class="line"></div><div class="line"><span class="comment"># Homebrew installs numpy in a non standard path (keg only)</span></div><div class="line"><span class="comment"># PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include</span></div><div class="line"><span class="comment"># PYTHON_LIB += $(shell brew --prefix numpy)/lib</span></div><div class="line"></div><div class="line"><span class="comment"># Uncomment to support layers written in Python (will link against Python libs)</span></div><div class="line">WITH_PYTHON_LAYER := 1</div><div class="line"></div><div class="line"><span class="comment"># Whatever else you find you need goes here.</span></div><div class="line"><span class="comment"># INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include</span></div><div class="line"><span class="comment"># LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib</span></div><div class="line">INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/<span class="built_in">local</span>/include /usr/include/hdf5/serial </div><div class="line">LIBRARY_DIRS := $(PYTHON_LIB) /usr/<span class="built_in">local</span>/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial</div><div class="line"></div><div class="line"><span class="comment"># If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies</span></div><div class="line"><span class="comment"># INCLUDE_DIRS += $(shell brew --prefix)/include</span></div><div class="line"><span class="comment"># LIBRARY_DIRS += $(shell brew --prefix)/lib</span></div><div class="line"></div><div class="line"><span class="comment"># Uncomment to use `pkg-config` to specify OpenCV library paths.</span></div><div class="line"><span class="comment"># (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)</span></div><div class="line"><span class="comment"># USE_PKG_CONFIG := 1</span></div><div class="line"></div><div class="line"><span class="comment"># N.B. both build and distribute dirs are cleared on `make clean`</span></div><div class="line">BUILD_DIR := build</div><div class="line">DISTRIBUTE_DIR := distribute</div><div class="line"></div><div class="line"><span class="comment"># Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171</span></div><div class="line"><span class="comment"># DEBUG := 1</span></div><div class="line"></div><div class="line"><span class="comment"># The ID of the GPU that 'make runtest' will use to run unit tests.</span></div><div class="line">TEST_GPUID := 0</div><div class="line"></div><div class="line"><span class="comment"># enable pretty build (comment to see full commands)</span></div><div class="line">Q ?= @</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Demo运行"><a href="#Demo运行" class="headerlink" title="Demo运行"></a>Demo运行</h3><p>为了检验你的py-faster-rcnn是否成功安装，作者给出了一个demo，可以利用在PASCAL VOC2007数据集上体现训练好的模型，来进行demo的运行，步骤如下：</p>
<ol>
<li><p>下载预训练好的Faster R-CNN检测器：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> py-faster-rcnn</div><div class="line">./data/scripts/fetch_faster_rcnn_models.sh</div></pre></td></tr></table></figure>
<p>这条命令会自动下载名为<code>faster_rcnn_models.tgz</code>的文件，解压后会创建<code>data/faster_rcnn_models</code>文件夹，里面会有两个模型：</p>
<ul>
<li>ZF_faster_rcnn_final.caffemodel：在ZF网络模型下训练所得</li>
<li>VGG16_faster_rcnn_final.caffemodel：在VGG16网络模型下训练所得。</li>
</ul>
</li>
<li><p>运行demo：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> py-faster-rcnn</div><div class="line">./tools/demo.py</div></pre></td></tr></table></figure>
</li>
<li><p>demo会检测5张图片，这5张图片放在<code>data/demo/</code>文件夹下，其中一张的检测结果如下：</p>
<p><img src="https://ww2.sinaimg.cn/large/006tNbRwgy1fcthbwzgobj30gv0e0wh9.jpg" alt=""></p>
</li>
<li><p>至此如果上述过程没有出错，那么py-faster-rcnn算是成功编译安装。</p>
</li>
<li><p>若出现报错如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ImportError: /xx/xx/xx/py-faster-rcnn/tools/../lib/nms/cpu_nms.so: undefined symbol: PyFPE_jbuf</div></pre></td></tr></table></figure>
<p>需要将<code>lib/fast_rcnn/nms_wrapper.py</code>文件中的<code>from nms.cpu_nms import cpu_nms</code>注释掉即可。</p>
</li>
</ol>
<h2 id="准备工作2——Caltech数据集"><a href="#准备工作2——Caltech数据集" class="headerlink" title="准备工作2——Caltech数据集"></a>准备工作2——Caltech数据集</h2><p>由于Faster R-CNN的一部分实验是在PASCAL VOC2007数据集上进行的，所以要想用Faster R-CNN训练我们自己的数据集，首先应该搞清楚PASCAL VOC2007数据集中的目录、图片、标注格式，这样我们才能用自己的数据集制作出类似于PASCAL VOC2007类似的数据集，供Faster R-CNN来进行训练及测试。</p>
<h3 id="获取PASCAL-VOC2007数据集"><a href="#获取PASCAL-VOC2007数据集" class="headerlink" title="获取PASCAL VOC2007数据集"></a>获取PASCAL VOC2007数据集</h3><p>这一部分不是必须的，如果你需要PASCAL VOC2007数据集，可以利用以下命令获取数据集，但<strong>我们下载VOC数据集的目的主要是观察他的文件结构和文件内容，以便于我们构建符合要求的自己的数据集。</strong></p>
<ol>
<li><p>创建一个专门用来存数据集的地方，假设是<code>$HOME/data</code>文件夹。</p>
</li>
<li><p>下载PASCAL VOC2007的训练、验证和测试数据集：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span>/data</div><div class="line">wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar</div><div class="line">wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar</div></pre></td></tr></table></figure>
</li>
<li><p>下载完后用以下命令解压：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tar xvf VOCtrainval_06-Nov-2007.tar</div><div class="line">tar xvf VOCtest_06-Nov-2007.tar</div></pre></td></tr></table></figure>
</li>
<li><p>会得到如下文件结构：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$HOME</span>/data/VOCdevkit/                        <span class="comment"># 根文件夹</span></div><div class="line"><span class="variable">$HOME</span>/data/VOCdevkit/VOC2007                 <span class="comment"># VOC2007文件夹</span></div><div class="line"><span class="variable">$HOME</span>/data/VOCdevkit/VOC2007/Annotations     <span class="comment"># 标记文件夹</span></div><div class="line"><span class="variable">$HOME</span>/data/VOCdevkit/VOC2007/ImageSets       <span class="comment"># 供train.txt、test.txt、val.txt等文件存放的文件夹</span></div><div class="line"><span class="variable">$HOME</span>/data/VOCdevkit/VOC2007/JPEGImages      <span class="comment"># 存放图片文件夹</span></div><div class="line"><span class="comment"># ... 以及其他的文件夹及子文件夹 ...</span></div></pre></td></tr></table></figure>
</li>
<li><p>创建快捷方式symlinks来连接到VOC数据集存放的地方：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> py-faster-rcnn/data</div><div class="line">ln <span class="_">-s</span> <span class="variable">$HOME</span>/data/VOCdevkit/ VOCdevkit</div></pre></td></tr></table></figure>
<p>这里需要把<code>$HOME/data/VOCdevkit/</code>改为你存放<code>VOCdevkit</code>文件夹的路径</p>
<p><strong>最好使用symlinks来在共享同一份数据集，防止数据集多处拷贝，占用空间。</strong></p>
</li>
<li><p>至此VOC数据集创建完毕。</p>
</li>
</ol>
<h3 id="PASCAL-VOC数据集的分析"><a href="#PASCAL-VOC数据集的分析" class="headerlink" title="PASCAL VOC数据集的分析"></a>PASCAL VOC数据集的分析</h3><p>PASCAL VOC数据集的文件结构，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">└── VOCdevkit</div><div class="line">    └── VOC2007　</div><div class="line">        ├── Annotations　　</div><div class="line">        ├── ImageSets　　</div><div class="line">        │   ├── Layout　　</div><div class="line">        │   ├── Main　　</div><div class="line">        │   └── Segmentation　　</div><div class="line">        ├── JPEGImages　　</div><div class="line">        ├── SegmentationClass　　</div><div class="line">        └── SegmentationObject</div></pre></td></tr></table></figure>
<h4 id="Annotations"><a href="#Annotations" class="headerlink" title="Annotations"></a>Annotations</h4><p>该文件夹主要用来存放图片标注（即为ground truth），文件是.xml格式，每张图片都有一个.xml文件与之对应。选取其中一个文件进行如下分析：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">annotation</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">folder</span>&gt;</span>VOC2007<span class="tag">&lt;/<span class="name">folder</span>&gt;</span> # 必须有，父文件夹的名称</div><div class="line">	<span class="tag">&lt;<span class="name">filename</span>&gt;</span>000005.jpg<span class="tag">&lt;/<span class="name">filename</span>&gt;</span>　#　必须有</div><div class="line">	<span class="tag">&lt;<span class="name">source</span>&gt;</span>　# 可有可无</div><div class="line">		<span class="tag">&lt;<span class="name">database</span>&gt;</span>The VOC2007 Database<span class="tag">&lt;/<span class="name">database</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">annotation</span>&gt;</span>PASCAL VOC2007<span class="tag">&lt;/<span class="name">annotation</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">image</span>&gt;</span>flickr<span class="tag">&lt;/<span class="name">image</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">flickrid</span>&gt;</span>325991873<span class="tag">&lt;/<span class="name">flickrid</span>&gt;</span></div><div class="line">	<span class="tag">&lt;/<span class="name">source</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">owner</span>&gt;</span>　# 可有可无</div><div class="line">		<span class="tag">&lt;<span class="name">flickrid</span>&gt;</span>archintent louisville<span class="tag">&lt;/<span class="name">flickrid</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>?<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">	<span class="tag">&lt;/<span class="name">owner</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">size</span>&gt;</span>　# 表示图像大小</div><div class="line">		<span class="tag">&lt;<span class="name">width</span>&gt;</span>500<span class="tag">&lt;/<span class="name">width</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">height</span>&gt;</span>375<span class="tag">&lt;/<span class="name">height</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">depth</span>&gt;</span>3<span class="tag">&lt;/<span class="name">depth</span>&gt;</span></div><div class="line">	<span class="tag">&lt;/<span class="name">size</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">segmented</span>&gt;</span>0<span class="tag">&lt;/<span class="name">segmented</span>&gt;</span>　# 用于分割</div><div class="line">	<span class="tag">&lt;<span class="name">object</span>&gt;</span>　# 目标信息，类别，bbox信息，图片中每个目标对应一个<span class="tag">&lt;<span class="name">object</span>&gt;</span>标签</div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>chair<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">pose</span>&gt;</span>Rear<span class="tag">&lt;/<span class="name">pose</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">truncated</span>&gt;</span>0<span class="tag">&lt;/<span class="name">truncated</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">difficult</span>&gt;</span>0<span class="tag">&lt;/<span class="name">difficult</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">bndbox</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">xmin</span>&gt;</span>263<span class="tag">&lt;/<span class="name">xmin</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">ymin</span>&gt;</span>211<span class="tag">&lt;/<span class="name">ymin</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">xmax</span>&gt;</span>324<span class="tag">&lt;/<span class="name">xmax</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">ymax</span>&gt;</span>339<span class="tag">&lt;/<span class="name">ymax</span>&gt;</span></div><div class="line">		<span class="tag">&lt;/<span class="name">bndbox</span>&gt;</span></div><div class="line">	<span class="tag">&lt;/<span class="name">object</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">object</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>chair<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">pose</span>&gt;</span>Unspecified<span class="tag">&lt;/<span class="name">pose</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">truncated</span>&gt;</span>1<span class="tag">&lt;/<span class="name">truncated</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">difficult</span>&gt;</span>1<span class="tag">&lt;/<span class="name">difficult</span>&gt;</span></div><div class="line">		<span class="tag">&lt;<span class="name">bndbox</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">xmin</span>&gt;</span>5<span class="tag">&lt;/<span class="name">xmin</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">ymin</span>&gt;</span>244<span class="tag">&lt;/<span class="name">ymin</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">xmax</span>&gt;</span>67<span class="tag">&lt;/<span class="name">xmax</span>&gt;</span></div><div class="line">			<span class="tag">&lt;<span class="name">ymax</span>&gt;</span>374<span class="tag">&lt;/<span class="name">ymax</span>&gt;</span></div><div class="line">		<span class="tag">&lt;/<span class="name">bndbox</span>&gt;</span></div><div class="line">	<span class="tag">&lt;/<span class="name">object</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">annotation</span>&gt;</span></div></pre></td></tr></table></figure>
<p><strong>需要注意的</strong>，对于我们自己准备的xml标记文件中，每个<code>&lt;object&gt;</code>标签中的<code>&lt;xmin&gt;</code>和<code>&lt;ymin&gt;</code>标签中所对应的坐标值最好大于0，千万不能为负数，否则在训练过程中会报错：<code>AssertionError: assert (boxes[:, 2]) &gt;= boxes[:, 0]).all()</code>，如下：</p>
<p><img src="https://ww2.sinaimg.cn/large/006tNbRwly1fctnwv48gzj30h701waa0.jpg" alt=""></p>
<p>所以为了能够顺利训练，一定要仔细检查自己的xml文件中的左上角的坐标是否都为正。我被这个bug卡了一两天，最终把自己标记中所有的错误坐标找出来，才得以顺利训练。</p>
<h4 id="ImageSets"><a href="#ImageSets" class="headerlink" title="ImageSets"></a>ImageSets</h4><p>ImageSets文件夹下有三个子文件夹，这里我们只需关注Main文件夹即可。Main文件夹下主要用到的是train.txt、val.txt、test.txt、trainval.txt文件，每个文件中写着供训练、验证、测试所用的文件名的集合，如下：</p>
<p><img src="https://ww3.sinaimg.cn/large/006tNbRwly1fctobmg8rkj302f03vjrc.jpg" alt=""></p>
<h4 id="JPEGImages"><a href="#JPEGImages" class="headerlink" title="JPEGImages"></a>JPEGImages</h4><p>JPEGImages文件夹下主要存放着所有的.jpg文件格式的输入图片，不在赘述。</p>
<h3 id="制作VOC类似的Caltech数据集"><a href="#制作VOC类似的Caltech数据集" class="headerlink" title="制作VOC类似的Caltech数据集"></a>制作VOC类似的Caltech数据集</h3><p>经过以上对PASCAL VOC数据集文件结构的分析，我们仿照其，创建首先创建类似的文件结构即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">└── VOCdevkit</div><div class="line">    └── VOC2007　</div><div class="line">    └── Caltech　</div><div class="line">        ├── Annotations　　</div><div class="line">        ├── ImageSets　　　</div><div class="line">        │   └── Main　　</div><div class="line">        └── JPEGImages</div></pre></td></tr></table></figure>
<p>我建议将Caltech文件创建一个symlinks链接到VOCdevkit文件夹之下，因为这样会方便之后训练代码的修改。</p>
<ul>
<li>至于<a href="https://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/" target="_blank" rel="external">Caltech数据集</a>如何从.seq文件转化为一张张.jpg图片，这里可以<a href="https://github.com/mitmul/caltech-pedestrian-dataset-converter" target="_blank" rel="external">参考这里</a>。</li>
<li>至于Annotations中一个个.xml标记文件是实验室师兄给我的，<a href="https://github.com/mitmul/caltech-pedestrian-dataset-converter" target="_blank" rel="external">上面提到的方法</a>也可以转化，但是并不符合要求。</li>
<li>至于ImageSets中的train.txt是根据.xml文件得来的，test.txt是每个seq中每隔30帧取一帧图片得来的。</li>
</ul>
<p>以上所有和<a href="https://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/" target="_blank" rel="external">Caltech数据集</a>有关的文件，都可以直接邮件与我联系，我直接发给你，可以省下不少制作数据集的时间。</p>
<h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol>
<li><a href="http://www.cnblogs.com/louyihang-loves-baiyan/p/4885659.html?utm_source=tuicool&amp;utm_medium=referral" target="_blank" rel="external"><strong>FastRCNN 训练自己数据集 (1编译配置)</strong></a></li>
<li><a href="https://saicoco.github.io/object-detection-4/" target="_blank" rel="external"><strong>目标检测—Faster RCNN2</strong></a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;Faster R-CNN是Ross Girshick大神在Fast R-CNN基础上提出的又一个更加快速、更高mAP的用于目标检测的深度学习框架，它对Fast R-CNN进行的最主要的优化就是在Region Proposal阶段，引入了Region Proposal Network (RPN)来进行Region Proposal，同时可以达到和检测网络共享整个图片的卷积网络特征的目标，使得region proposal几乎是&lt;strong&gt;cost free&lt;/strong&gt;的。&lt;/p&gt;
&lt;p&gt;关于Faster R-CNN的详细介绍，可以参考我&lt;a href=&quot;http://jacobkong.github.io/posts/3802700508/&quot;&gt;上一篇博客&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;Faster R-CNN的代码是开源的，有两个版本：&lt;a href=&quot;https://github.com/ShaoqingRen/faster_rcnn&quot;&gt;MATLAB版本(&lt;strong&gt;faster_rcnn&lt;/strong&gt;)&lt;/a&gt;，&lt;a href=&quot;https://github.com/rbgirshick/py-faster-rcnn&quot;&gt;Python版本(&lt;strong&gt;py-faster-rcnn&lt;/strong&gt;)&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;这里我主要使用的是Python版本，Python版本在测试期间会比MATLAB版本慢10%，因为Python layers中的一些操作是在CPU中执行的，但是准确率应该是差不多的。&lt;/p&gt;
    
    </summary>
    
      <category term="经验" scheme="http://jacobkong.github.io/categories/%E7%BB%8F%E9%AA%8C/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：Faster R-CNN</title>
    <link href="http://jacobkong.github.io/posts/3802700508/"/>
    <id>http://jacobkong.github.io/posts/3802700508/</id>
    <published>2016-12-16T22:32:24.000Z</published>
    <updated>2017-03-09T06:32:01.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li><strong>Region Proposal的计算</strong>是基于Region Proposal算法来假设物体位置的物体检测网络比如：<strong>SPPnet, Fast R-CNN</strong>运行时间的瓶颈。</li>
<li>Faster R-CNN引入了<strong>Region Proposal Network（RPN）</strong>来和检测网络共享整个图片的卷积网络特征，因此使得region proposal几乎是<strong>cost free</strong>的。</li>
<li>RPN-&gt;预测物体边界（object bounds）和在每一位置的分数（objectness score）</li>
<li>通过在一个网络中共享RPN和Fast R-CNN的卷积特征来融合两者——<strong>使用“attention”机制。</strong></li>
<li>300 proposals pre image.</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>RP是当前许多先进检测系统的瓶颈。</li>
<li>Region proposal methods:<ul>
<li>Selective Search: one of the most popular method </li>
<li>EdgeBoxes: trade off between proposal quality and speed.</li>
<li>region proposal这一步依旧和检测网络花费同样多的时间。</li>
</ul>
</li>
<li>Fast R-CNN生成的feature map 也能用来生成RP。在这些卷积特征之上我们通过这样的方式构建RPN：通过添加几个额外的卷积层来模拟一个regular grid上每一个位置的regress region bounds和objectness scores。<strong>所以RPN也是一种fully convolutional network(FCN)</strong>，从而可以端到端训练来产生detection proposals。</li>
<li><strong>anchor boxes</strong>：references at multiple scales and aspect ratios. 我们的方法可以看成pyramid of regression reference，从而避免枚举多尺寸、多横纵比的images或者filters</li>
</ul>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul>
<li>R-CNN主要是一个分类器，他不能预测object bounds，他的准确性依赖于Region proposal模块的表现</li>
</ul>
<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><ul>
<li>由两个模块组成：<ul>
<li>第一个模块：A deep fuuly convolutional network that proposes regions，<strong>用来proposes regions</strong>.</li>
<li>第二个模块：Fast R-CNN检测器，使用第一模块提出的regions。</li>
</ul>
</li>
<li><strong>Attention mechanisms</strong>：RPN module告诉Fast R-CNN module 往哪里看（where to look）</li>
</ul>
<h3 id="Region-Proposal-Networks"><a href="#Region-Proposal-Networks" class="headerlink" title="Region Proposal Networks"></a>Region Proposal Networks</h3><ul>
<li><p>输入：一张<strong>任意尺寸</strong>的图片。</p>
</li>
<li><p>输出：一组矩形object proposal，每个proposal都有一个score。</p>
</li>
<li><p>是一个fully convolutional network（<strong>FCN</strong>），由于我们需要在RPN和Fast RCNN之间共享权值，所以我们假设两个网络<strong>共享一组共同的卷积层</strong>。</p>
</li>
<li><p>为了生成region proposals，我们在<strong>最后一层共享卷积层</strong>输出的feature map上滑动一个微型网络。这个微型网络将输入的feature map上的nxn的空间窗口作为输入。每一个滑动窗口被映射为一个低维特征(ZF: 256-d, VGG: 512-d, 之后跟着ReLU层)。这些特征然后被送到两个sibling全连接层中——<strong>一个box-regression(reg)层</strong>和<strong>一个box-classification(cls)层。</strong></p>
</li>
<li><p>注意：因为微型网络以滑动窗口方式操作，<strong>所以完全连接层在所有空间位置上共享。</strong> 这种结构自然地通过一个n×n卷积层，后面是两个同级<strong>1×1</strong>卷积层（分别用于rpn_reg和rpn_cls）来实现。</p>
</li>
<li><p>生成region proposal的思路：<br><img src="https://ww4.sinaimg.cn/large/006tKfTcjw1fcb3fr2k40j30cq07tt9a.jpg" alt=""></p>
</li>
<li><p>rpn网络结构定义如下：</p>
<p><img src="https://ww4.sinaimg.cn/large/006tNc79ly1fdgik44luaj30pf0di40j.jpg" alt=""></p>
<p>​</p>
<p>​</p>
</li>
</ul>
<h4 id="Anchors"><a href="#Anchors" class="headerlink" title="Anchors"></a>Anchors</h4><ul>
<li>假设每个位置最大可能的proposal的数量为k，在每个sliding-window位置，同时预测几个RP：<ul>
<li><em>reg layer</em>：有4k个输出</li>
<li><em>cls layer</em>：有2k个输出，指出该每一个proposal是否是object，<strong>estimate probability of object or not object for each proposal</strong>。</li>
</ul>
</li>
<li>k个proposal相对于k个参考框（reference boxes）而参数化，我们将参考框称为<strong>anchor</strong>。</li>
<li>一个anchor位于sliding window的中间，同时关联着一个scale和aspect ration。</li>
</ul>
<h5 id="Translation-Invariant-Anchors-平移不变性"><a href="#Translation-Invariant-Anchors-平移不变性" class="headerlink" title="Translation-Invariant Anchors(平移不变性)"></a>Translation-Invariant Anchors(平移不变性)</h5><ul>
<li>如果移动了一张图像中的一个物体，这proposal应该也移动了，而且相同的函数可以预测出热议未知的proposal。MultiBox不具备如此功能</li>
<li>平移不变性可以减少模型大小。</li>
</ul>
<h5 id="Multi-Scale-Anchor-as-Regression-References"><a href="#Multi-Scale-Anchor-as-Regression-References" class="headerlink" title="Multi-Scale Anchor as Regression References"></a>Multi-Scale Anchor as Regression References</h5><ul>
<li>Two popular ways for multi-scale predictions:<ul>
<li>第一种：based on image/feature pyramids, 如：DPM and CNN-based methods。图像被resized成不同尺寸，然后为每一种尺寸计算feature maps(HOG或者deep convolutional features)。这种方法比较<strong>费时</strong>。</li>
<li>第二种：use sliding windows of multiple scales (and/or aspect ratios) on the feature maps.——<strong>filters金字塔</strong>。第二种方法经常和第一种方法联合使用</li>
</ul>
</li>
<li>本论文的方法：<strong>anchor金字塔</strong>——more cost-efficient，只依靠单尺寸的图像和feature map。</li>
<li>The design of multiscale anchors is a <strong>key </strong>component for <strong>sharing features</strong> without extra cost for addressing scales.</li>
</ul>
<h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><ul>
<li><p>为了训练RPN，我们给每个anchor设置了一个二元标签（是物体或者不是物体）</p>
</li>
<li><p>两类anchor是有<strong>正标签</strong>（is object）的：</p>
<ul>
<li>anchor/anchors with highest IoU overlap with a ground-truth box。</li>
<li>an anchor that has IoU overlap higher than 0.7 with any ground-truth box.</li>
<li>第二种方法更好检测正样本，在第二种情况下如果找不到正样本，那么使用第一种。</li>
</ul>
</li>
<li><p>如果一个anchor和任何ground-truth boxes的IoU值小于0.3，那么该anchor为<strong>负标签</strong></p>
</li>
<li><p>非正非负样本对training objective没有用。</p>
</li>
<li><p>Loss Function：</p>
<p>​</p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcjw1fcbwo1ut3hj309q02hjrj.jpg" alt=""></p>
<p>$N<em>{cls}=256,N</em>{reg}=256*9=2304,\lambda=10$，这样两个loss就可以权重基本相当了。</p>
</li>
<li><p>Bounding box regression</p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcjw1fcbyiywha8j308k0300sy.jpg" alt=""></p>
<p>这个可以考虑为从anchor box回归到附近的ground truth box。</p>
</li>
<li><p>和R-CNN和Fast R-CNN的bounding box regression方法不同的是：</p>
<ul>
<li>前两种的回归是在从任意大小RoI中提取的特征进行回归的，所以regression weights在<strong>所有尺寸中共享</strong>。</li>
<li>在我们的方法中，用于回归的特征都是同一个3x3的空间特征。考虑到变化的尺寸，有k个不同的bounding boxe回归器去学习，每一个回归器负责去学习一个尺寸一个衡重比的anchor。所以k个回归器是<strong>不共享权值的</strong>。所以<strong>得益于anchor的设计，即使特征规定，我们依旧可以去预测不同尺寸的box。</strong></li>
</ul>
</li>
</ul>
<h4 id="Training-RPNs"><a href="#Training-RPNs" class="headerlink" title="Training RPNs"></a>Training RPNs</h4><ul>
<li><strong>image-centric</strong> sampling strategy</li>
<li>mini-batch: arises from a single image that contains many positive and negative example anchors.</li>
<li>随机在一张图片中采样256个anchors来计算一个mini-batch的loss function。正负anchors = 1:1.</li>
<li>all new layers的<strong>权值初始化</strong>：高斯分布$(\mu = 0, \sigma = 0.01)$，all other layers（比如共享卷积层）用ImageNet来<strong>权值初始化</strong>。用ZF net来进行进行<strong>微调</strong>。</li>
<li><strong>学习率</strong>：0.001(60k)-&gt;0.0001(20k)</li>
<li><strong>动量</strong>：0.9</li>
<li><strong>weight decay</strong>: 0.0005</li>
</ul>
<h3 id="Sharing-Feature-for-RPN-and-Fast-R-CNN"><a href="#Sharing-Feature-for-RPN-and-Fast-R-CNN" class="headerlink" title="Sharing Feature for RPN and Fast R-CNN"></a>Sharing Feature for RPN and Fast R-CNN</h3><ul>
<li><p><strong>sharing convolutional layers between the two networks, rather than learning two separate networks</strong></p>
</li>
<li><p>三种特征共享的方法：</p>
<ul>
<li><p>Alternating training：迭代，先训练PRN，然后用proposal去训练Fast R-CNN。被Fast R-CNN微调的网络然后用来初始化PRN，以此迭代。本论文所有的实现都是使用该方法。</p>
</li>
<li><p>Approximate joint training：</p>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcjw1fccbdrnzhij30bd0b1gml.jpg" alt=""></p>
<p>RPN和Fast R-CNN融合到一个网络中进行训练。在每次SGD迭代过程中：</p>
<ul>
<li>前向传递：RPN产生region proposals，这些proposals被当做固定的、提前计算好的proposal来训练Fast R-CNN检测器。</li>
<li>反向传递：对于共享层来说，来自RPN的loss和Fast R-CNN的loss结合.</li>
<li>但是这种方法不考虑Bounding Boxes，忽略了proposal boxes的坐标也是网络的输出。所以这种方法叫做approximate</li>
</ul>
</li>
<li><p>Non-approximate joint training: 考虑Bounding Boxes。</p>
</li>
</ul>
</li>
<li><p>4-step Alternating Training:</p>
<ul>
<li>Step 1: train the RPN, initialized with an ImageNet-pre-trained model and <strong>ﬁne-tuned</strong> end-to-end for the region proposal task.</li>
<li>Step 2: train a separate detection network by Fast R-CNN using the proposals generated by the step-1 RPN. 同样使用ImageNet-pre-trained model来初始化。<strong>此时两个网络并没有共享卷积层。</strong></li>
<li>Step 3: use the detector network to initialize RPN training but we ﬁx the shared convolutional layers and only ﬁne-tune the layers unique to RPN. <strong>现在两个网络共享卷积层</strong></li>
<li>Step 4: keeping the shared convolutional layers ﬁxed, we ﬁne-tune the unique layers of Fast R-CNN.</li>
</ul>
</li>
</ul>
<h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><ul>
<li>Multi-scale与speed-accuracy之间的trade-off</li>
<li>To reduce redundancy, we adopt <strong>non-maximum suppression (NMS)</strong> on the proposal regions based on their cls scores.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Region Proposal的计算&lt;/strong&gt;是基于Region Proposal算法来假设物体位置的物体检测网络比如：&lt;strong&gt;SPPnet, Fast R-CNN&lt;/strong&gt;运行时间的瓶颈。&lt;/li&gt;
&lt;li&gt;Faster R-CNN引入了&lt;strong&gt;Region Proposal Network（RPN）&lt;/strong&gt;来和检测网络共享整个图片的卷积网络特征，因此使得region proposal几乎是&lt;strong&gt;cost free&lt;/strong&gt;的。&lt;/li&gt;
&lt;li&gt;RPN-&amp;gt;预测物体边界（object bounds）和在每一位置的分数（objectness score）&lt;/li&gt;
&lt;li&gt;通过在一个网络中共享RPN和Fast R-CNN的卷积特征来融合两者——&lt;strong&gt;使用“attention”机制。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;300 proposals pre image.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：Fast R-CNN</title>
    <link href="http://jacobkong.github.io/posts/1679631826/"/>
    <id>http://jacobkong.github.io/posts/1679631826/</id>
    <published>2016-12-07T22:32:24.000Z</published>
    <updated>2017-03-09T04:27:47.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><ul>
<li>mAP：detection quality.</li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>本文提出一种基于快速区域的卷积网络方法（快速R-CNN）用于对象检测。</li>
<li>快速R-CNN采用多项创新技术来提高训练和测试速度，同时提高检测精度。</li>
<li>采用VGG16的网络：VGG: 16 layers of 3x3 convolution interleaved with max pooling + 3 fully-connected layers</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>物体检测相对于图像分类是更复杂的，应为需要物体准确的位置。<ul>
<li>首先，必须处理许多候选对象位置（通常称为“proposal”）。</li>
<li>其次，这些候选者只提供粗略的定位，必须进行精确定位才能实现精确定位。</li>
<li>这些问题的解决方案经常损害 <strong>速度</strong> ， <strong>准确性</strong> 或 <strong>简单性</strong> 。</li>
</ul>
</li>
</ul>
<h3 id="R-CNN-and-SPPnet"><a href="#R-CNN-and-SPPnet" class="headerlink" title="R-CNN and SPPnet"></a>R-CNN and SPPnet</h3><ul>
<li>R-CNN(Region-based Convolution Network)具有几个显著的缺点：<ul>
<li>训练是一个多级管道。</li>
<li>训练在空间和时间上是昂贵的。</li>
<li>物体检测速度很慢。</li>
</ul>
</li>
<li>R-CNN是慢的，因为它对每个对象proposal执行ConvNet正向传递，而不共享计算（sharing computation）。</li>
<li>Spatial pyramid pooling networks（SPPnets），利用sharing computation对R-CNN进行了加速，但是SPPnets也具有明显的缺点，像R-CNN一样，SPPnets也需要：<ul>
<li>训练是一个多阶段流程，</li>
<li>涉及提取特征，</li>
<li>用对数损失精简网络</li>
<li>训练SVM</li>
<li>赋予边界框回归。</li>
<li>特征也需要也写入磁盘。</li>
</ul>
</li>
<li>但与R-CNN <strong>不同</strong> ，在[11]中提出的fine-tuning算法不能更新在空间金字塔池之前的卷积层。 不出所料，这种限制（固定的卷积层）限制了非常深的网络的精度。</li>
</ul>
<h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><ul>
<li>Fast R-CNN优点：</li>
</ul>
<ol>
<li>比R-CNN，SPPnet更高的检测质量（mAP）</li>
<li>训练是单阶段的，使用多任务损失（multi-task loss）</li>
<li>训练可以更新所有网络层</li>
<li>特征缓存不需要磁盘存储</li>
</ol>
<h2 id="Fast-R-CNN-architecture-and-training"><a href="#Fast-R-CNN-architecture-and-training" class="headerlink" title="Fast R-CNN architecture and training"></a>Fast R-CNN architecture and training</h2><ul>
<li><p>整体框架<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfo7na6fij30jk0efdju.jpg" alt=""></p>
</li>
<li><p>快速R-CNN网络将<strong>整个图像</strong>和<strong>一组object proposals</strong>作为输入。</p>
<ul>
<li>网络首先使用几个卷积（conv）和最大池层来处理整个图像，以产生conv feature map。</li>
<li>然后，对于每个对象proposal， <strong>感兴趣区域（RoI）池层</strong> 从特征图中抽取固定长度的特征向量。</li>
<li>每个特征向量被馈送到完全连接（fc）层序列，其最终分支成两个同级输出层：<ul>
<li>一个产生对K个对象类加上全部捕获的“背景”类的softmax概率估计(one that produces softmax probability estimates over K object classes plus a catch-all “background” class)</li>
<li>另一个对每个K对象类输出四个实数，每组4个值编码提炼定义K个类中的一个的的边界框位置。(another layer that outputs four real-valued numbers for each of the K object classes. Each set of 4 values encodes reﬁned bounding-box positions for one of the K classes.)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="The-RoI-pooling-layer"><a href="#The-RoI-pooling-layer" class="headerlink" title="The RoI pooling layer"></a>The RoI pooling layer</h3><ul>
<li>Rol pooling layer的作用主要有两个：<ul>
<li>一个是将image中的RoI定位到feature map中对应patch</li>
<li>另一个是用一个单层的SPP layer将这个feature map patch下采样为大小固定的feature再传入全连接层。</li>
</ul>
</li>
<li>RoI池层使用最大池化将任何有效的RoI区域内的特征转换成具有H×W（例如，7×7）的固定空间范围的小feature map，其中H和W是层<strong>超参数</strong> 它们独立于任何特定的RoI。</li>
<li>在本文中，RoI是conv feature map中的一个矩形窗口。</li>
<li>每个RoI由定义其左上角（r，c）及其高度和宽度（h，w）的四元组（r，c，h，w）定义。</li>
<li>RoI层仅仅是Sppnets中的spatial pyramid pooling layer的特殊形式，其中<strong>只有一个金字塔层</strong>.</li>
</ul>
<h3 id="Initializing-from-pre-trained-networks"><a href="#Initializing-from-pre-trained-networks" class="headerlink" title="Initializing from pre-trained networks"></a>Initializing from pre-trained networks</h3><ul>
<li>用了3个预训练的ImageNet网络（CaffeNet/ VGG_CNN_M_1024 /VGG16）。预训练的网络初始化Fast RCNN要经过三次变形：</li>
</ul>
<ol>
<li>最后一个max pooling层替换为RoI pooling层，设置H’和W’与第一个全连接层兼容。</li>
<li>最后一个全连接层和softmax（原本是1000个类）替换为softmax的对K+1个类别的分类层，和bounding box 回归层。</li>
<li>输入修改为两种数据：一组N个图形，R个RoI，batch size和ROI数、图像分辨率都是可变的。</li>
</ol>
<h3 id="Fine-tuning-for-detection"><a href="#Fine-tuning-for-detection" class="headerlink" title="Fine-tuning for detection"></a>Fine-tuning for detection</h3><ul>
<li><p>利用反向传播算法进行训练所有网络的权重是Fast R-CNN很重要的一个能力。</p>
</li>
<li><p>我们提出了一种更有效的训练方法，利用在训练期间的特征共享（feature sharing during training）。</p>
</li>
<li><p>在Fast R-CNN训练中， <strong>随机梯度下降（SGD）小批量分层采样</strong> ，首先通过采样N个图像，然后通过从每个图像采样 <strong>R/N个</strong> RoIs。</p>
</li>
<li><p><strong>关键的是，来自同一图像的RoI在向前和向后传递中 共享计算 和存储。</strong></p>
</li>
<li><p>此外为了分层采样，Fast R-CNN使用了一个<strong>流水线训练过程</strong>，利用一个fine-tuning阶段来联合优化一个softmax分类器和bounding box回归，而非训练一个softmax分类器，SVMs，和regression在三个独立的阶段。</p>
</li>
<li><p>Multi-task loss：</p>
<ul>
<li>两个sibling输出层：<ul>
<li>第一层：输出离散概率分布（针对每个RoIs），$p=(p_0,…,p_K)$，分别对应$K+1$个类。p是在一个全连接层的$K+1$个输出上的softmax。</li>
<li>第二层：输出bounding-box的回归偏移(bounding-box regression offsets)，针对K object classes中的每一个类，计算$t^k=(t^k_x,t^k_y,t^k_w,t^k_h)$，<strong>具体见R-CNN得补充材料，里面有很详细的介绍bounding box regression</strong>。</li>
</ul>
</li>
<li>每一个训练RoIs被标注一个ground truth类$u$，和一个ground truth bounding box 回归目标$v$。</li>
</ul>
<ul>
<li>两个loss，以下分别介绍：<ul>
<li>对于分类loss，是一个N+1路的softmax输出，其中的N是类别个数，1是背景。</li>
<li>对于回归loss，是一个4xN路输出的regressor，也就是说对于每个类别都会训练一个单独的regressor的意思，比较有意思的是，这里regressor的loss不是L2的，而是一个平滑的L1，形式如下：</li>
</ul>
</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfo7oia0ij30bg05n74f.jpg" alt=""></p>
</li>
<li><p>我们利用一个multi-task loss L 在每个被标注的RoI上来联合训练分类器和bounding box regression</p>
</li>
<li><p>Mini-batch sampling：在微调时，每个SGD的mini-batch是随机找两个图片，R为128，因此每个图上取样64个RoI。从object proposal中选25%的RoI，就是和ground-truth交叠至少为0.5的。剩下的作为背景。</p>
</li>
<li><p>Back-propagation through RoI pooling layers：</p>
<ul>
<li><p>RoI pooling层计算损失函数对每个输入变量x的偏导数，如下：</p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfo7owdcpj306q01mwee.jpg" alt=""></p>
<p>y是pooling后的输出单元，x是pooling前的输入单元，如果y由x pooling而来，则将损失L对y的偏导计入累加值，最后累加完R个RoI中的所有输出单元。下面是我理解的x、y、r的关系：</p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfoo7cuv7j30qf0ardgq.jpg" alt="20151208163114338"></p>
</li>
</ul>
</li>
</ul>
<h3 id="Scale-invariance"><a href="#Scale-invariance" class="headerlink" title="Scale invariance"></a>Scale invariance</h3><ul>
<li>这里讨论object的scale问题，就是网络对于object的scale应该是要不敏感的。这里还是引用了SPP的方法，有两种:<ul>
<li>brute force （single scale），也就是简单认为object不需要预先resize到类似的scale再传入网络，直接将image定死为某种scale，直接输入网络来训练就好了，然后期望网络自己能够学习到scale-invariance的表达。</li>
<li>image pyramids （multi scale），也就是要生成一个金字塔，然后对于object，在金字塔上找到一个大小比较接近227x227的投影版本，然后用这个版本去训练网络。</li>
</ul>
</li>
<li>可以看出，2应该比1更加好，作者也在5.2讨论了，2的表现确实比1好，但是好的不算太多，大概是1个mAP左右，但是时间要慢不少，所以作者实际采用的是第一个策略，也就是single scale。</li>
<li>这里，FRCN测试之所以比SPP快，很大原因是因为这里，因为SPP用了2，而FRCN用了1。</li>
</ul>
<h2 id="Fast-R-CNN-detection"><a href="#Fast-R-CNN-detection" class="headerlink" title="Fast R-CNN detection"></a>Fast R-CNN detection</h2><ul>
<li>大型全连接层很容易的可以通过将他们与 <strong>truncated SVD(奇异值分解)</strong> 压缩来加速计算。</li>
</ul>
<h2 id="Main-results"><a href="#Main-results" class="headerlink" title="Main results"></a>Main results</h2><ul>
<li>All Fast R-CNN results in this paper using VGG16 ﬁne-tune layers conv3 1 and up; all experments with models S and M ﬁne-tune layers conv2 and up.</li>
</ul>
<h2 id="Design-evaluation"><a href="#Design-evaluation" class="headerlink" title="Design evaluation"></a>Design evaluation</h2><h3 id="Do-we-need-more-training-data"><a href="#Do-we-need-more-training-data" class="headerlink" title="Do we need more training data?"></a>Do we need more training data?</h3><ul>
<li>在训练期间，作者做过的唯一一个数据增量的方式是水平翻转。 作者也试过将VOC12的数据也作为拓展数据加入到finetune的数据中，结果VOC07的mAP从66.9到了70.0，说明对于网络来说， <strong>数据越多就是越好的。</strong></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;知识点&quot;&gt;&lt;a href=&quot;#知识点&quot; class=&quot;headerlink&quot; title=&quot;知识点&quot;&gt;&lt;/a&gt;知识点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;mAP：detection quality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;本文提出一种基于快速区域的卷积网络方法（快速R-CNN）用于对象检测。&lt;/li&gt;
&lt;li&gt;快速R-CNN采用多项创新技术来提高训练和测试速度，同时提高检测精度。&lt;/li&gt;
&lt;li&gt;采用VGG16的网络：VGG: 16 layers of 3x3 convolution interleaved with max pooling + 3 fully-connected layers&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</title>
    <link href="http://jacobkong.github.io/posts/3054155989/"/>
    <id>http://jacobkong.github.io/posts/3054155989/</id>
    <published>2016-12-06T22:32:24.000Z</published>
    <updated>2017-01-28T09:00:55.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>现有的深卷积神经网络（CNN）需要固定尺寸（例如，224×224）的输入图像。</li>
<li>新的网络结构，称为SPP-net，可以生成固定长度的表示，而不管图像大小/规模。</li>
<li>使用SPP-net，我们从整个图像只计算一次特征图，然后在任意区域（子图像）中池特征以生成固定长度表示以训练检测器。</li>
</ul>
<a id="more"></a>
<h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><ul>
<li>在CNN的训练和测试中存在技术问题：普遍的CNN需要固定的输入图像大小（例如，224×224），其限制了输入图像的宽高比和比例。</li>
<li>Cropping</li>
<li>Warping-&gt;unwanted geometric distortion(不需要的几何失真)</li>
<li><p>那么为什么CNN需要固定输入大小？</p>
<ul>
<li>CNN主要由两部分组成：卷积层和跟随的完全连接的层。</li>
<li>事实上，卷积层不需要固定的图像大小，并且可以生成任何大小的特征图</li>
<li>另一方面，根据定义：完全连接的层需要具有固定尺寸/长度输入。所以固定尺寸完全来自于 <strong>全连接层</strong></li>
</ul>
</li>
<li><p>我们提出了一个spatial pyramid pooling（空间金字塔池化层）来去掉额昂罗固定输入的约束。</p>
</li>
<li><p>具体来说，我们在最后一个卷积层的顶部添加一个SPP层。 SPP层汇集特征并产生固定长度的输出，然后馈送到完全连接的层（或其他分类器）。</p>
</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfnxkqg21j30jl03hwf2.jpg" alt=""></p>
<ul>
<li><p>SPP对于深度CNN有着一些显著的特性：</p>
<ul>
<li>1）SPP能够生成固定长度的输出，而不管输入大小，而在以前的深度网络[3]中使用的滑动窗口池不能;</li>
<li>2）SPP使用多级空间仓，而滑动窗口池仅使用单个窗口大小。 多层池化已被证明对于对象变形是鲁棒的[15];</li>
<li>3）由于输入尺度的灵活性，SPP可以在可变尺度上提取的特征。</li>
</ul>
</li>
<li><p>实验表明，这种多尺寸训练与传统的单尺寸训练一样收敛，并导致更好的测试精度。</p>
</li>
<li><p>SPP的优点是与特定的CNN设计是正交的。</p>
</li>
<li><p>Caltech101: L. Fei-Fei, R. Fergus, and P. Perona, “Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories,” CVIU, 2007.</p>
</li>
<li><p>VOC 2007: M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results,” 2007.</p>
</li>
<li><p>但是R-CNN中的特征计算是耗时的，因为它对每个图像的数千个wraped区域的原始像素重复应用深卷积网络。而本文提出的方法可以在一整张图像上只跑一次卷积层</p>
</li>
</ul>
<h2 id="DEEP-NETWORKS-WITH-SPATIAL-PYRAMID-POOLING"><a href="#DEEP-NETWORKS-WITH-SPATIAL-PYRAMID-POOLING" class="headerlink" title="DEEP NETWORKS WITH SPATIAL PYRAMID POOLING"></a>DEEP NETWORKS WITH SPATIAL PYRAMID POOLING</h2><ul>
<li>输入图像中的这些形状激活在相应位置的feature map</li>
</ul>
<h3 id="The-Spatial-Pyramid-Pooling-Layer"><a href="#The-Spatial-Pyramid-Pooling-Layer" class="headerlink" title="The Spatial Pyramid Pooling Layer"></a>The Spatial Pyramid Pooling Layer</h3><ul>
<li>Bag-of-Words (BoW) approach-&gt;用来将生成的特征进行pool从而产生固定长度的向量。</li>
<li>空间金字塔池提高BoW，因为它可以通过在局部空间仓中汇集来 <strong>维护空间信息</strong> 。</li>
<li><p>“global pooling” operation</p>
<ul>
<li>a global average pooling</li>
<li>a global average pooling</li>
</ul>
</li>
</ul>
<h3 id="Training-the-Network"><a href="#Training-the-Network" class="headerlink" title="Training the Network"></a>Training the Network</h3><ul>
<li>Single-size training</li>
<li>Multi-size training</li>
</ul>
<h2 id="SPP-NET-FOR-IMAGE-CLASSIFICATION"><a href="#SPP-NET-FOR-IMAGE-CLASSIFICATION" class="headerlink" title="SPP-NET FOR IMAGE CLASSIFICATION"></a>SPP-NET FOR IMAGE CLASSIFICATION</h2><h2 id="SPP-NET-FOR-OBJECT-DETECTION"><a href="#SPP-NET-FOR-OBJECT-DETECTION" class="headerlink" title="SPP-NET FOR OBJECT DETECTION"></a>SPP-NET FOR OBJECT DETECTION</h2><ul>
<li>对于R-CNN来说，Feature extraction is the major timing bottleneck in testing.</li>
<li>对于我们的SPP-net来说，我们从一整张图片中值提取一次特征。</li>
<li>On the contrary, our method enables feature extraction in <strong>arbitrary windows</strong> from the deep convolutional feature maps.</li>
</ul>
<h3 id="Detection-Algorithm"><a href="#Detection-Algorithm" class="headerlink" title="Detection Algorithm"></a>Detection Algorithm</h3><p><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfnxjqg0ej30h50lfai3.jpg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;现有的深卷积神经网络（CNN）需要固定尺寸（例如，224×224）的输入图像。&lt;/li&gt;
&lt;li&gt;新的网络结构，称为SPP-net，可以生成固定长度的表示，而不管图像大小/规模。&lt;/li&gt;
&lt;li&gt;使用SPP-net，我们从整个图像只计算一次特征图，然后在任意区域（子图像）中池特征以生成固定长度表示以训练检测器。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习论文笔记：Rich feature hierarchies for accurate object detection and semantic segmentation</title>
    <link href="http://jacobkong.github.io/posts/4241353321/"/>
    <id>http://jacobkong.github.io/posts/4241353321/</id>
    <published>2016-12-05T22:32:24.000Z</published>
    <updated>2017-01-28T09:00:43.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>mAP: mean average precision，平均准确度</li>
<li><p>我们的方法结合两个关键的见解：</p>
<ul>
<li>第一：采用高容量的卷积神经网络来从上到下的进行region proposal，从而实现定位和分割物体。</li>
<li>当标记的训练数据稀缺时，可以先对辅助数据集（任务）进行受监督的预训练， 随后是基于域进行特定调整，产生显着的性能提升。</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>关于各种视觉识别任务的上一个十年的进展主要基于SIFT和HOG的使用</li>
<li><p>实现这个结果需要解决两个问题：</p>
<ul>
<li>利用深度网络将对象定位</li>
<li>仅利用少量的注释检测数据来训练训练高容量模型。</li>
</ul>
</li>
<li>我们通过在“使用区域识别”范例内操作，来解决CNN定位问题</li>
<li>在测试时，我们的方法为输入图像生成大约2000个类别无关区域提案，使用CNN从每个proposal中提取固定长度的特征向量，然后使用类别特定的线性SVM对每个区域进行分类。</li>
<li>检测中面临的第二个挑战是标记的数据不足，目前可用的数据数量不足以训练大型CNN。这个问题的常规解决方案是使用无监督预训练，然后是监督 fine-tuning。</li>
<li>我们发现，对于CNN，有很大比例的参数（94%）可以在检测精度的适度降低的情况下被去除。</li>
<li>我们证明一个简单的 <strong>边界框回归方法（bounding box regression）</strong> 显着减少误定位，这是主要的误差模式(error mode)。</li>
<li>在开发技术细节之前，我们注意到，因为R-CNN在是区域上操作，所以很自然将其扩展到语义分割（semantic segmentation）的任务。</li>
</ul>
<h2 id="Object-detection-with-R-CNN"><a href="#Object-detection-with-R-CNN" class="headerlink" title="Object detection with R-CNN"></a>Object detection with R-CNN</h2><ul>
<li><p>我们的对象检测系统由三个模块组成:</p>
<ul>
<li>首先生成类别独立(category-independent)区域proposal。 这些proposal定义了可用于检测器的候选检测集合。</li>
<li>第二个模块是大卷积神经网络，从每个区域提取固定长度的特征向量。</li>
<li>第三个模块是一类特定类型的线性SVM。</li>
</ul>
</li>
</ul>
<h3 id="Module-design"><a href="#Module-design" class="headerlink" title="Module design"></a>Module design</h3><ul>
<li><p>Region proposals: 目前有很多用来生成category-independent的region proposal的方法：</p>
<ul>
<li>Objectness</li>
<li>selective search</li>
<li>category-independent object proposals</li>
<li>constrained parametric min-cuts (CPMC)</li>
<li>multi-scale combinatorial grouping</li>
<li>detect mitotic cells by applying a CNN to regularly-spaced square crops, which are a special case of region proposals.(通过将CNN应用于规则间隔的方形作物来检测有丝分裂细胞，这是区域提案的特殊情况。)</li>
</ul>
</li>
<li><p>虽然R-CNN与特定区域建议方法无关，但我们使用选择性搜索(selective search)来实现与先前检测工作的受控比较</p>
</li>
<li><p>Feature extraction:我们从每个区域提案中提取一个4096维特征向量，特征通过前向传播对227×227 RGB图像通过 <strong>五个卷积层和两个完全连接的层</strong> 计算。</p>
</li>
<li>无论候选区域的大小或宽高比如何，我们都会将其周围的紧密边界框中的所有像素装到所需的大小(227x227像素尺寸)。</li>
</ul>
<h3 id="Test-time-detection"><a href="#Test-time-detection" class="headerlink" title="Test-time detection"></a>Test-time detection</h3><ul>
<li>在测试时，我们对测试图像运行选择性搜索以提取大约2000个区域建议（我们在所有实验中使用选择性搜索的“快速模式（fast mode）”）。</li>
<li>给定图像中的所有得分区域，我们应用贪心非最大抑制(greedy non-maximum suppression)（对于每个类独立地），如果与的饭较高的区域有重叠，且IoU大于学习到的阈值，则该拒绝区域。</li>
<li><p>Run-time analysis.两个属性使检测更高校。</p>
<ul>
<li>首先，所有CNN参数在所有类别中共享。</li>
<li>第二，CNN计算的特征向量与其他常见方法（例如具有视觉词袋编码的空间棱金字塔）相比是 <strong>低维的</strong> 。</li>
<li>唯一的类特定(class-specific)计算是特征和SVM权重之间的点积和非最大抑制。</li>
</ul>
</li>
</ul>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ul>
<li>除了用随机初始化的21路分类层（对于20个VOC类加上背景）替换CNN的ImageNet特定的1000路分类层之外，CNN架构是不变的。</li>
<li>我们将所有region proposal与一个ground-truth重叠为IoU&gt;0.5，作为该框类的阳性，其余作为阴性。</li>
<li>我们以0.001的学习速率（初始预训练速率的1/10）开始SGD，这允许精细调整进行，而不是破坏初始化。</li>
<li>一旦提取特征并应用训练标签，我们对每个类优化一个线性SVM。</li>
<li>由于训练数据太大，无法记忆，我们采用标准 <strong>hard negative mining method</strong> 。</li>
</ul>
<h3 id="Results-on-PASCAL-VOC-2010-12"><a href="#Results-on-PASCAL-VOC-2010-12" class="headerlink" title="Results on PASCAL VOC 2010-12"></a>Results on PASCAL VOC 2010-12</h3><h2 id="Visualization-ablation-and-modes-of-error"><a href="#Visualization-ablation-and-modes-of-error" class="headerlink" title="Visualization, ablation, and modes of error"></a>Visualization, ablation, and modes of error</h2><h3 id="Visualizing-learned-features"><a href="#Visualizing-learned-features" class="headerlink" title="Visualizing learned features"></a>Visualizing learned features</h3><ul>
<li>pool-5，是网络第五个也是最后一个卷基层的max-pool层的输出。（是一个max-pooling层）</li>
<li>The pool-5 feature map is 6 × 6 × 256 = 9216维。</li>
<li>忽略边界效应，每个pool-5单元在原始227×227像素输入中具有195×195像素的接收场。</li>
</ul>
<h3 id="Ablation-studies"><a href="#Ablation-studies" class="headerlink" title="Ablation studies"></a>Ablation studies</h3><ul>
<li>Fc6与pool-5全连接，为了计算特征，他它将 <strong>4096×9216的权重矩阵乘以pool-5的feature map</strong> （重新形成为9216维矢量），然后添加偏差矢量。</li>
<li>Fc7是网络的最后一层，通过将由fc 6计算的特征乘以 <strong>4096×4096</strong> 权重矩阵，并类似地添加偏置矢量和应用半波整流来实现。</li>
<li>大多数CNN的表示能力来自它的卷积层，而不是来自大得多的密集连接的层。</li>
<li>All R-CNN variants strongly outperform the three DPM baselines</li>
</ul>
<h3 id="Detection-error-analysis"><a href="#Detection-error-analysis" class="headerlink" title="Detection error analysis"></a>Detection error analysis</h3><h3 id="Bounding-box-regression"><a href="#Bounding-box-regression" class="headerlink" title="Bounding box regression"></a>Bounding box regression</h3><h2 id="Semantic-segmentation"><a href="#Semantic-segmentation" class="headerlink" title="Semantic segmentation"></a>Semantic segmentation</h2><ul>
<li>full</li>
<li>fg</li>
<li>full+fg</li>
<li>The fg strategy slightly outperforms full, indicating that the masked region shape provides a stronger signal, matching our intuition.</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>之前最好的性能系统是将多个低级图像特征与来自对象检测器和场景分类器的高级上下文组合在一起的复杂集合。</li>
<li>本文提出了一个简单和可扩展的对象检测算法，与PASCAL VOC 2012上的最佳以前的结果相比提供30％的相对改进。</li>
<li>我们推测“supervised pre-training/domain-speciﬁc ﬁne-tuning”范例将对各种数据缺乏的视觉问题高度有效。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;mAP: mean average precision，平均准确度&lt;/li&gt;
&lt;li&gt;&lt;p&gt;我们的方法结合两个关键的见解：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一：采用高容量的卷积神经网络来从上到下的进行region proposal，从而实现定位和分割物体。&lt;/li&gt;
&lt;li&gt;当标记的训练数据稀缺时，可以先对辅助数据集（任务）进行受监督的预训练， 随后是基于域进行特定调整，产生显着的性能提升。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>行人检测论文笔记：Fused DNN - A deep neural network fusion approach to fast and robust pedestrian detection</title>
    <link href="http://jacobkong.github.io/posts/2553947436/"/>
    <id>http://jacobkong.github.io/posts/2553947436/</id>
    <published>2016-12-04T22:32:24.000Z</published>
    <updated>2017-01-28T09:00:31.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="相关知识点"><a href="#相关知识点" class="headerlink" title="相关知识点"></a>相关知识点</h2><ul>
<li><strong>L1范数</strong> 也称为最小绝对偏差（LAD），最小绝对误差（LAE）。它基本上最小化目标值(Yi)和估计值(f(xi))之间的绝对差(S)的和</li>
</ul>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfqk8unvtj306d0273ye.jpg" alt=""></p>
<ul>
<li>L2范数也称为最小二乘。它基本上最小化目标值(Yi)和估计值(f(xi))之间的差(S)的平方的和</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfqk9chp0j305w01wjra.jpg" alt=""></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>所提出的网络融合架构允许多个网络的并行处理来提高速度。</li>
<li>首先是一个深度卷积网络被训练为一个物体检测器来生成所有有可能的不同尺寸和遮挡的行人候选集。</li>
<li>然后，多个深度神经网络被并行使用来之后提炼这些行人候选集。</li>
<li>我们引入基于软拒绝的网络融合方法将来自所有网络的软度量融合在一起，以产生最终置信分数。</li>
<li>此外，我们提出了一种用于将逐像素语义分割网络（ pixel-wise semantic segmentation network）集成到网络融合架构中作为行人检测器的加强的方法。</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>Tradeoff between accuracy and speed.</li>
<li>其他因素，如拥挤的场景，非人堵塞物体(non-person occluding objects)或不同的行人外观（不同的姿势或服装风格）也使这个Real-time行人检测问题具有挑战性。</li>
<li>行人检测的一般框架可以分解为：</li>
<li>region proposal generation,</li>
<li>feature extraction,</li>
<li><p>pedestrian verification</p>
</li>
<li><p>Fused Deep Neural Network(F-DNN)</p>
</li>
<li>该架构包括行人pedestrian candidiate generator，其通过训练深卷积神经网络获得以，从而具有高检测率，虽然有大的假阳性率。</li>
<li>使用深度扩展卷积和上下文聚合的并行语义分割网络[30]为候选行人提供了另一个软的信任投票，它进一步与候选生成器和分类网络融合。</li>
</ul>
<h2 id="The-Fused-Deep-Neural-Network"><a href="#The-Fused-Deep-Neural-Network" class="headerlink" title="The Fused Deep Neural Network"></a>The Fused Deep Neural Network</h2><ul>
<li>提出的网络架构包括行人候选生成器，分类网络和像素级语义分割网络。</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfqk6f5o3j30q00iumyv.jpg" alt=""></p>
<ul>
<li><p>SSD: a single shot multi-box detector(单镜头多箱检测器)，行人候选生成器是一个single shot multi-box detector（SSD）</p>
</li>
<li><p>每个行人候选者与其定位BB坐标和置信度得分相关联。</p>
</li>
<li>我们提出了一种新的网络融合方法——称为基于软拒绝的网络融合（SNF）。并非是执行接受或拒绝候选者的硬二进制分类，而是基于来自分类器的候选者的 <strong>聚合度</strong> 来提升或折扣行人候选者的置信度分数。</li>
<li>我们进一步提出了一种利用具有语义分割（SS）的上下文聚集扩展卷积网络（context aggregation dilated convolutional network with semantic segmentation）作为另一个分类器并将其集成到我们的网络融合架构中的方法。但是在速度上会变得特别慢。</li>
</ul>
<h3 id="Pedestrian-Candidate-Generator"><a href="#Pedestrian-Candidate-Generator" class="headerlink" title="Pedestrian Candidate Generator"></a>Pedestrian Candidate Generator</h3><ul>
<li>SSD是具有截断VGG16(truncated VGG16)作为基础网络的前馈卷积网络。</li>
<li>SSD的结构：</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfqk7e7e6j30pe07rgmh.jpg" alt=""></p>
<ul>
<li><p>L2归一化技术用于缩小特征量</p>
</li>
<li><p>对于大小为m×n×p的每个输出层，在每个位置处设置不同尺度和纵横比的一组默认BB。 将3×3×p个卷积内核应用于每个位置以产生关于默认BB位置的分类分数和BB位置偏移。</p>
</li>
<li>训练的目标函数是：</li>
</ul>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfqka1k2nj306u01nq2u.jpg" alt=""></p>
<h3 id="Classiﬁcation-Network-and-Soft-rejection-based-DNN-Fusion"><a href="#Classiﬁcation-Network-and-Soft-rejection-based-DNN-Fusion" class="headerlink" title="Classiﬁcation Network and Soft-rejection based DNN Fusion"></a>Classiﬁcation Network and Soft-rejection based DNN Fusion</h3><ul>
<li>分类网络由多个二元分类深层神经网络组成，这些网络在第一阶段的生成的行人候选集中训练。</li>
<li>SNF：考虑一个行人候选人和一个分类器。如果分类器对候选人有高的信任度，我们通过乘以大于1的置信因子乘以候选发生器来提高其原始分数。否则，我们以小于1的缩放因子减小其得分。我们将“置信”定义为至少为ac的分类概率。为了融合所有M个分类器，我们将候选者的原始信任得分与分类网络中所有分类器的信任缩放因子的乘积相乘。</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfqkaszbpj30q202qt9i.jpg" alt=""></p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfqkaaiiij3086024t8n.jpg" alt=""></p>
<ul>
<li>SNF背后的关键思想是，我们不直接接受或拒绝任何候选行人，而是基于分类概率的因素来扩展它们。</li>
</ul>
<h3 id="Pixel-wise-semantic-segmentation-for-object-detection-reinforcement"><a href="#Pixel-wise-semantic-segmentation-for-object-detection-reinforcement" class="headerlink" title="Pixel-wise semantic segmentation for object detection reinforcement"></a>Pixel-wise semantic segmentation for object detection reinforcement</h3><ul>
<li>为了执行密集预测，SS网络由完全卷积的VGG16网络组成，其适应于作为前端预测模块的扩展卷积，其输出被馈送到多尺度上下文聚合模块，该多尺度上下文聚合模块由完全卷积网络组成，其卷积层具有增加扩张因子。</li>
<li>输入图像被缩放并由SS网络直接处理，SS网络产生具有显示出行人类激活像素的一种颜色和显示出背景的其他颜色的二进遮罩。</li>
<li>我们使用以下策略来融合结果：如果行人像素占据候选BB区域的至少20％，我们接受候选者并保持其得分不变; 否则，我们应用SNF来缩放原始的信任分数。</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfqkbeg8wj30g002cwen.jpg" alt=""></p>
<h2 id="Experiments-and-result-analysis"><a href="#Experiments-and-result-analysis" class="headerlink" title="Experiments and result analysis"></a>Experiments and result analysis</h2><h3 id="Data-and-evaluation-settings"><a href="#Data-and-evaluation-settings" class="headerlink" title="Data and evaluation settings"></a>Data and evaluation settings</h3><h3 id="Training-details-and-results"><a href="#Training-details-and-results" class="headerlink" title="Training details and results"></a>Training details and results</h3><ul>
<li><strong>硬拒绝（Hard Rejection）</strong> 被定义为消除由任何分类器分类为假阳性的任何候选者。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;相关知识点&quot;&gt;&lt;a href=&quot;#相关知识点&quot; class=&quot;headerlink&quot; title=&quot;相关知识点&quot;&gt;&lt;/a&gt;相关知识点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;L1范数&lt;/strong&gt; 也称为最小绝对偏差（LAD），最小绝对误差（LAE）。它基本上最小化目标值(Yi)和估计值(f(xi))之间的绝对差(S)的和&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://ww2.sinaimg.cn/large/006tKfTcgw1fbfqk8unvtj306d0273ye.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L2范数也称为最小二乘。它基本上最小化目标值(Yi)和估计值(f(xi))之间的差(S)的平方的和&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://ww4.sinaimg.cn/large/006tKfTcgw1fbfqk9chp0j305w01wjra.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;所提出的网络融合架构允许多个网络的并行处理来提高速度。&lt;/li&gt;
&lt;li&gt;首先是一个深度卷积网络被训练为一个物体检测器来生成所有有可能的不同尺寸和遮挡的行人候选集。&lt;/li&gt;
&lt;li&gt;然后，多个深度神经网络被并行使用来之后提炼这些行人候选集。&lt;/li&gt;
&lt;li&gt;我们引入基于软拒绝的网络融合方法将来自所有网络的软度量融合在一起，以产生最终置信分数。&lt;/li&gt;
&lt;li&gt;此外，我们提出了一种用于将逐像素语义分割网络（ pixel-wise semantic segmentation network）集成到网络融合架构中作为行人检测器的加强的方法。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Deep Learning" scheme="http://jacobkong.github.io/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://jacobkong.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>行人检测论文笔记：Robust Real-Time Face Detection</title>
    <link href="http://jacobkong.github.io/posts/2903903730/"/>
    <id>http://jacobkong.github.io/posts/2903903730/</id>
    <published>2016-12-03T22:32:24.000Z</published>
    <updated>2017-01-28T07:37:22.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><ul>
<li>傅里叶变换的一个推论：<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrudkda8j308h01d3yg.jpg" alt=""></li>
</ul>
<p>一个时域下的复杂信号函数可以分解成多个简单信号函数的和，然后对各个子信号函数做傅里叶变换并再次求和，就求出了原信号的傅里叶变换。</p>
<ul>
<li><p>卷积定理(Convolution Theorem)：信号f和信号g的卷积的傅里叶变换，等于f、g各自的傅里叶变换的积<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfrue2zlyj304n01gdfp.jpg" alt=""></p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrufxyw0j306z0263yf.jpg" alt=""></p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfru90t5uj30jt09j75c.jpg" alt=""><br>整个过程的核心就是“（反转），移动，乘积，求和”</p>
</li>
</ul>
<a id="more"></a>
<ul>
<li><p>二维卷积</p>
<ul>
<li><p>数学定义<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfru5ml16j30f901kdfv.jpg" alt=""></p>
<p>二维卷积在图像处理中会经常遇到，图像处理中用到的大多是二维卷积的离散形式：<br><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfrum5q4ej30cv01o74a.jpg" alt=""></p>
</li>
<li>图像处理中的二维卷积，二维卷积就是一维卷积的扩展，原理差不多。核心还是（反转），移动，乘积，求和。这里二维的反转就是将卷积核沿反对角线翻转，比如：<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfru4uj25j307m02wjrd.jpg" alt=""><br>之后，卷积核在二维平面上平移，并且卷积核的每个元素与被卷积图像对应位置相乘，再求和。通过卷积核的不断移动，我们就有了一个新的图像， <strong>这个图像完全由卷积核在各个位置时的乘积求和的结果组成。</strong></li>
</ul>
</li>
<li><p>巴拿赫空间：更精确地说，巴拿赫空间是一个具有范数并对此范数完备的向量空间。</p>
</li>
<li><p>许多在数学分析中学到的无限维函数空间都是巴拿赫空间。</p>
</li>
<li><p>巴拿赫空间有两种常见的类型：“实巴拿赫空间”及“复巴拿赫空间”，分别是指将巴拿赫空间的向量空间定义于由实数或复数组成的域之上。</p>
</li>
<li><p>Overcomplete：</p>
<ul>
<li>对于Banach space X中的一个子集，如果X中的每一个元素都可以利用子集中的元素在范数内进行有限线性组合来良好近似，则该系统X是完备Complete的。</li>
<li>该完备系统是过完备（Overcomplete）的，如果从子集中移去一个元素，该系统依旧是完备的，则该系统称为过完备的。</li>
<li>在不同的研究中，比如信号处理和功能近似，过完备可以帮助研究人员达到一个更稳定、更健壮，或者相比于使用基向量更紧凑的分解。</li>
<li>如果 # (basis vector基向量)&gt;输入的维度，则我们有一个overcomplete representation.</li>
</ul>
</li>
<li><p>ROC曲线：在信号检测理论中，接收者操作特征曲线（receiver operating characteristic curve，或者叫ROC曲线）是一种坐标图式的分析工具，用于</p>
<ul>
<li>(1) 选择最佳的信号侦测模型、舍弃次佳的模型。</li>
<li>(2) 在同一模型中设定最佳阈值。</li>
<li>从 (0, 0) 到 (1,1) 的对角线将ROC空间划分为左上／右下两个区域，在这条线的 <strong>以上的点</strong> 代表了一个 <strong>好</strong> 的分类结果（胜过随机分类），而在这条线 <strong>以下的点</strong> 代表了 <strong>差</strong> 的分类结果（劣于随机分类）。</li>
<li>完美的预测是一个在左上角的点.</li>
<li>曲线下面积（AUC）：ROC曲线下方的面积，若随机抽取一个阳性样本和一个阴性样本，分类器正确判断阳性样本的值高于阴性样本之机率=AUC。简单说：AUC值越大的分类器，正确率越高。</li>
</ul>
</li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>介绍一个脸部检测框架。</li>
<li>三个贡献：</li>
<li>引入新图像表示——称为“积分图像”，其允许我们的检测器非常快速地计算所使用的特征。</li>
<li>提出一个利用AdaBost学习算法构建的简单有效的分类器，来从极大潜在特征集中选出很少的关键视觉特征。</li>
<li>在级联中组合分类器，从而快速丢弃图像的背景区域，同时在有可能的面部区域上花费更多的计算。</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><p>Haar Basis 函数：<br><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfru9jaxij308q05st8t.jpg" alt=""></p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrucbw2pj30fn07kaaj.jpg" alt=""></p>
</li>
<li><p>Integral image: 类似于计算机图形学中利用求和区域表来进行纹理映射。</p>
</li>
<li><p>Haar-like features：就是mount两个或多个区域的像素值之和的差值。</p>
</li>
<li>AdaBoost：自适应增强， 具体说来，整个Adaboost 迭代算法就3步：</li>
<li>初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。</li>
<li>训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。</li>
<li>将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。</li>
<li>级联检测过程的结构基本上是简并决策树的结构</li>
</ul>
<h2 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h2><ul>
<li>基于特征的系统操作肯定比一个基于像素的系统更更快</li>
<li>（Two-rectangle feature）两矩形特征的值是两个矩形区域内的像素之和的差</li>
<li>(Three-rectangle feature)三矩形特征计算从中心矩形中的和减去的两个外部矩形的和。</li>
<li>(Four-rectangle feature)四矩形特征计算矩形对角线对之间的差异。<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrul9r26j30aw09ydg6.jpg" alt=""></li>
</ul>
<p>矩阵特征=从灰色矩形中的像素的和中减去位于白色矩形内的像素的和。</p>
<h3 id="Integral-Image"><a href="#Integral-Image" class="headerlink" title="Integral Image"></a>Integral Image</h3><ul>
<li><p>矩阵特征可以通过图像的中间表示来快速计算，从而成为Integral Image.</p>
</li>
<li><p>积分图的每一点（x, y）的值是原图中对应位置的左上角区域的所有值得和。</p>
</li>
<li><p>积分图每一点的（x, y）值是：<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrujq2ygj30f601g749.jpg" alt=""></p>
</li>
<li><p>位置x，y处的积分图像包含x，y（包括端点）上方和左侧的像素的和:<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfruhirv4j309d021t8p.jpg" alt=""></p>
</li>
</ul>
<p>ii(x, y) is the integral image</p>
<p>i(x, y) is the original image<br><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfru76drdj30au02gwem.jpg" alt=""></p>
<p>s（x，y）是累积行和</p>
<p>s(x, −1) = 0, ii(−1, y) = 0)</p>
<p>积分图可以只遍历一次图像即可有效的计算出来</p>
<ul>
<li><p>使用积分图像，可以在四个阵列参考中计算任何矩形和。<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfru8nyo9j30c8096jrh.jpg" alt=""></p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfruav2urj30d602jwej.jpg" alt=""></p>
</li>
<li><p>Two-rectangle feature：需要6个阵列参考<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrughx0aj30ga0gr408.jpg" alt=""></p>
</li>
<li><p>Three-rectangle feature：需要8个阵列参考</p>
</li>
<li><p>Four-rectangle feature：需要9个阵列参考</p>
</li>
<li><p>在线性运算（例如f.g）的情况下，如果其逆被应用于结果，则任何可逆线性算子可以应用于f或g。</p>
</li>
<li><p>例如在卷积的情况下，如果导数运算符被应用于图像和卷积核，则结果必须被双重积分.<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfru56d7dj307g01ra9z.jpg" alt=""></p>
</li>
<li><p>如果f和g的导数稀疏（或可以这样做），卷积可以显着加速。</p>
</li>
<li><p>类似的一个认识是：如果其逆被应用于g，则一个可逆线性算子可以应用于f。<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfru68n1lj307x01tdfs.jpg" alt=""></p>
</li>
<li><p>在该框架中观察，矩形和的计算可以表示为点积i·r，其中i是图像，r是box car图像（在感兴趣的矩形内的值为1，外面是0）。 此操作可以重写：<br><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfrufk96vj306c02gwee.jpg" alt=""></p>
</li>
</ul>
<p>积分图像实际上是图像的二重积分（首先沿行，然后沿列）。</p>
<ul>
<li>矩形的二阶导数（第一行在行中，然后在列中）在矩形的角处产生四个delta函数。 第二点积的评估通过四个阵列访问来完成。</li>
</ul>
<h3 id="Feature-Discussion"><a href="#Feature-Discussion" class="headerlink" title="Feature Discussion"></a>Feature Discussion</h3><ul>
<li>与可操纵滤波器（Steerable filters）等替代方案相比，矩形特性有点原始。</li>
<li>可控滤波器对边界的详细分析，图像压缩和纹理分析的非常有用。</li>
<li>由于正交性不是这个特征集的中心，我们选择生成一个非常大而且各种各样的矩形特征集。</li>
<li>从经验上看，似乎矩形特征集提供了丰富的图像表示，能支持有效的学习。</li>
<li>为了利用积分图像技术的计算有事，考虑用更常规的方法去计算图像金字塔。</li>
<li>像大多数面部检测系统一样，我们的检测器在许多尺度扫描输入; 从以尺寸为24×24像素检测面部的基本刻度开始，在12个刻度以大于上一个的1.25倍的因子扫描384×288像素的图像。</li>
</ul>
<h2 id="Learning-Classification-Functions"><a href="#Learning-Classification-Functions" class="headerlink" title="Learning Classification Functions"></a>Learning Classification Functions</h2><ul>
<li>给定检测器的基本分辨率是24×24，矩形特征的穷尽集是相当大的，160000.</li>
<li>我们的假设是，由实验证明，非常少数的矩形特征可以组合形成一个有效的分类器。 <strong>主要的挑战是找到这些功能。</strong></li>
<li>Adaboost：将多个弱分类器组合成一个强分类器。（一个简单学习算法叫做weak learner）。</li>
<li>传统的的AdaBoost过程可以容易地解释为贪心特征选择过程。</li>
<li>一个 <strong>挑战</strong> 是将大的权重与每个良好的分类函数相关联，并将较小的权重与较差的函数相关联。</li>
<li>AdaBoost是一个用于搜索少数具有显着品种的良好“特征”的有效程序。</li>
<li>将一个weak learn限制到分类函数几何中，每一个函数都只依赖于一个单一的特征。</li>
<li>若学习宣发选择单一的能够最好分开正和负样本的矩形特征。</li>
<li>对于每一个特征，weak learner决定最优分类函数阈值，从而可以使得最少数目的样本被错分。</li>
<li>一个弱分类器h(x, f, p, θ)因此包含一个特征f，一个阈值θ，一个显示不等式方向的极性p：<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfruf310ij30ai01odfv.jpg" alt=""></li>
</ul>
<p>这里x是一个图片24*24像素的子窗口。</p>
<ul>
<li>我们使用的弱分类器（阈值单一特征）可以被视为单节点决策树。</li>
<li><strong>Boosting 算法</strong> ：T是利用每个单个特征构造的假设，最终假设是T个假设的加权线性组合，其中权重与训练误差成反比。</li>
</ul>
<ol>
<li>给定样本图片(x1, y1), (x2, y2), …, (xn, yn)。其中yi=0, 1分别为负样本和正样本。</li>
<li>初始化权值w1, i=1/(2m), 1/(2l)分别当yi=0, 1。其中m和l分别是负样本和正样本的数量。</li>
<li><p>For t=1, …, T:</p>
</li>
<li><p>归一化权重,<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrucsnrpj305e01bgli.jpg" alt=""></p>
</li>
<li><p>根据加权错误选择最佳弱分类器：<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfruidyr6j30cx01zgln.jpg" alt=""></p>
</li>
<li><p>定义 ht(x) = h(x, ft, pt,θt) 其中ft, pt, 和 θt 是εt的最小值.</p>
</li>
<li><p>更新权值：<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfruadstnj306101c0sm.jpg" alt=""><br>其中ei=0当样例xi被正确的分类，否则ei=1，并且<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfruklba8j303c017mx0.jpg" alt=""></p>
</li>
<li><p>最后的强分类器是：</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfru83b9xj30c104xjrk.jpg" alt=""><br>其中<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfru6hdsij303t0160sl.jpg" alt=""></p>
<h3 id="Learning-Discussion"><a href="#Learning-Discussion" class="headerlink" title="Learning Discussion"></a>Learning Discussion</h3></li>
</ol>
<ul>
<li><p>弱分类器选择算法过程如下：</p>
<ul>
<li>对于每个特征，根据特征值对样例进行排序。</li>
<li>该特征的AdaBoost最佳阈值可以在该排序列表上的单次通过中计算。</li>
<li>对于排序列表中的每个元素，四个和被维护和评估：</li>
<li>正实例权重T+的总和。</li>
<li>负实例权重T-的总和。</li>
<li>当前示例S+之下的正权重的和。</li>
<li>当前示例S-之下的负权重的和。</li>
</ul>
</li>
<li><p>在排序一个划分当前和上一示例之间的范围的阈值的错误是：<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrud40xsj30dk01gjrd.jpg" alt=""></p>
</li>
</ul>
<h3 id="Learning-Results"><a href="#Learning-Results" class="headerlink" title="Learning Results"></a>Learning Results</h3><ul>
<li>在现实应用中，假正例率必须接近1/1000000。</li>
<li>所选择的 <strong>第一特征</strong> 似乎集中于属性即眼睛的区域通常比鼻子和脸颊的区域更暗。</li>
<li>所选择的 <strong>第二特征</strong> 依赖于眼睛比鼻梁更暗的特性。</li>
<li>提高性能最直接技术是添加更多的特征，但这样直接导致计算时间的增加。</li>
<li>Receiver operating characteristic (ROC)曲线：<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfruhvzrpj30e30ao3yq.jpg" alt=""></li>
</ul>
<h2 id="The-Attentional-Cascade"><a href="#The-Attentional-Cascade" class="headerlink" title="The Attentional Cascade"></a>The Attentional Cascade</h2><ul>
<li>本节描述了用于构造级联的分类器的算法，其实现了提高的检测性能，同时从根本上减少了计算时间。</li>
<li>阈值越低，检测率越高，假正例率越高。</li>
<li>从双特征强分类器开始，可以通过 <strong>调整强分类器阈值</strong> 以最小化假阴性来获得有效的面部滤波器。</li>
<li>可以调整双特征分类器以50％的假阳性率来检测100％的面部。</li>
<li>整体的检测过程形式是简并决策树的形式，我们称之为“级联”。</li>
<li>在任何点上的否定结果立即导致对该子窗口的拒绝。</li>
<li>更深的分类器面临的更困难的例子,将整个ROC曲线向下推。 在给定的检测率下，较深的分类器具有相应较高的假阳性率。</li>
</ul>
<h3 id="Training-a-Cascade-of-Classifiers"><a href="#Training-a-Cascade-of-Classifiers" class="headerlink" title="Training a Cascade of Classifiers"></a>Training a Cascade of Classifiers</h3><ul>
<li><p>Given a trained cascade of classifiers, the false positive rate of the cascade is：<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfru7u7f1j303g02bjr9.jpg" alt=""></p>
</li>
<li><p>The detection rate is:</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrumlwqej303p02idfp.jpg" alt=""></p>
</li>
<li><p>The expected number of features which are evaluated is:</p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrubq3n2j307s02c0sq.jpg" alt=""></p>
</li>
<li><p>用于训练后续层的负样例集合是通过运行检测器收集通过在不包含任何面部实例的一组图像上而找到的所有错误检测来获得。</p>
</li>
<li><p>构建一个练级检测器的训练算法：</p>
</li>
</ul>
<h3 id="Simple-Experiment"><a href="#Simple-Experiment" class="headerlink" title="Simple Experiment"></a>Simple Experiment</h3><h3 id="Detector-Cascade-Discussion"><a href="#Detector-Cascade-Discussion" class="headerlink" title="Detector Cascade Discussion"></a>Detector Cascade Discussion</h3><ul>
<li>将检测器训练为分类器序列的隐藏好处是:最终检测器看到的有效数目的负样例数目可能非常大。</li>
<li>在实践中，由于我们的检测器的形式和它使用的特性是非常高效的，所以在每个尺度和位置评估我们的检测器的 <strong>摊销成本</strong> 比在整个图像中找到并分组边缘更快。</li>
</ul>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><h3 id="Training-Dataset"><a href="#Training-Dataset" class="headerlink" title="Training Dataset"></a>Training Dataset</h3><ul>
<li>事实上，包含在较大子窗口中的附加信息可以用于在检测级联中较早地拒绝non-face。</li>
</ul>
<h3 id="Structure-of-the-Detector-Cascade"><a href="#Structure-of-the-Detector-Cascade" class="headerlink" title="Structure of the Detector Cascade"></a>Structure of the Detector Cascade</h3><ul>
<li>最终的检测器是38层分级器，包括总共6060个特征。</li>
<li>级联中的第一个分类器是使用两个特征构造的，在检测100%的面部时可以拒绝50%的non-faces.</li>
<li>下一个分类器具有十个特征，并且在检测几乎100％的面部时拒绝80％的非面部。</li>
<li>接下来的两层是25个特征分类器，其后是三个50特征分类器，再之后是具有根据表2中的算法选择的各种不同数目的特征的分类器。</li>
<li>添加更多层，直到验证集上的假阳性率接近零，同时仍保持高的正确检测率。</li>
</ul>
<h3 id="Speed-of-the-Final-Detector"><a href="#Speed-of-the-Final-Detector" class="headerlink" title="Speed of the Final Detector"></a>Speed of the Final Detector</h3><ul>
<li>级联检测器的速度直接与每个被扫描的子窗口的特征数量相关。</li>
</ul>
<h3 id="Image-Processing"><a href="#Image-Processing" class="headerlink" title="Image Processing"></a>Image Processing</h3><ul>
<li>用于训练的所有示例子窗口被 <strong>方差归一</strong> 化以使不同照明条件的影响最小化。</li>
<li>可以使用 <strong>一对积分图像</strong> 来快速计算图像子窗口的方差。</li>
<li>在扫描期间，可以通过对特征值进行后乘，而不是对像素进行操作来实现图像归一化的效果。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;知识点&quot;&gt;&lt;a href=&quot;#知识点&quot; class=&quot;headerlink&quot; title=&quot;知识点&quot;&gt;&lt;/a&gt;知识点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;傅里叶变换的一个推论：&lt;br&gt;&lt;img src=&quot;https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrudkda8j308h01d3yg.jpg&quot; alt=&quot;&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个时域下的复杂信号函数可以分解成多个简单信号函数的和，然后对各个子信号函数做傅里叶变换并再次求和，就求出了原信号的傅里叶变换。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;卷积定理(Convolution Theorem)：信号f和信号g的卷积的傅里叶变换，等于f、g各自的傅里叶变换的积&lt;br&gt;&lt;img src=&quot;https://ww3.sinaimg.cn/large/006tKfTcgw1fbfrue2zlyj304n01gdfp.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrufxyw0j306z0263yf.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ww1.sinaimg.cn/large/006tKfTcgw1fbfru90t5uj30jt09j75c.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;整个过程的核心就是“（反转），移动，乘积，求和”&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Object Detection" scheme="http://jacobkong.github.io/tags/Object-Detection/"/>
    
      <category term="目标检测" scheme="http://jacobkong.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>行人检测论文笔记：How Far are We from Solving Pedestrian Detection?</title>
    <link href="http://jacobkong.github.io/posts/2397281138/"/>
    <id>http://jacobkong.github.io/posts/2397281138/</id>
    <published>2016-12-03T22:32:24.000Z</published>
    <updated>2017-03-06T06:52:51.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="文章疑问点"><a href="#文章疑问点" class="headerlink" title="文章疑问点"></a>文章疑问点</h2><ul>
<li>Human Baseline 的标准是如何确定的?</li>
<li><p>Ground-truth是什么意思？</p>
<ul>
<li>Groun-truth 指的是正确的标注（真实值）</li>
<li>在有监督学习中，数据是有标注的，以(x, t)的形式出现，其中x是输入数据，t是标注.正确的t标注是ground truth，错误的标记则不是。（也有人将所有标注数据都叫做ground truth）。</li>
</ul>
</li>
<li><p>Intersection over Union（IoU）是什么？</p>
<ul>
<li><p>Intersection over Union is an evaluation metric used to measure the accuracy of an object detector on a particular dataset.</p>
</li>
<li><p>Any algorithm that provides predicted bounding boxes as output can be evaluated using IoU.</p>
</li>
<li><p>As long as we have these two sets of bounding boxes we can apply Intersection over Union.</p>
</li>
<li><p>An Intersection over Union score &gt; 0.5 is normally considered a “good” prediction.</p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfsw1e3pcj308c06i0ss.jpg" alt=""></p>
</li>
</ul>
</li>
</ul>
<ul>
<li>FPPI: False Positive Per Image</li>
<li>Oracle Experiment: An oracle experiment is used to compare your actual system to how your system would behave if some component of it always did the right thing.</li>
</ul>
<a id="more"></a>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>调查了当前最先进的方法与“完美单帧检测器”之间的差距。</li>
<li>基于Caltech数据集创建了一个人工的基准。</li>
<li>手工聚合了顶级检测器经常出现的错误。</li>
<li><p>刻画了定位，前景 vs 背景两方面的错误</p>
<ul>
<li>针对定位错误：研究了训练集标记噪声对检测器性能的影响</li>
<li>前景 vs 背景错误：研究了convnets，讨论了哪些因素影响其性能</li>
</ul>
</li>
<li><p>提供了一个新的、更纯净的训练/测试标注集。</p>
</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>我们的结果显示localization是高置信度false positives的重要来源</li>
</ul>
<h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><h3 id="Caltech-USA-pedestrian-detection-benchmark"><a href="#Caltech-USA-pedestrian-detection-benchmark" class="headerlink" title="Caltech-USA pedestrian detection benchmark"></a>Caltech-USA pedestrian detection benchmark</h3><ul>
<li><p>最流行的数据集：Caltech-USA、KITTI</p>
<ul>
<li>Caltech-USA有2.5小时、30Hz的从LA街道的一个check里面录制的</li>
<li>一共350000个标注、覆盖2300各单一的行人</li>
<li>测试集：4024帧</li>
</ul>
</li>
<li><p>MR: miss rate</p>
</li>
</ul>
<h3 id="Filtered-channel-features-detector"><a href="#Filtered-channel-features-detector" class="headerlink" title="Filtered channel features detector"></a>Filtered channel features detector</h3><ul>
<li>截止到最近的主要会议（CVPR 15），最好的方法是 <strong>Checkerboards</strong></li>
<li>Checkerboards：是ICF的一种，ICF(Integral Channels Feature detector)</li>
<li>目前最好的执行convnets方法对底层检测建议很敏感，因此我们首先通过优化过滤的通道特征检测器来关注这些建议。</li>
<li>环境和光流可以提高检测（额外的提示）</li>
</ul>
<h2 id="Analyzing-the-state-of-the-art"><a href="#Analyzing-the-state-of-the-art" class="headerlink" title="Analyzing the state of the art"></a>Analyzing the state of the art</h2><h3 id="Are-we-reaching-saturation"><a href="#Are-we-reaching-saturation" class="headerlink" title="Are we reaching saturation?"></a>Are we reaching saturation?</h3><ul>
<li>在现在的基准上，我们还有多少提升空间？为了回答这个问题，我们提出可一个人工的基准线作为最低极限。</li>
<li>机器检测算法应该达到至少人类水平，最终超过人类水平。</li>
<li>人工基准线——为了公平比较，关注于单帧单目检测，注释器需要根据行人外表和单帧环境来注释。</li>
<li>Intersection over Union (IoU) ≥ 0.5 matching criterion。</li>
<li>在所有情况下人类基准线表现远远超过当前最好的检测器，说明对于自动方法来说，还有提升空间。</li>
</ul>
<h3 id="Failure-analysis"><a href="#Failure-analysis" class="headerlink" title="Failure analysis"></a>Failure analysis</h3><h4 id="Error-sources"><a href="#Error-sources" class="headerlink" title="Error sources"></a>Error sources</h4><ul>
<li><p>一个检测器可以有两类错误：</p>
<ul>
<li>假阳性（检测到了背景，或者很弱的定位检测）</li>
<li>假阴性（低得分率或者错过某些行人检测，检测不全）</li>
</ul>
</li>
<li><p>FP聚类成11个分类</p>
</li>
<li>FN聚类成6个分类，其中side view 和 cyclists是由于数据集偏差导致的，用这些案例的外部图像增强训练集可能是一个有效的策略。</li>
<li>对于small pedestrains，发现低像素是主要困难来源，所以合理的利用所有像素，以及周围上下文是很必要的。</li>
</ul>
<h4 id="Oracle-test-cases"><a href="#Oracle-test-cases" class="headerlink" title="Oracle test cases"></a>Oracle test cases</h4><ul>
<li>对于大多数执行最好的方法，localization和background-vs-forground误差对检测质量具有相等的影响。 他们同样重要。</li>
</ul>
<h4 id="Improved-Caltech-USA-annotations"><a href="#Improved-Caltech-USA-annotations" class="headerlink" title="Improved Caltech-USA annotations"></a>Improved Caltech-USA annotations</h4><ul>
<li>原始注释是基于跨越多个帧内插稀疏注释（interpolating sparse annotations ），并且这些稀疏注释不一定位于评估的帧上。</li>
<li><p>我们的目标是两方面：</p>
<ul>
<li>在一方面，我们希望提供对现有技术的更准确的评估，特别是适合于接近该问题的“最后20％”的评估。</li>
<li>另一方面，我们希望有训练注释，并评估改进的注释导怎么样更好的检测。</li>
</ul>
</li>
<li><p>总之，我们的新注释与人类基线在以下方面不同：训练和测试集都被注释，忽略区域和闭塞也被注释，完整的视频数据用于决策，并且允许同一图像的多个修订。</p>
</li>
</ul>
<h3 id="Improving-the-state-of-the-art"><a href="#Improving-the-state-of-the-art" class="headerlink" title="Improving the state of the art"></a>Improving the state of the art</h3><h4 id="Impact-of-training-annotations"><a href="#Impact-of-training-annotations" class="headerlink" title="Impact of training annotations"></a>Impact of training annotations</h4><ul>
<li><p>Pruning benefits:</p>
<ul>
<li>从原始到修剪注释的主要变化是删除注释错误，从修剪到新的，主要的变化是更好的对齐。</li>
<li>我们在MRN-2中看到，更强的检测器更好地受益于更好的数据，并且检测质量的最大增益来自移除注释错误。</li>
</ul>
</li>
<li><p>Alignment benefits:</p>
<ul>
<li>为了利用新的1×注释来利用9×剩余数据，我们在新的注释上训练模型，并使用该模型在9×部分上重新对准原始注释。<br> <img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfsntx4jxj30hn06zwg1.jpg" alt="Snip20161204_2"></li>
<li><p>因为新的注释更好地对齐，所以我们期望该模型能够修复原始注释中的轻微位置和缩放错误。</p>
</li>
<li><p>结果表明，使用检测器模型来提高整体数据对准确实是有效的，并且更好地对准训练数据导致更好的检测质量（在MRO和MRN中）。</p>
</li>
<li><p>使用高质量注释进行训练可提高整体检测质量，这得益于改进的对齐和减少的注释错误。</p>
</li>
</ul>
</li>
</ul>
<h4 id="Convnets-for-pedestrian-detection"><a href="#Convnets-for-pedestrian-detection" class="headerlink" title="Convnets for pedestrian detection"></a>Convnets for pedestrian detection</h4><ul>
<li><p>AlexNet 和 VGG16都在ImageNet上进行了预先训练，并使用SquaresChnFtrs建议对Caltech 10×（原始注释）进行了微调。</p>
</li>
<li><p>可以看出，VGG显着地减少了背景误差，而同时稍微增加了定位误差。</p>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfsqv4dcdj30df0c7gnt.jpg" alt="Snip20161204_3"></p>
</li>
<li><p>虽然卷积在图像分类和一般物体检测中具有很强的结果，但是当在小物体周围产生良好的局部检测分数时，它们似乎有局限性。 边界框回归（和NMS）是当前架构的一个关键因素。</p>
</li>
<li><p>表明神经网络的原始分类能力仍有改进的余地。</p>
</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li><p>相对于human baseline, there is a 10× gap still to be closed.</p>
</li>
<li><p>误差特性导致关于如何设计更好的检测器（在3.2节中提及;例如，对于人side-view的数据增加或在垂直轴上延伸检测器接收场）的具体建议。</p>
</li>
<li><p>我们通过衡量更好的注释对本地化准确性的影响，以及通过调查使用convnets来改善the background to foreground discrimination，来部分解决了一些问题。我们的研究结果表明，通过适当训练的ICF检测器可以实现显着更好的Alignment，并且，对于行人检测，Convent在localization上能力不强，但是可以通过边界框回归（bounding box regression）部分解决。 对于原始和新注释，所描述的检测方法都能达到最高性能。</p>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfsrgcdtaj30dh077jue.jpg" alt="Snip20161204_4"></p>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;文章疑问点&quot;&gt;&lt;a href=&quot;#文章疑问点&quot; class=&quot;headerlink&quot; title=&quot;文章疑问点&quot;&gt;&lt;/a&gt;文章疑问点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Human Baseline 的标准是如何确定的?&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ground-truth是什么意思？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Groun-truth 指的是正确的标注（真实值）&lt;/li&gt;
&lt;li&gt;在有监督学习中，数据是有标注的，以(x, t)的形式出现，其中x是输入数据，t是标注.正确的t标注是ground truth，错误的标记则不是。（也有人将所有标注数据都叫做ground truth）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Intersection over Union（IoU）是什么？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Intersection over Union is an evaluation metric used to measure the accuracy of an object detector on a particular dataset.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Any algorithm that provides predicted bounding boxes as output can be evaluated using IoU.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As long as we have these two sets of bounding boxes we can apply Intersection over Union.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An Intersection over Union score &amp;gt; 0.5 is normally considered a “good” prediction.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ww3.sinaimg.cn/large/006tKfTcgw1fbfsw1e3pcj308c06i0ss.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;FPPI: False Positive Per Image&lt;/li&gt;
&lt;li&gt;Oracle Experiment: An oracle experiment is used to compare your actual system to how your system would behave if some component of it always did the right thing.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Pedestrian Detection" scheme="http://jacobkong.github.io/tags/Pedestrian-Detection/"/>
    
      <category term="行人检测" scheme="http://jacobkong.github.io/tags/%E8%A1%8C%E4%BA%BA%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Review" scheme="http://jacobkong.github.io/tags/Review/"/>
    
      <category term="综述" scheme="http://jacobkong.github.io/tags/%E7%BB%BC%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>行人检测论文笔记：Ten Years of Pedestrian Detection, What Have We Learned?</title>
    <link href="http://jacobkong.github.io/posts/287090227/"/>
    <id>http://jacobkong.github.io/posts/287090227/</id>
    <published>2016-12-01T22:32:24.000Z</published>
    <updated>2017-01-28T08:21:18.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>这种新的决策林探测器在挑战性的Caltech-USA数据集上实现了当前最好的已知性能。</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>更重要的是，这是一个有着已建立的基准和评估指标的良好定义的问题。</li>
<li>用于对象检测的的主要范例有——”Viola＆Jones变体“，HOG + SVM模板，可变形部分检测器（DPM）和卷积神经网络（ConvNets）都已经被探索用于此任务。</li>
</ul>
<a id="more"></a>
<h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><ul>
<li>INRIA, ETH, TUD-Brussels, Daimler, Caltech-USA, and KITTI是使用最广的数据集。</li>
<li>INRIA：INRIA是最古老的，因此具有相对较少的图像。 然而，从不同设置（城市，海滩，山脉等）的行人的高质量注释，这是为什么它被普遍选择用来训练。</li>
<li>Daimler没有被所有的方法考虑，因为它缺乏颜色通道。</li>
<li>Daimler stereo，ETH和KITTI提供立体声信息。</li>
<li>所有数据集但INRIA都是从视频获取的，因此可以使用光流作为附加提示。</li>
<li>今天，Caltech-USA和KITTY是行人检测的主要基准。 两者都相对较大和具有挑战性。</li>
</ul>
<h2 id="Main-approaches-to-improve-pedestrian-detection"><a href="#Main-approaches-to-improve-pedestrian-detection" class="headerlink" title="Main approaches to improve pedestrian detection"></a>Main approaches to improve pedestrian detection</h2><ul>
<li><p>40+种行人检测的方法：</p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfron76nbj30gx0p8gv7.jpg" alt="Snip20161202_22"></p>
</li>
</ul>
<ul>
<li>我们不是讨论方法的个体特性，而是识别区分每种方法（表1的对号）的关键方面，并对其进行分组。 我们在下面的小节讨论这些方面。</li>
</ul>
<h3 id="Training-data"><a href="#Training-data" class="headerlink" title="Training data"></a>Training data</h3><h3 id="Solution-families"><a href="#Solution-families" class="headerlink" title="Solution families"></a>Solution families</h3><ul>
<li>总体上，我们注意到在40多种方法中，我们可以辨别三个家庭：</li>
</ul>
<ol>
<li>DPM变体（MultiResC [33]，MT-DPM [39]等）</li>
<li>深度网络（JointDeep [40]，ConvNet [13] ]等）</li>
<li><p>决策林（ChnFtrs，Roerei等）。</p>
</li>
<li><p>在表1中，我们将这些家族分别识别为DPM，DN和DF。</p>
</li>
</ol>
<h3 id="Better-classiﬁers"><a href="#Better-classiﬁers" class="headerlink" title="Better classiﬁers"></a>Better classiﬁers</h3><ul>
<li>特征和分类器之间的没有明确的界限</li>
</ul>
<h3 id="Additional-data"><a href="#Additional-data" class="headerlink" title="Additional data"></a>Additional data</h3><ul>
<li>一些方法探索在训练和测试时间利用额外的信息来改进检测。 他们考虑立体图像[45]，光流（使用以前的帧，例如MultiFtr + Motion [22]和ACF + SDt [42]），跟踪[46]或来自其他传感器 。</li>
<li>到目前为止，仅基于单个单目图像帧的方法已经能够跟上由附加信息引入的性能改进。</li>
</ul>
<h3 id="Exploiting-context"><a href="#Exploiting-context" class="headerlink" title="Exploiting context"></a>Exploiting context</h3><ul>
<li>上下文为行人检测提供了一致的改进，虽然改进的规模比额外的测试数据（§3.4）和深层架构（§3.8）要低。 大部分检测质量必须来自其他来源。</li>
</ul>
<h3 id="Deformable-parts"><a href="#Deformable-parts" class="headerlink" title="Deformable parts"></a>Deformable parts</h3><ul>
<li>对于行人检测，结果是有竞争性的，但不显着。</li>
<li>对于行人检测，除了遮挡处理的情况之外，仍然没有关于部件和部件的必要性的明确证据。</li>
</ul>
<h3 id="Multi-scale-models"><a href="#Multi-scale-models" class="headerlink" title="Multi-scale models"></a>Multi-scale models</h3><ul>
<li>最近已经注意到，不同分辨率的训练不同模型系统地将性能提高1〜2MR百分点</li>
<li>尽管不断改进，他们对最终质量的贡献是相当小的。</li>
</ul>
<h3 id="Deep-architectures"><a href="#Deep-architectures" class="headerlink" title="Deep architectures"></a>Deep architectures</h3><ul>
<li>尽管有共同的叙述，仍然没有明确的证据表明深层网络有利于行人检测的学习功能</li>
<li>最成功的方法使用这样的架构来模拟部件，遮挡和上下文的更高级别方面。 获得的结果与DPM和决策林方法相同，使得使用这样涉及的结构的 <strong>优点仍不清楚</strong> 。</li>
</ul>
<h3 id="Better-features"><a href="#Better-features" class="headerlink" title="Better features"></a>Better features</h3><ul>
<li>特征更多，具有更丰富和更高维的表示，分类任务变得更容易，从而改善结果。</li>
<li>越来越多样化的特性已经显示系统地提高性能。</li>
<li><p>尽管通过添加许多渠道的改进，顶级性能检测器仍然达到仅有10个通道：</p>
<ul>
<li>6个梯度方向，</li>
<li>1个梯度幅度</li>
<li><p>3个颜色通道</p>
</li>
<li><p>我们命名这些 <strong>HOG + LUV</strong> 。</p>
</li>
</ul>
</li>
<li><p>应当注意，还没有更好的用于行人检测的特征可以通过深度学习方法获得。</p>
</li>
<li><p>下一个科学的步骤将是开发一个更深刻的理解，什么使好的功能更哈珀，以及如何设计更好的特征。</p>
</li>
</ul>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><ul>
<li><p>基于我们在上一节中的分析，在对检测质量的影响方面，三个方面似乎是最有希望的：</p>
<ul>
<li>更好的特征（§3.9</li>
<li>附加数据（§3.4）</li>
<li>上下文信息（§3.5）</li>
</ul>
</li>
</ul>
<h3 id="Reviewing-the-eﬀect-of-features-特征的影响"><a href="#Reviewing-the-eﬀect-of-features-特征的影响" class="headerlink" title="Reviewing the eﬀect of features(特征的影响)"></a>Reviewing the eﬀect of features(特征的影响)</h3><ul>
<li>DCT: (discrete cosine transform)离散余弦变换</li>
<li>自VJ以来的许多进展可以通过使用基于定向梯度和颜色信息的更好的特征来解释。 对这些众所周知的特征（例如，基于DCT的投影）的简单调整仍然可以产生显着的改进。</li>
</ul>
<h3 id="Complementarity-of-approaches"><a href="#Complementarity-of-approaches" class="headerlink" title="Complementarity of approaches"></a>Complementarity of approaches</h3><ul>
<li>在重新审视4.1节中单帧特征的影响之后，我们现在考虑更好的特征（HOG + LUV + DCT），附加数据（通过光流）和上下文（通过人对人的交互）的互补。</li>
<li>我们的实验表明，即使从强检测器开始，添加额外的特征，流量和上下文信息在很大程度上是互补的（增加12％，而不是3 + 7 + 5％）。</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>虽然这些功能中的一些可能是由学习驱动的，但它们主要是通过尝试和错误手工制作的。</li>
<li>Better features + optical flow + context的结合可以在Caltech-USA上产生最好的检测性能。</li>
<li>The main challenge ahead seems to develop a deeper understanding of <strong>what makes good features good</strong>, so as to enable the <strong>design of even better ones</strong>.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;这种新的决策林探测器在挑战性的Caltech-USA数据集上实现了当前最好的已知性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;更重要的是，这是一个有着已建立的基准和评估指标的良好定义的问题。&lt;/li&gt;
&lt;li&gt;用于对象检测的的主要范例有——”Viola＆Jones变体“，HOG + SVM模板，可变形部分检测器（DPM）和卷积神经网络（ConvNets）都已经被探索用于此任务。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://jacobkong.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Pedestrian Detection" scheme="http://jacobkong.github.io/tags/Pedestrian-Detection/"/>
    
      <category term="行人检测" scheme="http://jacobkong.github.io/tags/%E8%A1%8C%E4%BA%BA%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Review" scheme="http://jacobkong.github.io/tags/Review/"/>
    
      <category term="综述" scheme="http://jacobkong.github.io/tags/%E7%BB%BC%E8%BF%B0/"/>
    
  </entry>
  
</feed>
