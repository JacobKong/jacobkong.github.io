<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  
  <title>行为检测论文笔记：One-shot Action Localization by Learning Sequence Matching Network | Weijie Kong&#39;s Homepage</title>

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <meta name="description" content="这是今年CVPR 2018中接受为数不多的动作时间轴定位论文中的另一篇，基于学习的时间轴动作定位方法需要大量的训练数据。 然而，这样的大规模视频数据集不仅非常难以获得而且可能因为存在无数的动作类别而不实用。 当训练样本少且罕见时，当前方法的弊端就暴露出来了。为了解决这个挑战，本文的解决方案是采用匹配网络的One-shot学习技术，并利用相关性来挖掘和定位以前没有看过类别的行为。 本文在THUMOS">
<meta property="og:type" content="article">
<meta property="og:title" content="行为检测论文笔记：One-shot Action Localization by Learning Sequence Matching Network">
<meta property="og:url" content="http://jacobkong.github.io/blog/3531717168/index.html">
<meta property="og:site_name" content="Weijie Kong's Homepage">
<meta property="og:description" content="这是今年CVPR 2018中接受为数不多的动作时间轴定位论文中的另一篇，基于学习的时间轴动作定位方法需要大量的训练数据。 然而，这样的大规模视频数据集不仅非常难以获得而且可能因为存在无数的动作类别而不实用。 当训练样本少且罕见时，当前方法的弊端就暴露出来了。为了解决这个挑战，本文的解决方案是采用匹配网络的One-shot学习技术，并利用相关性来挖掘和定位以前没有看过类别的行为。 本文在THUMOS">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tKfTcgy1frto52bt5ej30rj0e0gpp.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tKfTcly1frua9d9df8j30fs0b8q5p.jpg">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tKfTcly1fruajf2q51j30b701wwej.jpg">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tKfTcly1frub8m9vjtj309t029weg.jpg">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tKfTcly1frua9w27e6j30fs0b2di1.jpg">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tNc79ly1frujibjlk1j30bw01c748.jpg">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tNc79ly1frujixrzcpj30dk03h74i.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tNc79ly1frujj5jzvsj30cy02udfy.jpg">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tNc79ly1frujmmshe5j30df01wq2y.jpg">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tNc79ly1frujw8cs54j30fz0a1q59.jpg">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tNc79ly1frujwobo2qj30fw0ct40u.jpg">
<meta property="og:updated_time" content="2018-05-31T08:49:16.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="行为检测论文笔记：One-shot Action Localization by Learning Sequence Matching Network">
<meta name="twitter:description" content="这是今年CVPR 2018中接受为数不多的动作时间轴定位论文中的另一篇，基于学习的时间轴动作定位方法需要大量的训练数据。 然而，这样的大规模视频数据集不仅非常难以获得而且可能因为存在无数的动作类别而不实用。 当训练样本少且罕见时，当前方法的弊端就暴露出来了。为了解决这个挑战，本文的解决方案是采用匹配网络的One-shot学习技术，并利用相关性来挖掘和定位以前没有看过类别的行为。 本文在THUMOS">
<meta name="twitter:image" content="https://ws2.sinaimg.cn/large/006tKfTcgy1frto52bt5ej30rj0e0gpp.jpg">

  
    <link rel="alternate" href="/atom.xml" title="Weijie Kong&#39;s Homepage" type="application/atom+xml" />
  

  
  <!--[if lte IE 10 ]><link rel="shortcut icon" href="/favicon2.ico"><![endif]-->
  <!--[if !IE]><!-->
  <link rel="shortcut icon" href="/favicon2.png">

  <meta name="msapplication-TileImage" content="/favicon2.png"/>
  <meta name="msapplication-TileColor" content="#000000"/>

  <link rel="apple-touch-icon" href="/images/apple-touch-icon-57x57.png" />
  <link rel="apple-touch-icon" sizes="72x72" href="/images/apple-touch-icon-72x72.png" />
  <link rel="apple-touch-icon" sizes="114x114" href="/images/apple-touch-icon-114x114.png" />
  <link rel="apple-touch-icon" sizes="144x144" href="/images/apple-touch-icon-144x144.png" />

  <link rel="icon" sizes="256x256" href="/favicon2.png" />
  <!--<![endif]-->
  

  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro|Material+Icons|Raleway:400,300,700" rel="stylesheet" type="text/css" />

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

  <link rel="stylesheet" href="/css/academicons.min.css"/>
  <link rel="stylesheet" href="/css/vendors.css">
  <link rel="stylesheet" href="/css/style.css">
  
  <!-- Google Analytics -->
  <script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-89889116-1', 'auto');
  ga('send', 'pageview');

  </script>
  <!-- End Google Analytics -->



  <script src="/js/vendors.js"></script>

  <script>
    define('jquery', function () {
      return window.jQuery;
    });
  </script>


</head>
<body>

  <div class="navbar-fixed">
  <nav id="main-navbar" class="blue z-depth-1" role="navigation">
    <div class="nav-wrapper container">

      <a id="logo-container" href="/" class="brand-logo center-align">
        <span class="white-text">Weijie Kong&#39;s Homepage</span>
        
      </a>

      <ul class="right hide-on-med-and-down">
        
          <li>
            <a class="main-nav-link" href="/"><span class="white-text">Home</sapn></a>
          </li>
        
          <li>
            <a class="main-nav-link" href="/blog"><span class="white-text">Blog</sapn></a>
          </li>
        
          <li>
            <a class="main-nav-link" href="/cv/resume-zh.pdf"><span class="white-text">CV/中</sapn></a>
          </li>
        
          <li>
            <a class="main-nav-link" href="/cv"><span class="white-text">CV/En</sapn></a>
          </li>
        
      </ul>

      <a href="#" data-activates="nav-mobile" class="button-collapse">
        <i class="material-icons white-text">menu</i>
      </a>
    </div>
  </nav>
</div>

<ul id="nav-mobile" class="side-nav">
  
  <li>
    <a class="main-nav-link" href="/">Home</a>
  </li>
  
  <li>
    <a class="main-nav-link" href="/blog">Blog</a>
  </li>
  
  <li>
    <a class="main-nav-link" href="/cv/resume-zh.pdf">CV/中</a>
  </li>
  
  <li>
    <a class="main-nav-link" href="/cv">CV/En</a>
  </li>
  
</ul>


  <div id="main-container">
    
<div class="container">
  <div class="row">
    <div class="col s12">


      <article id="post-行为检测论文笔记：One-shot Action Localization" class="article article-type-post" itemscope itemprop="blogPost">

        <div class="article-inner">
          

          <header class="article-header">
          
              
  
    <h1 class="article-title header" itemprop="name">
      行为检测论文笔记：One-shot Action Localization by Learning Sequence Matching Network
    </h1>
  


          

            <div class="article-meta">
              <i class="fa fa-calendar"></i>
              <time datetime="2018-05-30T07:51:24.000Z" itemprop="datePublished">May 30, 2018</time>
            </div>
          </header>


          <div class="article-entry " itemprop="articleBody">
            
              <p>这是今年CVPR 2018中接受为数不多的动作时间轴定位论文中的另一篇，基于学习的时间轴动作定位方法需要大量的训练数据。 然而，这样的大规模视频数据集不仅非常难以获得而且可能因为存在无数的动作类别而不实用。 当训练样本少且罕见时，当前方法的弊端就暴露出来了。为了解决这个挑战，本文的解决方案是采用匹配网络的One-shot学习技术，并利用相关性来挖掘和定位以前没有看过类别的行为。 本文在THUMOS14和ActivityNet数据集上评估了本文的one-shot动作定位方法。</p>
<a id="more"></a>
<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li>现在显存的基于深度学习的行为定位的方法都采用很强的监督学习策略，需要大量的标注数据，非常耗时去收集。</li>
<li>虽然转移学习或模型预训练可能在一定程度上缓解了这个问题，但处理新的动作类别和将学习的网络模型以高数据效率适应到新场景中仍然具有挑战性。</li>
<li>在本文中，考虑one(few)-shot的动作定位学习场景：给出一个（或几个）新动作类的例子，通常每个类一个例子，我们的目标是检测未修剪视频中所有出现的每个类。</li>
<li>目前很少有工作将one-shot learning应用到检测时空特征目标中。</li>
</ul>
<h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><p>为缓解目前用来训练的动作视频数据量稀少且难以获取的问题，利用one(few)-shot learning的方法，通过少量训练样本即可达到时间轴定位以及未见过的动作的预测，提高模型的泛化性能。</p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>论文框架如下：</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1frto52bt5ej30rj0e0gpp.jpg" alt=""></p>
<ul>
<li><p>本文开发了一种新颖的<strong>元学习（meta-learning）策略</strong>，将<strong>视频序列匹配</strong>的任务级先验知识集成到学习动作定位中。 我们的一次学习策略的关键思想是动作视频的直观，结构化表示适合于用来匹配（部分）序列（matching  sequences），以及一种相似性度量，它允许我们将动作示例的标签转换为未修剪视频中的<strong>动作提议</strong>。</p>
</li>
<li><p>本文提出了一个新的Matching Network结构，首先生成许多proposal，然后这些proposal送入到三个网络组件中进行动作标签预测：</p>
<ul>
<li><p><strong>Video Encoder Network：</strong>它为每个行动建议和参考行动计算一个segment-based的动作表示，它维护行动的时间结构并用于准确定位。</p>
<ul>
<li>该网络的性能依赖于候选proposal和参考视频之间的能否良好对应，为了实现准确对齐，我们打算在行动表示中保留动作视频的时间结构。 为此，我们开发了一个利用ranking LSTM来获得segment-based的视频表示，以便将每个动作实例编码为固定长度序列的视频片段特征。</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1frua9d9df8j30fs0b8q5p.jpg" alt=""></p>
</li>
<li><p><strong>Similarity Network:</strong> 将行动建议利用相似性网络与每个参考行动（reference action）进行比较，该网络在每个时间步骤生成一组相关分数。</p>
<ul>
<li><p>类似于Matching Network，网络首先计算每个单独示例$x_i$相对于整个支持集（support set）的完整上下文嵌入</p>
</li>
<li><p>然后给定一个行动建议$\hat{x}$及其编码向量$g(x_i)$，相似性网络计算提议表示与所有示例之间的<strong>余弦距离</strong>：</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fruajf2q51j30b701wwej.jpg" alt="mage-20180531100105"></p>
</li>
<li><p>接着基于上述距离，原始匹配网络使用关注机制和投票策略将测试数据分类到支持集中的一个类中</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1frub8m9vjtj309t029weg.jpg" alt="mage-20180531102522"></p>
</li>
<li><p>我们注意到，由于支持集只由前景类组成，这种分类方法不适用于定位任务，我们还必须区分前景和背景。 因此，在本文的 one-shot action localization 架构中，我们使用相似性网络计算相关分数，并设计一个单独的标签网络来推断每个提案的类别标签（包括背景）。</p>
</li>
</ul>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1frua9w27e6j30fs0b2di1.jpg" alt=""></p>
</li>
<li><p><strong>Labeling Network:</strong> 设置不同长度的时间窗口，根据时间窗口内的encoding vector和correlation scores，第三个网络会在每个时间步骤中预测提案的动作类别标签（作为前景类别或背景之一）。</p>
<ul>
<li>标记网络直接应用于相关矩阵。 类似于通过比较余弦距离进行分类，我们通过在短暂的时间跨度上在相关矩阵上应用全连接层来比较相关矩阵的相同列的不同行，并且输出在前景和通过sigmoid激活的背景。</li>
<li>全连接网络沿时间维度滑过相关矩阵，为每个提案确定前景/背景。 在标签网络确定为前景的提议中，公式6应用于预测动作标签，如框架图的上半部分所示。</li>
<li>相关矩阵包含有关特定特定类的信息以及提案是属于背景还是属于前景。 例如，如果与一个示例的关联比其他示例的关联高得多，那么该提议很可能属于前景并且与示例具有相同的动作标签; 如果与所有例子的相关性都比较低，那么它可能属于背景。 <strong>标签网络通过训练学习这些标准。</strong></li>
<li>还要注意标签网络在某种意义上<strong>独立于动作类别</strong>，因为它应用于相关矩阵而不是每个视频的特征表示。 这意味着它学到的标准应该适用于输入来自不同类别的视频，<strong>这使得它适用于一次性预测。</strong></li>
<li>后处理阶段：我们结合多尺度 proposal-level 预测来获得frame-level的单帧级预测，并将相同标号的相邻帧分组以获得动作实例。</li>
</ul>
</li>
</ul>
</li>
<li><p>本文的定位系统是针对 one-shot action localization 而设计的，相应的 Meta Learning Formulation 可用于模型的训练。 系统的每个组成部分都是可导的，并且可以对系统进行端到端的培训。 然而，为了获得更好的初始化和性能，我们对<strong>Video Encoder Network</strong>和<strong>Similarity Network</strong>进行了预训练。 </p>
<ul>
<li><p>Meta Learning Formulation</p>
<ul>
<li>在元学习中，模型在一组训练任务的元阶段进行训练，并对另一组测试任务进行评估</li>
<li>元学习的目标是在元测试任务分布中找到最小化损失的模型</li>
</ul>
</li>
<li><p>Optimization for the Localization System</p>
<ul>
<li><p>损失函数为：</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1frujibjlk1j30bw01c748.jpg" alt=""></p>
<p>其中，两个损失函数分别如下：</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1frujixrzcpj30dk03h74i.jpg" alt="mage-20180531151204"></p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1frujj5jzvsj30cy02udfy.jpg" alt="mage-20180531151217"></p>
</li>
</ul>
</li>
<li><p>Pretraining for Video Encoder &amp; Similarity Net</p>
<ul>
<li><p>预训练两者的损失函数：</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1frujmmshe5j30df01wq2y.jpg" alt="mage-20180531151538"></p>
</li>
<li><p>利用到了一个Ranking Loss，其直观的想法是：当给予越来越多的视频内容，分类器会生成越来越置信度高的预测</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><ul>
<li><p>实验在<strong>Thumos14</strong>和<strong>ActivityNet 1.2</strong>上进行评估</p>
</li>
<li><p><strong>One-shot问题设置要求测试期间的类别不得存在在训练期间</strong>，因此在训练和测试过程中需要对两个数据集进行划分，从而达到这一要求</p>
</li>
<li><p>训练和测试阶段有很多细节，建议看论文，这里不再赘述</p>
</li>
<li><p>和强监督方法的比较，我们从表1和表2可以看出，虽然one-shot和全监督行为检测之间仍然存在性能差距，但我们的方法在以one-shot设置进行测试时，显着优于目前最先进的方法。 我们在Thumos14数据集上使用更多的训练数据，每类15个样本进一步测试我们的方法。 我们的结果表明，性能出现明显提升，而CDC只有很小的提升，这表明我们的方法相对于样本数量<strong>具有良好的可扩展性</strong>。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1frujw8cs54j30fz0a1q59.jpg" alt="mage-20180531152451"></p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1frujwobo2qj30fw0ct40u.jpg" alt="mage-20180531152512"></p>
</li>
<li><p>还有一些消融探究实验，这里也不再赘述</p>
</li>
</ul>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li>针对目前视频数据集标注费时、难以获得等痛点，作者首次将one(few)-shot的方法引入了时间轴定位，很有创新点，和弱监督学习有异曲同工之妙</li>
<li>提出了一种基于匹配网络框架的动作定位问题的<strong>元学习方法</strong>，它能够捕获<strong>任务级别（task-level）的先验知识</strong></li>
<li>后处理中的grouping策略可以要用在我之前的工作中</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>性能目前看来并不是特别理想，和全监督的方法还有差距，还有很多改进的空间，这可能需要多去了解一些<strong>one-shot learning或transfer learning</strong>等方向的论文。</li>
</ul>

            
          </div>

          

          <footer class="article-footer">
            <a data-url="http://jacobkong.github.io/blog/3531717168/" data-id="cjrnebv71004w1k2p8zslwnva" class="article-share-link">Share</a>
            
              <a href="http://jacobkong.github.io/blog/3531717168/#disqus_thread" class="article-comment-link">Comments</a>
            
            
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Action-Detection/">Action Detection</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/行为检测/">行为检测</a></li></ul>

          </footer>

        </div>

        
          
<nav id="article-nav" class="white">
  <div class="nav-wrapper">
    <ul class="row">
    
      <li class="col s6">
        <a href="/blog/3799204522/" id="article-nav-newer" class="article-nav-link-wrap grey-text text-darken-1 truncate">
          <i class="fa fa-arrow-left"></i>
          <span class="article-nav-title">论文笔记：CVPR 2018 关于行为识别论文略读笔记（一）</span>
        </a>
      </li>
    

    
      <li class="col s6">
        <a href="/blog/3697434189/" id="article-nav-older" class="article-nav-link-wrap grey-text text-darken-1 right-align truncate">
          <span class="article-nav-title">行为检测论文笔记：Rethinking the Faster R-CNN Architecture for Temporal Action Localization</span>
          <i class="fa fa-arrow-right"></i>
        </a>
      </li>
    

    </ul>
  </div>
</nav>


        
      </article>


      

      <hr />

      <section id="comments">
        <div id="disqus_thread">
          <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        </div>
      </section>
      



    </div>
  </div>
</div>


  
  <script>
    var disqus_shortname = 'jacobkong';
    
    var disqus_url = 'http://jacobkong.github.io/blog/3531717168/';
    
    (function(){
      var dsq = document.createElement('script');
      dsq.type = 'text/javascript';
      dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>

  



  </div>

  <footer class="page-footer blue lighten-2">
    <div class="container">
        <div class="row">
            <div class="col l6 s12">
                <h5><a href="http://media.pkusz.edu.cn" class="white-text" target="_blank">Digital Media R&D Center, PKU</a></h5>
            </div>

        </div>
    </div>
    <div class="footer-copyright">
      <div class="container">
            Created by <a class="orange-text text-lighten-3" href="http://hexo.io/">Weijie Kong</a>. Last
            update 2019/03/02.

            <!-- <div class="right">
              Powered by <a href="http://hexo.io/" rel="nofollow" class="white-text" target="_blank">Hexo</a>
            </div> -->
        </div>
    </div>
  </footer>

  <script src="/js/app.js"></script>

</body>
</html>
