<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  
  <title>目标检测论文笔记：R-FCN | Weijie Kong&#39;s Homepage</title>

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <meta name="description" content="Abstract
提出了一个region-based, fully convolutional的网络来准确高效的进行物体检测。
不同于Fast/Faster R-CNN，其应用了计算成本很高的每个区域子网络数百次，本论文的region-based detector是完全卷积化的，几乎一张图像上所有的计算都是共享的。
为了实现这一目标，我们提出position-sensitive score map">
<meta property="og:type" content="article">
<meta property="og:title" content="目标检测论文笔记：R-FCN">
<meta property="og:url" content="http://jacobkong.github.io/blog/3678248031/index.html">
<meta property="og:site_name" content="Weijie Kong's Homepage">
<meta property="og:description" content="Abstract
提出了一个region-based, fully convolutional的网络来准确高效的进行物体检测。
不同于Fast/Faster R-CNN，其应用了计算成本很高的每个区域子网络数百次，本论文的region-based detector是完全卷积化的，几乎一张图像上所有的计算都是共享的。
为了实现这一目标，我们提出position-sensitive score map">
<meta property="og:image" content="https://ww4.sinaimg.cn/large/006tKfTcgy1fd58i0j1icj30ql0dcmzl.jpg">
<meta property="og:image" content="https://ww3.sinaimg.cn/large/006tNc79gy1fdvwup92g7j302p06e0sp.jpg">
<meta property="og:image" content="https://ww3.sinaimg.cn/large/006tKfTcly1fd597p3an8j30vs0en40p.jpg">
<meta property="og:updated_time" content="2018-05-28T07:35:11.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="目标检测论文笔记：R-FCN">
<meta name="twitter:description" content="Abstract
提出了一个region-based, fully convolutional的网络来准确高效的进行物体检测。
不同于Fast/Faster R-CNN，其应用了计算成本很高的每个区域子网络数百次，本论文的region-based detector是完全卷积化的，几乎一张图像上所有的计算都是共享的。
为了实现这一目标，我们提出position-sensitive score map">
<meta name="twitter:image" content="https://ww4.sinaimg.cn/large/006tKfTcgy1fd58i0j1icj30ql0dcmzl.jpg">

  
    <link rel="alternate" href="/atom.xml" title="Weijie Kong&#39;s Homepage" type="application/atom+xml" />
  

  
  <!--[if lte IE 10 ]><link rel="shortcut icon" href="/favicon2.ico"><![endif]-->
  <!--[if !IE]><!-->
  <link rel="shortcut icon" href="/favicon2.png">

  <meta name="msapplication-TileImage" content="/favicon2.png"/>
  <meta name="msapplication-TileColor" content="#000000"/>

  <link rel="apple-touch-icon" href="/images/apple-touch-icon-57x57.png" />
  <link rel="apple-touch-icon" sizes="72x72" href="/images/apple-touch-icon-72x72.png" />
  <link rel="apple-touch-icon" sizes="114x114" href="/images/apple-touch-icon-114x114.png" />
  <link rel="apple-touch-icon" sizes="144x144" href="/images/apple-touch-icon-144x144.png" />

  <link rel="icon" sizes="256x256" href="/favicon2.png" />
  <!--<![endif]-->
  

  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro|Material+Icons|Raleway:400,300,700" rel="stylesheet" type="text/css" />

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

  <link rel="stylesheet" href="/css/academicons.min.css"/>
  <link rel="stylesheet" href="/css/vendors.css">
  <link rel="stylesheet" href="/css/style.css">
  
  <!-- Google Analytics -->
  <script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-89889116-1', 'auto');
  ga('send', 'pageview');

  </script>
  <!-- End Google Analytics -->



  <script src="/js/vendors.js"></script>

  <script>
    define('jquery', function () {
      return window.jQuery;
    });
  </script>


</head>
<body>

  <div class="navbar-fixed">
  <nav id="main-navbar" class="blue z-depth-1" role="navigation">
    <div class="nav-wrapper container">

      <a id="logo-container" href="/" class="brand-logo center-align">
        <span class="white-text">Weijie Kong&#39;s Homepage</span>
        
      </a>

      <ul class="right hide-on-med-and-down">
        
          <li>
            <a class="main-nav-link" href="/"><span class="white-text">Home</sapn></a>
          </li>
        
          <li>
            <a class="main-nav-link" href="/blog"><span class="white-text">Blog</sapn></a>
          </li>
        
          <li>
            <a class="main-nav-link" href="/cv/resume-zh.pdf"><span class="white-text">CV/中</sapn></a>
          </li>
        
          <li>
            <a class="main-nav-link" href="/cv"><span class="white-text">CV/En</sapn></a>
          </li>
        
      </ul>

      <a href="#" data-activates="nav-mobile" class="button-collapse">
        <i class="material-icons white-text">menu</i>
      </a>
    </div>
  </nav>
</div>

<ul id="nav-mobile" class="side-nav">
  
  <li>
    <a class="main-nav-link" href="/">Home</a>
  </li>
  
  <li>
    <a class="main-nav-link" href="/blog">Blog</a>
  </li>
  
  <li>
    <a class="main-nav-link" href="/cv/resume-zh.pdf">CV/中</a>
  </li>
  
  <li>
    <a class="main-nav-link" href="/cv">CV/En</a>
  </li>
  
</ul>


  <div id="main-container">
    
<div class="container">
  <div class="row">
    <div class="col s12">


      <article id="post-深度学习论文笔记：R-FCN" class="article article-type-post" itemscope itemprop="blogPost">

        <div class="article-inner">
          

          <header class="article-header">
          
              
  
    <h1 class="article-title header" itemprop="name">
      目标检测论文笔记：R-FCN
    </h1>
  


          

            <div class="article-meta">
              <i class="fa fa-calendar"></i>
              <time datetime="2017-02-27T07:51:24.000Z" itemprop="datePublished">Feb 27, 2017</time>
            </div>
          </header>


          <div class="article-entry " itemprop="articleBody">
            
              <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>提出了一个region-based, fully convolutional的网络来准确高效的进行物体检测。</li>
<li>不同于Fast/Faster R-CNN，其应用了计算成本很高的每个区域子网络数百次，本论文的region-based detector是完全卷积化的，几乎一张图像上所有的计算都是共享的。</li>
<li>为了实现这一目标，我们提出position-sensitive score maps，以解决在图像分类的平移不变性（translation-invariance）和物体检测中的平移可变性（translation-variance）之间的困境。</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>最近流行的用于目标检测的深度学习框架依据RoI层的不同可以分为两大subnetworks：<ul>
<li>一类是独立于RoIs的、共享的、fully convolutional的subnetwork。</li>
<li>另一类是RoI-wise的subnetwork，不共享计算。</li>
</ul>
</li>
<li>在图像分类网络中，一个convolutional subnetwork会以一个sptial pooling layer跟随着几个fully-connected  layer最为结尾，所以图像分类中的sptial pooling layer自然转化为目标检测中的RoI pooling layer。</li>
<li>ResNet和GoogleLeNets都被设计成fully convolutional的。</li>
<li>在ResNet论文中，Faster R-CNN中的RoI pooling layer被不自然的插入到两个卷积层集之间，带来了准确率的提升，<strong>但是速度由于unshared per-RoI计算降低。</strong></li>
<li>对于<strong>图像分类</strong>任务来说：更倾向于平移不变性。对于<strong>图像检测</strong>任务来说：更倾向于<strong>平移变换性</strong>。</li>
<li>假设图像分类网络中更深层的卷积层对translation不敏感，所以为了解决translation invariance和translation variance之间的困难，ResNet将RoI pooling layer插入到了卷积神经网络之间。这个区域特定的操作打破了平移不变性，并且在不同区域之间进行评估时，RoI之后的卷积层不再是平移不变的。</li>
<li>为了将translation variance结合到FCN中，我们通过使用一组专用卷积层作为FCN输出来构造一组位置敏感得分图（position-sensitive score maps）。每一个得分图将相对于相对空间位置（例如，“在对象的左边”）的位置信息进行编码。在这个FCN之上，我们附加一个位置敏感的RoI池层（position-sensitive RoI pooling layer），从这些得分图中获取信息，没有跟随的权重的（卷积/ fc）层。</li>
</ul>
<h2 id="Our-approach"><a href="#Our-approach" class="headerlink" title="Our approach"></a>Our approach</h2><ul>
<li><p>本论文的方法参考R-CNN，也是使用two-stage的目标检测策略。</p>
<ul>
<li>region proposal</li>
<li>region classification</li>
</ul>
</li>
<li><p>虽然不依赖于region proposal的目标检测方法确实存在，如SSD何YOLO，但是region-based system依旧在几个基准上保持领先的准确性。</p>
</li>
<li><p>Overall architecture of R-FCN:</p>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgy1fd58i0j1icj30ql0dcmzl.jpg" alt=""></p>
<p>用RPN来提出candidate RoIs，然后这些RoIs被应用到score maps，在RPN和R-FCN之间共享特征。</p>
<p>给定一个RoI，R-FCN架构对RoI进行分类（分为物体类别或者背景）。所有可学习权值的层都是卷积层，并且是在整张图片上计算得到的权重。<strong>最后卷积层为每个类别产生一个$k^2$个position-sensitive score maps</strong>，因此具有带有C个对象类别（背景为+1）的$k^2(C + 1)$通道的输出层。</p>
<p><img src="https://ww3.sinaimg.cn/large/006tNc79gy1fdvwup92g7j302p06e0sp.jpg" alt=""></p>
<p><strong>每一个category有一个$k^2$的score map</strong>，对于这里来说k=3，所以最后RoI pooling层产生3x3x(C+1)维的feature map。</p>
</li>
<li><p>RPN以一个position-sensitive RoI pooling layer结束，该层聚合最后卷积层的输出并产生每个RoI的分数。我们的位置敏感的RoI pooling layer进行选择性合并，each of the k × k bin aggregates responses from only one score map out of the bank of k × k score maps。利用端到端训练，这个RoI层管理最后的卷积层以学习专门的position-sensitive score maps。 </p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcly1fd597p3an8j30vs0en40p.jpg" alt=""></p>
</li>
</ul>
<h3 id="Backbone-architecture"><a href="#Backbone-architecture" class="headerlink" title="Backbone architecture"></a>Backbone architecture</h3><ul>
<li>本论文R-FCN基于ResNet-101。</li>
<li>ResNet-101具有100个卷积层，后面是global average pooling和一个1000-class的fc层。我们移去了average pooling layer and the fc layer，仅使用convolutional layer来计算feature maps。</li>
<li>我们使用ResNet-101，在ImageNet上进行预训练，ResNet-101中的最后一个卷积块是2048-d，并且我们附加随机初始化的1024-d 1×1卷积层以减小尺寸。然后，我们应用$k^2(C + 1)$通道卷积层来生成分数图，如下所述。</li>
</ul>
<h3 id="Position-sensitive-score-maps-amp-Position-sensitive-RoI-pooling"><a href="#Position-sensitive-score-maps-amp-Position-sensitive-RoI-pooling" class="headerlink" title="Position-sensitive score maps &amp; Position-sensitive RoI pooling."></a>Position-sensitive score maps &amp; Position-sensitive RoI pooling.</h3><ul>
<li><p>为了将位置信息显式编码到每个RoI中，我们将RoI矩形划分为k x k个bins。</p>
</li>
<li><p>最后的卷积层为每个类别产生的$k^2$个分数图。在第(i, j)个bin内，我们定义了一个位置敏感的RoI池化操作，从而只在第(i, j)个score map上进行池化：</p>
<script type="math/tex; mode=display">
r_c(i, j|\theta)=\sum_{(x,y)\in bin(i,j)}{z_{i,j,c}(x+x_0,y+y_0|\theta)/n}</script><ul>
<li>其中$r_c(i, j)$表示从c-th类别中得到的(i,j)-th bin的池化响应。</li>
<li>$z_{i,j,c}$是$k^2(C+1)$个score maps中的一个score map。</li>
<li>$(x_0,y_0)$表示一个RoI的左上角。</li>
<li>$n$表示这个bin中的像素的数量。</li>
<li>$\theta$表示这个网络中所有的可学习权重。</li>
<li>该pooling属于AVE，也可以用MAX。</li>
</ul>
</li>
<li><p>对每个类别k*k的score map进行平均，最后每个RoI得到一个C+1维的向量。然后求loss，它们用于评估训练期间的交叉熵损失和推理期间的RoIs排名。</p>
</li>
<li><p>bounding boxes regression。对每个RoI产生一个$4k^2$维的向量。然后通过average voting将其聚合成4维向量。</p>
</li>
<li><p>在RoI层之后没有可学习的层次，实现了几乎无成本地区的计算、加速训练和推理。</p>
</li>
</ul>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><ul>
<li><p>利用提前计算好的region proposal，很容易来端到端的R-FCN架构训练。</p>
</li>
<li><p>loss function:</p>
<script type="math/tex; mode=display">
L(s,t_{x,y,w,h})=L_{cls}(s_{c^*})+\lambda[c^*>0]L_{reg}(t,t^*)</script></li>
<li><p>本框架很容易在训练的时候使用online hard example mining (OHEM)。</p>
<ul>
<li>我们的per-RoI计算可以进行几乎cost-free的example mining。</li>
<li>在前向传播中：假设每张图片N个proposals。我们计算所有N个proposal的loss，排序，选择最高的B个RoIs。然后在选中的proposal上进行反向传播。</li>
</ul>
</li>
<li><p>decay：0.0005</p>
</li>
<li><p>momentum：0.9</p>
</li>
<li><p>single-scale training。</p>
</li>
<li><p>B=128</p>
</li>
<li><p>lr = 0.001 ~20k, 0.0001 ~ 10k</p>
</li>
<li><p>同Faster R-CNN一趟，使用4步alternating training，在训练RPN和训练R-FCN之间。</p>
</li>
</ul>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul>
<li>为公平期间，在300个RoIs上进行评估，结果之后用NMS进行处理，IoU阈值0.3</li>
</ul>
<h3 id="A-trous-and-stride"><a href="#A-trous-and-stride" class="headerlink" title="À trous and stride"></a>À trous and stride</h3><ul>
<li>将ResNet-10的有效stride从32减为16像素，提高了score map的分辨率。</li>
<li>conv4阶段之前（stride=16）的所有层都没有改变。</li>
<li>第一个conv5块儿的stride从2改为1，并且conv5阶段的卷积核都被改为hole algorithm，（Algorithme à trous）以补偿减少的步幅。</li>
<li>为了公平比较，RPN在conv4之上进行计算。从而RPN不被à trous影响。</li>
</ul>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>R-CNN已经说明了带深度网络的区域候选的有效性。R-CNN计算那些关于裁剪不正常的覆盖区域的卷积网络，并且计算在区域直接是不共享的。SPPnet，Fast R-CNN和Faster R-CNN是半卷积的（semi-convolutional），在卷积子网络中是计算共享的，在另一个子网络是各自计算独立的区域。</p>
<p>物体检测器可以被认为是全卷积模型。OverFeat 检测物体通过在convolutional feature maps上进行多尺度的窗口滑动。 在某些情况下，可以将单精度的滑动窗口改造成一个单层的卷积层。在Faster R-CNN中的RPN组件是一个全卷积检测器，用来预测是一个关于多尺寸的参考边框的实际边框。原始的RPN是class-agnostic（class无关的）。但是对应的class-specific是可应用的。</p>
<p>另一个用于物体检测的是fc layer（fully-connected）用来基于整幅图片的完整物体检测。</p>

            
          </div>

          

          <footer class="article-footer">
            <a data-url="http://jacobkong.github.io/blog/3678248031/" data-id="cjrnebv5w00481k2pefbz8omj" class="article-share-link">Share</a>
            
              <a href="http://jacobkong.github.io/blog/3678248031/#disqus_thread" class="article-comment-link">Comments</a>
            
            
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Object-Detection/">Object Detection</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/深度学习/">深度学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/目标检测/">目标检测</a></li></ul>

          </footer>

        </div>

        
          
<nav id="article-nav" class="white">
  <div class="nav-wrapper">
    <ul class="row">
    
      <li class="col s6">
        <a href="/blog/3085218970/" id="article-nav-newer" class="article-nav-link-wrap grey-text text-darken-1 truncate">
          <i class="fa fa-arrow-left"></i>
          <span class="article-nav-title">深度学习论文笔记：Deep Residual Learning for Image Recognition</span>
        </a>
      </li>
    

    
      <li class="col s6">
        <a href="/blog/3118967289/" id="article-nav-older" class="article-nav-link-wrap grey-text text-darken-1 right-align truncate">
          <span class="article-nav-title">深度学习论文笔记：SSD</span>
          <i class="fa fa-arrow-right"></i>
        </a>
      </li>
    

    </ul>
  </div>
</nav>


        
      </article>


      

      <hr />

      <section id="comments">
        <div id="disqus_thread">
          <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        </div>
      </section>
      



    </div>
  </div>
</div>


  
  <script>
    var disqus_shortname = 'jacobkong';
    
    var disqus_url = 'http://jacobkong.github.io/blog/3678248031/';
    
    (function(){
      var dsq = document.createElement('script');
      dsq.type = 'text/javascript';
      dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>

  



  </div>

  <footer class="page-footer blue lighten-2">
    <div class="container">
        <div class="row">
            <div class="col l6 s12">
                <h5><a href="http://media.pkusz.edu.cn" class="white-text" target="_blank">Digital Media R&D Center, PKU</a></h5>
            </div>

        </div>
    </div>
    <div class="footer-copyright">
      <div class="container">
            Created by <a class="orange-text text-lighten-3" href="http://hexo.io/">Weijie Kong</a>. Last
            update 2019/03/02.

            <!-- <div class="right">
              Powered by <a href="http://hexo.io/" rel="nofollow" class="white-text" target="_blank">Hexo</a>
            </div> -->
        </div>
    </div>
  </footer>

  <script src="/js/app.js"></script>

</body>
</html>
