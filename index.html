<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Pedestrian Detection, Deep Learning, Caffe, Faster RCNN, Computer Vision, 行人检测, 深度学习, 目标检测, 训练" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="欢迎来到孔伟杰（@JacobKong_Dev）的博客。 本人目前是北大信工的研一菜鸟一枚。 研究兴趣：计算机视觉|深度学习|行人检测。 欢迎大家一块儿交流！">
<meta property="og:type" content="website">
<meta property="og:title" content="JacobKong's Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="JacobKong's Blog">
<meta property="og:description" content="欢迎来到孔伟杰（@JacobKong_Dev）的博客。 本人目前是北大信工的研一菜鸟一枚。 研究兴趣：计算机视觉|深度学习|行人检测。 欢迎大家一块儿交流！">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="JacobKong's Blog">
<meta name="twitter:description" content="欢迎来到孔伟杰（@JacobKong_Dev）的博客。 本人目前是北大信工的研一菜鸟一枚。 研究兴趣：计算机视觉|深度学习|行人检测。 欢迎大家一块儿交流！">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title> JacobKong's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">JacobKong's Blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <p class="site-subtitle">路漫漫其修远兮......</p>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-search">
          <a href="/search" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-search"></i> <br />
            
            搜索
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/posts/3802700508/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Jacob Kong">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/uploads/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="JacobKong's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="JacobKong's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/posts/3802700508/" itemprop="url">
                  Caffe安装：Ubuntu 16.04+CUDA 8.0+cudnn v5.0+MATLABR2016b
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-12-17T06:32:24+08:00">
                2016-12-17
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/3802700508/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="posts/3802700508/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/posts/2903903730/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Jacob Kong">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/uploads/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="JacobKong's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="JacobKong's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/posts/2903903730/" itemprop="url">
                  行人检测论文笔记：Robust Real-Time Face Detection
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-12-15T06:32:24+08:00">
                2016-12-15
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/2903903730/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="posts/2903903730/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><ul>
<li>傅里叶变换的一个推论：<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrudkda8j308h01d3yg.jpg" alt=""></li>
</ul>
<p>一个时域下的复杂信号函数可以分解成多个简单信号函数的和，然后对各个子信号函数做傅里叶变换并再次求和，就求出了原信号的傅里叶变换。</p>
<ul>
<li>卷积定理(Convolution Theorem)：信号f和信号g的卷积的傅里叶变换，等于f、g各自的傅里叶变换的积<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfrue2zlyj304n01gdfp.jpg" alt=""></li>
</ul>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrufxyw0j306z0263yf.jpg" alt=""></p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfru90t5uj30jt09j75c.jpg" alt=""></p>
<p>整个过程的核心就是“（反转），移动，乘积，求和”</p>
<ul>
<li><p>二维卷积</p>
</li>
<li><p>数学定义<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfru5ml16j30f901kdfv.jpg" alt=""></p>
</li>
</ul>
<p>二维卷积在图像处理中会经常遇到，图像处理中用到的大多是二维卷积的离散形式：<br><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfrum5q4ej30cv01o74a.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">* 图像处理中的二维卷积，二维卷积就是一维卷积的扩展，原理差不多。核心还是（反转），移动，乘积，求和。这里二维的反转就是将卷积核沿反对角线翻转，比如：</div></pre></td></tr></table></figure>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfru4uj25j307m02wjrd.jpg" alt=""></p>
<p>之后，卷积核在二维平面上平移，并且卷积核的每个元素与被卷积图像对应位置相乘，再求和。通过卷积核的不断移动，我们就有了一个新的图像， <strong>这个图像完全由卷积核在各个位置时的乘积求和的结果组成。</strong></p>
<ul>
<li>巴拿赫空间：更精确地说，巴拿赫空间是一个具有范数并对此范数完备的向量空间。</li>
<li>许多在数学分析中学到的无限维函数空间都是巴拿赫空间。</li>
<li><p>巴拿赫空间有两种常见的类型：“实巴拿赫空间”及“复巴拿赫空间”，分别是指将巴拿赫空间的向量空间定义于由实数或复数组成的域之上。</p>
</li>
<li><p>Overcomplete：</p>
</li>
<li>对于Banach space X中的一个子集，如果X中的每一个元素都可以利用子集中的元素在范数内进行有限线性组合来良好近似，则该系统X是完备Complete的。</li>
<li>该完备系统是过完备（Overcomplete）的，如果从子集中移去一个元素，该系统依旧是完备的，则该系统称为过完备的。</li>
<li>在不同的研究中，比如信号处理和功能近似，过完备可以帮助研究人员达到一个更稳定、更健壮，或者相比于使用基向量更紧凑的分解。</li>
<li><p>如果 # (basis vector基向量)&gt;输入的维度，则我们有一个overcomplete representation.</p>
</li>
<li><p>ROC曲线：在信号检测理论中，接收者操作特征曲线（receiver operating characteristic curve，或者叫ROC曲线）是一种坐标图式的分析工具，用于</p>
</li>
<li>(1) 选择最佳的信号侦测模型、舍弃次佳的模型。</li>
<li>(2) 在同一模型中设定最佳阈值。</li>
<li>从 (0, 0) 到 (1,1) 的对角线将ROC空间划分为左上／右下两个区域，在这条线的 <strong>以上的点</strong> 代表了一个 <strong>好</strong> 的分类结果（胜过随机分类），而在这条线 <strong>以下的点</strong> 代表了 <strong>差</strong> 的分类结果（劣于随机分类）。</li>
<li><p>完美的预测是一个在左上角的点.</p>
</li>
<li><p>曲线下面积（AUC）：ROC曲线下方的面积，若随机抽取一个阳性样本和一个阴性样本，分类器正确判断阳性样本的值高于阴性样本之机率=AUC。简单说：AUC值越大的分类器，正确率越高。</p>
</li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>介绍一个脸部检测框架。</li>
<li>三个贡献：</li>
<li>引入新图像表示——称为“积分图像”，其允许我们的检测器非常快速地计算所使用的特征。</li>
<li>提出一个利用AdaBost学习算法构建的简单有效的分类器，来从极大潜在特征集中选出很少的关键视觉特征。</li>
<li>在级联中组合分类器，从而快速丢弃图像的背景区域，同时在有可能的面部区域上花费更多的计算。</li>
</ul>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul>
<li>Haar Basis 函数：<br><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfru9jaxij308q05st8t.jpg" alt=""></li>
</ul>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrucbw2pj30fn07kaaj.jpg" alt=""></p>
<ul>
<li><p>Integral image: 类似于计算机图形学中利用求和区域表来进行纹理映射。</p>
</li>
<li><p>Haar-like features：就是mount两个或多个区域的像素值之和的差值。</p>
</li>
<li>AdaBoost：自适应增强， 具体说来，整个Adaboost 迭代算法就3步：</li>
<li>初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。</li>
<li>训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。</li>
<li><p>将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。</p>
</li>
<li><p>级联检测过程的结构基本上是简并决策树的结构</p>
</li>
</ul>
<h2 id="2-Features"><a href="#2-Features" class="headerlink" title="2. Features"></a>2. Features</h2><ul>
<li>基于特征的系统操作肯定比一个基于像素的系统更更快</li>
<li>（Two-rectangle feature）两矩形特征的值是两个矩形区域内的像素之和的差</li>
<li>(Three-rectangle feature)三矩形特征计算从中心矩形中的和减去的两个外部矩形的和。</li>
<li>(Four-rectangle feature)四矩形特征计算矩形对角线对之间的差异。<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrul9r26j30aw09ydg6.jpg" alt=""></li>
</ul>
<p>矩阵特征=从灰色矩形中的像素的和中减去位于白色矩形内的像素的和。</p>
<h3 id="2-1-Integral-Image"><a href="#2-1-Integral-Image" class="headerlink" title="2.1 Integral Image"></a>2.1 Integral Image</h3><ul>
<li><p>矩阵特征可以通过图像的中间表示来快速计算，从而成为Integral Image.</p>
</li>
<li><p>积分图的每一点（x, y）的值是原图中对应位置的左上角区域的所有值得和。</p>
</li>
<li><p>积分图每一点的（x, y）值是：<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrujq2ygj30f601g749.jpg" alt=""></p>
</li>
<li><p>位置x，y处的积分图像包含x，y（包括端点）上方和左侧的像素的和:<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfruhirv4j309d021t8p.jpg" alt=""></p>
</li>
</ul>
<p>ii(x, y) is the integral image</p>
<p>i(x, y) is the original image<br><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfru76drdj30au02gwem.jpg" alt=""></p>
<p>s（x，y）是累积行和</p>
<p>s(x, −1) = 0, ii(−1, y) = 0)</p>
<p>积分图可以只遍历一次图像即可有效的计算出来</p>
<ul>
<li>使用积分图像，可以在四个阵列参考中计算任何矩形和。<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfru8nyo9j30c8096jrh.jpg" alt=""></li>
</ul>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfruav2urj30d602jwej.jpg" alt=""></p>
<ul>
<li><p>Two-rectangle feature：需要6个阵列参考<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrughx0aj30ga0gr408.jpg" alt=""></p>
</li>
<li><p>Three-rectangle feature：需要8个阵列参考</p>
</li>
<li><p>Four-rectangle feature：需要9个阵列参考</p>
</li>
<li><p>在线性运算（例如f.g）的情况下，如果其逆被应用于结果，则任何可逆线性算子可以应用于f或g。</p>
</li>
<li><p>例如在卷积的情况下，如果导数运算符被应用于图像和卷积核，则结果必须被双重积分.<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfru56d7dj307g01ra9z.jpg" alt=""></p>
</li>
<li><p>如果f和g的导数稀疏（或可以这样做），卷积可以显着加速。</p>
</li>
<li><p>类似的一个认识是：如果其逆被应用于g，则一个可逆线性算子可以应用于f。<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfru68n1lj307x01tdfs.jpg" alt=""></p>
</li>
<li><p>在该框架中观察，矩形和的计算可以表示为点积i·r，其中i是图像，r是box car图像（在感兴趣的矩形内的值为1，外面是0）。 此操作可以重写：<br><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfrufk96vj306c02gwee.jpg" alt=""></p>
</li>
</ul>
<p>积分图像实际上是图像的二重积分（首先沿行，然后沿列）。</p>
<ul>
<li>矩形的二阶导数（第一行在行中，然后在列中）在矩形的角处产生四个delta函数。 第二点积的评估通过四个阵列访问来完成。</li>
</ul>
<h3 id="2-2-Feature-Discussion"><a href="#2-2-Feature-Discussion" class="headerlink" title="2.2. Feature Discussion"></a>2.2. Feature Discussion</h3><ul>
<li>与可操纵滤波器（Steerable filters）等替代方案相比，矩形特性有点原始。</li>
<li>可控滤波器对边界的详细分析，图像压缩和纹理分析的非常有用。</li>
<li>由于正交性不是这个特征集的中心，我们选择生成一个非常大而且各种各样的矩形特征集。</li>
<li>从经验上看，似乎矩形特征集提供了丰富的图像表示，能支持有效的学习。</li>
<li>为了利用积分图像技术的计算有事，考虑用更常规的方法去计算图像金字塔。</li>
<li>像大多数面部检测系统一样，我们的检测器在许多尺度扫描输入; 从以尺寸为24×24像素检测面部的基本刻度开始，在12个刻度以大于上一个的1.25倍的因子扫描384×288像素的图像。</li>
</ul>
<h2 id="3-Learning-Classification-Functions"><a href="#3-Learning-Classification-Functions" class="headerlink" title="3. Learning Classification Functions"></a>3. Learning Classification Functions</h2><ul>
<li>给定检测器的基本分辨率是24×24，矩形特征的穷尽集是相当大的，160000.</li>
<li>我们的假设是，由实验证明，非常少数的矩形特征可以组合形成一个有效的分类器。 <strong>主要的挑战是找到这些功能。</strong></li>
<li>Adaboost：将多个弱分类器组合成一个强分类器。（一个简单学习算法叫做weak learner）。</li>
<li>传统的的AdaBoost过程可以容易地解释为贪心特征选择过程。</li>
<li>一个 <strong>挑战</strong> 是将大的权重与每个良好的分类函数相关联，并将较小的权重与较差的函数相关联。</li>
<li>AdaBoost是一个用于搜索少数具有显着品种的良好“特征”的有效程序。</li>
<li>将一个weak learn限制到分类函数几何中，每一个函数都只依赖于一个单一的特征。</li>
<li>若学习宣发选择单一的能够最好分开正和负样本的矩形特征。</li>
<li>对于每一个特征，weak learner决定最优分类函数阈值，从而可以使得最少数目的样本被错分。</li>
<li>一个弱分类器h(x, f, p, θ)因此包含一个特征f，一个阈值θ，一个显示不等式方向的极性p：<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfruf310ij30ai01odfv.jpg" alt=""></li>
</ul>
<p>这里x是一个图片24*24像素的子窗口。</p>
<ul>
<li>我们使用的弱分类器（阈值单一特征）可以被视为单节点决策树。</li>
<li><strong>Boosting 算法</strong> ：T是利用每个单个特征构造的假设，最终假设是T个假设的加权线性组合，其中权重与训练误差成反比。</li>
</ul>
<ol>
<li>给定样本图片(x1, y1), (x2, y2), …, (xn, yn)。其中yi=0, 1分别为负样本和正样本。</li>
<li>初始化权值w1, i=1/(2m), 1/(2l)分别当yi=0, 1。其中m和l分别是负样本和正样本的数量。</li>
<li><p>For t=1, …, T:</p>
</li>
<li><p>归一化权重,<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrucsnrpj305e01bgli.jpg" alt=""></p>
</li>
<li><p>根据加权错误选择最佳弱分类器：<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfruidyr6j30cx01zgln.jpg" alt=""></p>
</li>
<li><p>定义 ht(x) = h(x, ft, pt,θt) 其中ft, pt, 和 θt 是εt的最小值.</p>
</li>
<li><p>更新权值：<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfruadstnj306101c0sm.jpg" alt=""></p>
</li>
</ol>
<p>其中ei=0当样例xi被正确的分类，否则ei=1，并且<br><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfruklba8j303c017mx0.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">4. 最后的强分类器是：</div></pre></td></tr></table></figure>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfru83b9xj30c104xjrk.jpg" alt=""></p>
<p>其中<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfru6hdsij303t0160sl.jpg" alt=""></p>
<h3 id="3-1-Learning-Discussion"><a href="#3-1-Learning-Discussion" class="headerlink" title="3.1. Learning Discussion"></a>3.1. Learning Discussion</h3><ul>
<li><p>弱分类器选择算法过程如下：</p>
</li>
<li><p>对于每个特征，根据特征值对样例进行排序。</p>
</li>
<li>该特征的AdaBoost最佳阈值可以在该排序列表上的单次通过中计算。</li>
<li>对于排序列表中的每个元素，四个和被维护和评估：</li>
<li>正实例权重T+的总和。</li>
<li>负实例权重T-的总和。</li>
<li>当前示例S+之下的正权重的和。</li>
<li><p>当前示例S-之下的负权重的和。</p>
</li>
<li><p>在排序一个划分当前和上一示例之间的范围的阈值的错误是：<br><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrud40xsj30dk01gjrd.jpg" alt=""></p>
</li>
</ul>
<h3 id="3-2-Learning-Results"><a href="#3-2-Learning-Results" class="headerlink" title="3.2. Learning Results"></a>3.2. Learning Results</h3><ul>
<li>在现实应用中，假正例率必须接近1/1000000。</li>
<li>所选择的 <strong>第一特征</strong> 似乎集中于属性即眼睛的区域通常比鼻子和脸颊的区域更暗。</li>
<li>所选择的 <strong>第二特征</strong> 依赖于眼睛比鼻梁更暗的特性。</li>
<li>提高性能最直接技术是添加更多的特征，但这样直接导致计算时间的增加。</li>
<li>Receiver operating characteristic (ROC)曲线：<br><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfruhvzrpj30e30ao3yq.jpg" alt=""></li>
</ul>
<h2 id="4-The-Attentional-Cascade"><a href="#4-The-Attentional-Cascade" class="headerlink" title="4. The Attentional Cascade"></a>4. The Attentional Cascade</h2><ul>
<li>本节描述了用于构造级联的分类器的算法，其实现了提高的检测性能，同时从根本上减少了计算时间。</li>
<li>阈值越低，检测率越高，假正例率越高。</li>
<li>从双特征强分类器开始，可以通过 <strong>调整强分类器阈值</strong> 以最小化假阴性来获得有效的面部滤波器。</li>
<li>可以调整双特征分类器以50％的假阳性率来检测100％的面部。</li>
<li>整体的检测过程形式是简并决策树的形式，我们称之为“级联”。</li>
<li>在任何点上的否定结果立即导致对该子窗口的拒绝。</li>
<li>更深的分类器面临的更困难的例子,将整个ROC曲线向下推。 在给定的检测率下，较深的分类器具有相应较高的假阳性率。</li>
</ul>
<h3 id="4-1-Training-a-Cascade-of-Classifiers"><a href="#4-1-Training-a-Cascade-of-Classifiers" class="headerlink" title="4.1. Training a Cascade of Classifiers"></a>4.1. Training a Cascade of Classifiers</h3><ul>
<li>Given a trained cascade of classifiers, the false positive rate of the cascade is：</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfru7u7f1j303g02bjr9.jpg" alt=""></p>
<ul>
<li>The detection rate is:</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfrumlwqej303p02idfp.jpg" alt=""></p>
<ul>
<li>The expected number of features which are evaluated is:</li>
</ul>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfrubq3n2j307s02c0sq.jpg" alt=""></p>
<ul>
<li><p>用于训练后续层的负样例集合是通过运行检测器收集通过在不包含任何面部实例的一组图像上而找到的所有错误检测来获得。</p>
</li>
<li><p>构建一个练级检测器的训练算法：</p>
</li>
</ul>
<h3 id="4-2-Simple-Experiment"><a href="#4-2-Simple-Experiment" class="headerlink" title="4.2. Simple Experiment"></a>4.2. Simple Experiment</h3><h3 id="4-3-Detector-Cascade-Discussion"><a href="#4-3-Detector-Cascade-Discussion" class="headerlink" title="4.3. Detector Cascade Discussion"></a>4.3. Detector Cascade Discussion</h3><ul>
<li>将检测器训练为分类器序列的隐藏好处是:最终检测器看到的有效数目的负样例数目可能非常大。</li>
<li>在实践中，由于我们的检测器的形式和它使用的特性是非常高效的，所以在每个尺度和位置评估我们的检测器的 <strong>摊销成本</strong> 比在整个图像中找到并分组边缘更快。</li>
</ul>
<h2 id="5-Results"><a href="#5-Results" class="headerlink" title="5. Results"></a>5. Results</h2><h3 id="5-1-Training-Dataset"><a href="#5-1-Training-Dataset" class="headerlink" title="5.1. Training Dataset"></a>5.1. Training Dataset</h3><ul>
<li>事实上，包含在较大子窗口中的附加信息可以用于在检测级联中较早地拒绝non-face。</li>
</ul>
<h3 id="5-2-Structure-of-the-Detector-Cascade"><a href="#5-2-Structure-of-the-Detector-Cascade" class="headerlink" title="5.2. Structure of the Detector Cascade"></a>5.2. Structure of the Detector Cascade</h3><ul>
<li>最终的检测器是38层分级器，包括总共6060个特征。</li>
<li>级联中的第一个分类器是使用两个特征构造的，在检测100%的面部时可以拒绝50%的non-faces.</li>
<li>下一个分类器具有十个特征，并且在检测几乎100％的面部时拒绝80％的非面部。</li>
<li>接下来的两层是25个特征分类器，其后是三个50特征分类器，再之后是具有根据表2中的算法选择的各种不同数目的特征的分类器。</li>
<li>添加更多层，直到验证集上的假阳性率接近零，同时仍保持高的正确检测率。</li>
</ul>
<h3 id="5-3-Speed-of-the-Final-Detector"><a href="#5-3-Speed-of-the-Final-Detector" class="headerlink" title="5.3. Speed of the Final Detector"></a>5.3. Speed of the Final Detector</h3><ul>
<li>级联检测器的速度直接与每个被扫描的子窗口的特征数量相关。</li>
</ul>
<h3 id="5-4-Image-Processing"><a href="#5-4-Image-Processing" class="headerlink" title="5.4. Image Processing"></a>5.4. Image Processing</h3><ul>
<li>用于训练的所有示例子窗口被 <strong>方差归一</strong> 化以使不同照明条件的影响最小化。</li>
<li>可以使用 <strong>一对积分图像</strong> 来快速计算图像子窗口的方差。</li>
<li>在扫描期间，可以通过对特征值进行后乘，而不是对像素进行操作来实现图像归一化的效果。</li>
</ul>
<p>#行人检测 #检测算法 </p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/posts/2397281138/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Jacob Kong">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/uploads/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="JacobKong's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="JacobKong's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/posts/2397281138/" itemprop="url">
                  行人检测论文笔记：How Far are We from Solving Pedestrian Detection?
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-12-15T06:32:24+08:00">
                2016-12-15
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/2397281138/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="posts/2397281138/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="文章疑问点"><a href="#文章疑问点" class="headerlink" title="文章疑问点"></a>文章疑问点</h2><ul>
<li>Human Baseline 的标准是如何确定的?</li>
<li><p>Ground-truth是什么意思？</p>
<ul>
<li>Groun-truth 指的是正确的标注（真实值）</li>
<li>在有监督学习中，数据是有标注的，以(x, t)的形式出现，其中x是输入数据，t是标注.正确的t标注是ground truth，错误的标记则不是。（也有人将所有标注数据都叫做ground truth）。</li>
</ul>
</li>
<li><p>Intersection over Union（IoU）是什么？</p>
<ul>
<li><p>Intersection over Union is an evaluation metric used to measure the accuracy of an object detector on a particular dataset.</p>
</li>
<li><p>Any algorithm that provides predicted bounding boxes as output can be evaluated using IoU.</p>
</li>
<li><p>As long as we have these two sets of bounding boxes we can apply Intersection over Union.</p>
</li>
<li><p>An Intersection over Union score &gt; 0.5 is normally considered a “good” prediction.</p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfsw1e3pcj308c06i0ss.jpg" alt=""></p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>FPPI: False Positive Per Image</p>
</li>
<li><p>Oracle Experiment: An oracle experiment is used to compare your actual system to how your system would behave if some component of it always did the right thing.</p>
</li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>调查了当前最先进的方法与“完美单帧检测器”之间的差距。</li>
<li>基于Caltech数据集创建了一个人工的基准。</li>
<li>手工聚合了顶级检测器经常出现的错误。</li>
<li><p>刻画了定位，前景 vs 背景两方面的错误</p>
<ul>
<li>针对定位错误：研究了训练集标记噪声对检测器性能的影响</li>
<li>前景 vs 背景错误：研究了convnets，讨论了哪些因素影响其性能</li>
</ul>
</li>
<li><p>提供了一个新的、更纯净的训练/测试标注集。</p>
</li>
</ul>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2. Preliminaries"></a>2. Preliminaries</h2><h3 id="2-1-Caltech-USA-pedestrian-detection-benchmark"><a href="#2-1-Caltech-USA-pedestrian-detection-benchmark" class="headerlink" title="2.1 Caltech-USA pedestrian detection benchmark"></a>2.1 Caltech-USA pedestrian detection benchmark</h3><ul>
<li><p>最流行的数据集：Caltech-USA、KITTI</p>
<ul>
<li>Caltech-USA有2.5小时、30Hz的从LA街道的一个check里面录制的</li>
<li>一共350000个标注、覆盖2300各单一的行人</li>
<li>测试集：4024帧</li>
</ul>
</li>
<li><p>MR: miss rate</p>
</li>
</ul>
<h3 id="2-2-Filtered-channel-features-detector"><a href="#2-2-Filtered-channel-features-detector" class="headerlink" title="2.2 Filtered channel features detector"></a>2.2 Filtered channel features detector</h3><ul>
<li>截止到最近的主要会议（CVPR 15），最好的方法是 <strong>Checkerboards</strong></li>
<li>Checkerboards：是ICF的一种，ICF(Integral Channels Feature detector)</li>
<li>目前最好的执行convnets方法对底层检测建议很敏感，因此我们首先通过优化过滤的通道特征检测器来关注这些建议。</li>
<li>环境和光流可以提高检测（额外的提示）</li>
</ul>
<h2 id="3-Analysing-the-state-of-the-art"><a href="#3-Analysing-the-state-of-the-art" class="headerlink" title="3. Analysing the state of the art"></a>3. Analysing the state of the art</h2><h3 id="3-1-Are-we-reaching-saturation"><a href="#3-1-Are-we-reaching-saturation" class="headerlink" title="3.1 Are we reaching saturation?"></a>3.1 Are we reaching saturation?</h3><ul>
<li>在现在的基准上，我们还有多少提升空间？为了回答这个问题，我们提出可一个人工的基准线作为最低极限。</li>
<li>机器检测算法应该达到至少人类水平，最终超过人类水平。</li>
<li>人工基准线——为了公平比较，关注于单帧单目检测，注释器需要根据行人外表和单帧环境来注释。</li>
<li>Intersection over Union (IoU) ≥ 0.5 matching criterion。</li>
<li>在所有情况下人类基准线表现远远超过当前最好的检测器，说明对于自动方法来说，还有提升空间。</li>
</ul>
<h3 id="3-2-Failure-analysis"><a href="#3-2-Failure-analysis" class="headerlink" title="3.2 Failure analysis"></a>3.2 Failure analysis</h3><p>3.2.1 Error sources</p>
<ul>
<li><p>一个检测器可以有两类错误：</p>
<ul>
<li>假阳性（检测到了背景，或者很弱的定位检测）</li>
<li>假阴性（低得分率或者错过某些行人检测，检测不全）</li>
</ul>
</li>
<li><p>FP聚类成11个分类</p>
</li>
<li>FN聚类成6个分类，其中side view 和 cyclists是由于数据集偏差导致的，用这些案例的外部图像增强训练集可能是一个有效的策略。</li>
<li>对于small pedestrains，发现低像素是主要困难来源，所以合理的利用所有像素，以及周围上下文是很必要的。</li>
</ul>
<p>3.2.2 Oracle test cases</p>
<ul>
<li>对于大多数执行最好的方法，localization和background-vs-forground误差对检测质量具有相等的影响。 他们同样重要。</li>
</ul>
<p>3.3. Improved Caltech-USA annotations</p>
<ul>
<li>原始注释是基于跨越多个帧内插稀疏注释（interpolating sparse annotations ），并且这些稀疏注释不一定位于评估的帧上。</li>
<li><p>我们的目标是两方面：</p>
<ul>
<li>在一方面，我们希望提供对现有技术的更准确的评估，特别是适合于接近该问题的“最后20％”的评估。</li>
<li>另一方面，我们希望有训练注释，并评估改进的注释导怎么样更好的检测。</li>
</ul>
</li>
<li><p>总之，我们的新注释与人类基线在以下方面不同：训练和测试集都被注释，忽略区域和闭塞也被注释，完整的视频数据用于决策，并且允许同一图像的多个修订。</p>
</li>
</ul>
<h3 id="4-Improving-the-state-of-the-art"><a href="#4-Improving-the-state-of-the-art" class="headerlink" title="4. Improving the state of the art"></a>4. Improving the state of the art</h3><p>4.1. Impact of training annotations</p>
<ul>
<li><p>Pruning benefits:</p>
<ul>
<li>从原始到修剪注释的主要变化是删除注释错误，从修剪到新的，主要的变化是更好的对齐。</li>
<li>我们在MRN-2中看到，更强的检测器更好地受益于更好的数据，并且检测质量的最大增益来自移除注释错误。</li>
</ul>
</li>
<li><p>Alignment benefits:</p>
<ul>
<li>为了利用新的1×注释来利用9×剩余数据，我们在新的注释上训练模型，并使用该模型在9×部分上重新对准原始注释。<br> <img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfsntx4jxj30hn06zwg1.jpg" alt="Snip20161204_2"></li>
<li><p>因为新的注释更好地对齐，所以我们期望该模型能够修复原始注释中的轻微位置和缩放错误。</p>
</li>
<li><p>结果表明，使用检测器模型来提高整体数据对准确实是有效的，并且更好地对准训练数据导致更好的检测质量（在MRO和MRN中）。</p>
</li>
<li><p>使用高质量注释进行训练可提高整体检测质量，这得益于改进的对齐和减少的注释错误。</p>
</li>
</ul>
</li>
</ul>
<p>4.2. Convnets for pedestrian detection</p>
<ul>
<li><p>AlexNet 和 VGG16都在ImageNet上进行了预先训练，并使用SquaresChnFtrs建议对Caltech 10×（原始注释）进行了微调。</p>
</li>
<li><p>可以看出，VGG显着地减少了背景误差，而同时稍微增加了定位误差。</p>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfsqv4dcdj30df0c7gnt.jpg" alt="Snip20161204_3"></p>
</li>
<li><p>虽然卷积在图像分类和一般物体检测中具有很强的结果，但是当在小物体周围产生良好的局部检测分数时，它们似乎有局限性。 边界框回归（和NMS）是当前架构的一个关键因素。</p>
</li>
<li><p>表明神经网络的原始分类能力仍有改进的余地。</p>
</li>
</ul>
<h3 id="5-Summary"><a href="#5-Summary" class="headerlink" title="5. Summary"></a>5. Summary</h3><ul>
<li><p>相对于human baseline, there is a 10× gap still to be closed.</p>
</li>
<li><p>误差特性导致关于如何设计更好的检测器（在3.2节中提及;例如，对于人side-view的数据增加或在垂直轴上延伸检测器接收场）的具体建议。</p>
</li>
<li><p>我们通过衡量更好的注释对本地化准确性的影响，以及通过调查使用convnets来改善the background to foreground discrimination，来部分解决了一些问题。我们的研究结果表明，通过适当训练的ICF检测器可以实现显着更好的Alignment，并且，对于行人检测，Convent在localization上能力不强，但是可以通过边界框回归（bounding box regression）部分解决。 对于原始和新注释，所描述的检测方法都能达到最高性能。</p>
<p><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfsrgcdtaj30dh077jue.jpg" alt="Snip20161204_4"></p>
</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/posts/1679631826/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Jacob Kong">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/uploads/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="JacobKong's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="JacobKong's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/posts/1679631826/" itemprop="url">
                  深度学习论文笔记：Fast R-CNN
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-12-08T06:32:24+08:00">
                2016-12-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/论文笔记/" itemprop="url" rel="index">
                    <span itemprop="name">论文笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/1679631826/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="posts/1679631826/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><ul>
<li>mAP：detection quality.</li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>本文提出一种基于快速区域的卷积网络方法（快速R-CNN）用于对象检测。</li>
<li>快速R-CNN采用多项创新技术来提高训练和测试速度，同时提高检测精度。</li>
<li>采用VGG16的网络：VGG: 16 layers of 3x3 convolution interleaved with max pooling + 3 fully-connected layers</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>物体检测相对于图像分类是更复杂的，应为需要物体准确的位置。<ul>
<li>首先，必须处理许多候选对象位置（通常称为“proposal”）。</li>
<li>其次，这些候选者只提供粗略的定位，必须进行精确定位才能实现精确定位。</li>
<li>这些问题的解决方案经常损害 <strong>速度</strong> ， <strong>准确性</strong> 或 <strong>简单性</strong> 。</li>
</ul>
</li>
</ul>
<h3 id="R-CNN-and-SPPnet"><a href="#R-CNN-and-SPPnet" class="headerlink" title="R-CNN and SPPnet"></a>R-CNN and SPPnet</h3><ul>
<li>R-CNN(Region-based Convolution Network)具有几个显著的缺点：<ul>
<li>训练是一个多级管道。</li>
<li>训练在空间和时间上是昂贵的。</li>
<li>物体检测速度很慢。</li>
</ul>
</li>
<li>R-CNN是慢的，因为它对每个对象proposal执行ConvNet正向传递，而不共享计算（sharing computation）。</li>
<li>Spatial pyramid pooling networks（SPPnets），利用sharing computation对R-CNN进行了加速，但是SPPnets也具有明显的缺点，像R-CNN一样，SPPnets也需要：<ul>
<li>训练是一个多阶段流程，</li>
<li>涉及提取特征，</li>
<li>用对数损失精简网络</li>
<li>训练SVM</li>
<li>赋予边界框回归。</li>
<li>特征也需要也写入磁盘。</li>
</ul>
</li>
<li>但与R-CNN <strong>不同</strong> ，在[11]中提出的fine-tuning算法不能更新在空间金字塔池之前的卷积层。 不出所料，这种限制（固定的卷积层）限制了非常深的网络的精度。</li>
</ul>
<h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><ul>
<li>Fast R-CNN优点：</li>
</ul>
<ol>
<li>比R-CNN，SPPnet更高的检测质量（mAP）</li>
<li>训练是单阶段的，使用多任务损失（multi-task loss）</li>
<li>训练可以更新所有网络层</li>
<li>特征缓存不需要磁盘存储</li>
</ol>
<h2 id="Fast-R-CNN-architecture-and-training"><a href="#Fast-R-CNN-architecture-and-training" class="headerlink" title="Fast R-CNN architecture and training"></a>Fast R-CNN architecture and training</h2><ul>
<li>整体框架</li>
</ul>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfo7na6fij30jk0efdju.jpg" alt=""></p>
<ul>
<li>快速R-CNN网络将整个图像和一组对象位置作为输入。<ul>
<li>网络首先使用几个卷积（conv）和最大池层来处理整个图像，以产生conv feature map。</li>
<li>然后，对于每个对象proposal， <strong>感兴趣区域（RoI）池层</strong> 从特征图中抽取固定长度的特征向量。</li>
<li>每个特征向量被馈送到完全连接（fc）层序列，其最终分支成两个同级输出层：<ul>
<li>一个产生对K个对象类加上全部捕获的“背景”类的softmax概率估计(one that produces softmax probability estimates over K object classes plus a catch-all “background” class)</li>
<li>另一个对每个K对象类输出四个实数，每组4个值编码提炼定义K个类中的一个的的边界框位置。(another layer that outputs four real-valued numbers for each of the K object classes. Each set of 4 values encodes reﬁned bounding-box positions for one of the K classes.)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="The-RoI-pooling-layer"><a href="#The-RoI-pooling-layer" class="headerlink" title="The RoI pooling layer"></a>The RoI pooling layer</h3><ul>
<li>Rol pooling layer的作用主要有两个：<ul>
<li>一个是将image中的RoI定位到feature map中对应patch</li>
<li>另一个是用一个单层的SPP layer将这个feature map patch下采样为大小固定的feature再传入全连接层。</li>
</ul>
</li>
<li>RoI池层使用最大池化将任何有效的RoI区域内的特征转换成具有H×W（例如，7×7）的固定空间范围的小feature map，其中H和W是层超参数 它们独立于任何特定的RoI。</li>
<li>在本文中，RoI是conv feature map中的一个矩形窗口。</li>
<li>每个RoI由定义其左上角（r，c）及其高度和宽度（h，w）的四元组（r，c，h，w）定义。</li>
<li>RoI层仅仅是Sppnets中的spatial pyramid pooling layer的特殊形式，其中只有一个金字塔层</li>
</ul>
<h3 id="Initializing-from-pre-trained-networks"><a href="#Initializing-from-pre-trained-networks" class="headerlink" title="Initializing from pre-trained networks"></a>Initializing from pre-trained networks</h3><ul>
<li>用了3个预训练的ImageNet网络（CaffeNet/ VGG_CNN_M_1024 /VGG16）。预训练的网络初始化Fast RCNN要经过三次变形：</li>
</ul>
<ol>
<li>最后一个max pooling层替换为RoI pooling层，设置H’和W’与第一个全连接层兼容。</li>
<li>最后一个全连接层和softmax（原本是1000个类）替换为softmax的对K+1个类别的分类层，和bounding box 回归层。</li>
<li>输入修改为两种数据：一组N个图形，R个RoI，batch size和ROI数、图像分辨率都是可变的。</li>
</ol>
<h3 id="Fine-tuning-for-detection"><a href="#Fine-tuning-for-detection" class="headerlink" title="Fine-tuning for detection"></a>Fine-tuning for detection</h3><ul>
<li>利用反向传播算法进行训练所有网络的权重是Fast R-CNN很重要的一个能力。</li>
<li>我们提出了一种更有效的训练方法，利用在训练期间的特征共享（feature sharing during training）。</li>
<li>在Fast R-CNN训练中， <strong>随机梯度下降（SGD）小批量分层采样</strong> ，首先通过采样N个图像，然后通过从每个图像采样 <strong>R/N个</strong> RoIs。</li>
<li>关键的是，来自同一图像的RoI在向前和向后传递中 <strong>共享计算</strong> 和存储。</li>
<li>此外为了分层采样，Fast R-CNN使用了一个流水线训练过程，利用一个fine-tuning阶段来联合优化一个softmax分类器和bounding box回归，而非训练一个softmax分类器，SVMs，和regression在三个独立的阶段。</li>
<li>Multi-task loss：<ul>
<li>两个loss，以下分别介绍：<ul>
<li>对于分类loss，是一个N+1路的softmax输出，其中的N是类别个数，1是背景。</li>
<li>对于回归loss，是一个4xN路输出的regressor，也就是说对于每个类别都会训练一个单独的regressor的意思，比较有意思的是，这里regressor的loss不是L2的，而是一个平滑的L1，形式如下：</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://ww4.sinaimg.cn/large/006tKfTcgw1fbfo7oia0ij30bg05n74f.jpg" alt=""></p>
<ul>
<li>我们利用一个multi-task loss L 在每个被标注的RoI上来联合训练分类器和bounding box regression</li>
<li>Mini-batch sampling：在微调时，每个SGD的mini-batch是随机找两个图片，R为128，因此每个图上取样64个RoI。从object proposal中选25%的RoI，就是和ground-truth交叠至少为0.5的。剩下的作为背景。</li>
<li><p>Back-propagation through RoI pooling layers：</p>
<ul>
<li><p>RoI pooling层计算损失函数对每个输入变量x的偏导数，如下：</p>
<p><img src="https://ww1.sinaimg.cn/large/006tKfTcgw1fbfo7owdcpj306q01mwee.jpg" alt=""></p>
<p>y是pooling后的输出单元，x是pooling前的输入单元，如果y由x pooling而来，则将损失L对y的偏导计入累加值，最后累加完R个RoI中的所有输出单元。下面是我理解的x、y、r的关系：</p>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfoo7cuv7j30qf0ardgq.jpg" alt="20151208163114338"></p>
</li>
</ul>
</li>
</ul>
<h3 id="Scale-invariance"><a href="#Scale-invariance" class="headerlink" title="Scale invariance"></a>Scale invariance</h3><ul>
<li>这里讨论object的scale问题，就是网络对于object的scale应该是要不敏感的。这里还是引用了SPP的方法，有两种:<ul>
<li>brute force （single scale），也就是简单认为object不需要预先resize到类似的scale再传入网络，直接将image定死为某种scale，直接输入网络来训练就好了，然后期望网络自己能够学习到scale-invariance的表达。</li>
<li>image pyramids （multi scale），也就是要生成一个金字塔，然后对于object，在金字塔上找到一个大小比较接近227x227的投影版本，然后用这个版本去训练网络。</li>
</ul>
</li>
<li>可以看出，2应该比1更加好，作者也在5.2讨论了，2的表现确实比1好，但是好的不算太多，大概是1个mAP左右，但是时间要慢不少，所以作者实际采用的是第一个策略，也就是single scale。</li>
<li>这里，FRCN测试之所以比SPP快，很大原因是因为这里，因为SPP用了2，而FRCN用了1。</li>
</ul>
<h2 id="Fast-R-CNN-detection"><a href="#Fast-R-CNN-detection" class="headerlink" title="Fast R-CNN detection"></a>Fast R-CNN detection</h2><ul>
<li>大型全连接层很容易的可以通过将他们与 <strong>truncated SVD(奇异值分解)</strong> 压缩来加速计算。</li>
</ul>
<h2 id="Main-results"><a href="#Main-results" class="headerlink" title="Main results"></a>Main results</h2><ul>
<li>All Fast R-CNN results in this paper using VGG16 ﬁne-tune layers conv3 1 and up; all experments with models S and M ﬁne-tune layers conv2 and up.</li>
</ul>
<h2 id="Design-evaluation"><a href="#Design-evaluation" class="headerlink" title="Design evaluation"></a>Design evaluation</h2><h3 id="Do-we-need-more-training-data"><a href="#Do-we-need-more-training-data" class="headerlink" title="Do we need more training data?"></a>Do we need more training data?</h3><ul>
<li>在训练期间，作者做过的唯一一个数据增量的方式是水平翻转。 作者也试过将VOC12的数据也作为拓展数据加入到finetune的数据中，结果VOC07的mAP从66.9到了70.0，说明对于网络来说， <strong>数据越多就是越好的。</strong></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/posts/3054155989/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Jacob Kong">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/uploads/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="JacobKong's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="JacobKong's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/posts/3054155989/" itemprop="url">
                  深度学习论文笔记：Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-12-07T06:32:24+08:00">
                2016-12-07
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/3054155989/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="posts/3054155989/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>现有的深卷积神经网络（CNN）需要固定尺寸（例如，224×224）的输入图像。</li>
<li>新的网络结构，称为SPP-net，可以生成固定长度的表示，而不管图像大小/规模。</li>
<li>使用SPP-net，我们从整个图像只计算一次特征图，然后在任意区域（子图像）中池特征以生成固定长度表示以训练检测器。</li>
</ul>
<h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1 INTRODUCTION"></a>1 INTRODUCTION</h2><ul>
<li>在CNN的训练和测试中存在技术问题：普遍的CNN需要固定的输入图像大小（例如，224×224），其限制了输入图像的宽高比和比例。</li>
<li>Cropping</li>
<li>Warping-&gt;unwanted geometric distortion(不需要的几何失真)</li>
<li><p>那么为什么CNN需要固定输入大小？</p>
<ul>
<li>CNN主要由两部分组成：卷积层和跟随的完全连接的层。</li>
<li>事实上，卷积层不需要固定的图像大小，并且可以生成任何大小的特征图</li>
<li>另一方面，根据定义：完全连接的层需要具有固定尺寸/长度输入。所以固定尺寸完全来自于 <strong>全连接层</strong></li>
</ul>
</li>
<li><p>我们提出了一个spatial pyramid pooling（空间金字塔池化层）来去掉额昂罗固定输入的约束。</p>
</li>
<li><p>具体来说，我们在最后一个卷积层的顶部添加一个SPP层。 SPP层汇集特征并产生固定长度的输出，然后馈送到完全连接的层（或其他分类器）。</p>
</li>
</ul>
<p><img src="https://ww3.sinaimg.cn/large/006tKfTcgw1fbfnxkqg21j30jl03hwf2.jpg" alt=""></p>
<ul>
<li><p>SPP对于深度CNN有着一些显著的特性：</p>
<ul>
<li>1）SPP能够生成固定长度的输出，而不管输入大小，而在以前的深度网络[3]中使用的滑动窗口池不能;</li>
<li>2）SPP使用多级空间仓，而滑动窗口池仅使用单个窗口大小。 多层池化已被证明对于对象变形是鲁棒的[15];</li>
<li>3）由于输入尺度的灵活性，SPP可以在可变尺度上提取的特征。</li>
</ul>
</li>
<li><p>实验表明，这种多尺寸训练与传统的单尺寸训练一样收敛，并导致更好的测试精度。</p>
</li>
<li>SPP的优点是与特定的CNN设计是正交的。</li>
<li>Caltech101: L. Fei-Fei, R. Fergus, and P. Perona, “Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories,” CVIU, 2007.</li>
<li>VOC 2007: M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results,” 2007.</li>
<li>但是R-CNN中的特征计算是耗时的，因为它对每个图像的数千个wraped区域的原始像素重复应用深卷积网络。而本文提出的方法可以在一整张图像上只跑一次卷积层</li>
</ul>
<h2 id="2-DEEP-NETWORKS-WITH-SPATIAL-PYRAMID-POOLING"><a href="#2-DEEP-NETWORKS-WITH-SPATIAL-PYRAMID-POOLING" class="headerlink" title="2 DEEP NETWORKS WITH SPATIAL PYRAMID POOLING"></a>2 DEEP NETWORKS WITH SPATIAL PYRAMID POOLING</h2><ul>
<li>输入图像中的这些形状激活在相应位置的feature map</li>
</ul>
<h3 id="2-2-The-Spatial-Pyramid-Pooling-Layer"><a href="#2-2-The-Spatial-Pyramid-Pooling-Layer" class="headerlink" title="2.2 The Spatial Pyramid Pooling Layer"></a>2.2 The Spatial Pyramid Pooling Layer</h3><ul>
<li>Bag-of-Words (BoW) approach-&gt;用来将生成的特征进行pool从而产生固定长度的向量。</li>
<li>空间金字塔池提高BoW，因为它可以通过在局部空间仓中汇集来 <strong>维护空间信息</strong> 。</li>
<li><p>“global pooling” operation</p>
<ul>
<li>a global average pooling</li>
<li>a global average pooling</li>
</ul>
</li>
</ul>
<h3 id="2-3-Training-the-Network"><a href="#2-3-Training-the-Network" class="headerlink" title="2.3 Training the Network"></a>2.3 Training the Network</h3><ul>
<li>Single-size training</li>
<li>Multi-size training</li>
</ul>
<h2 id="3-SPP-NET-FOR-IMAGE-CLASSIFICATION"><a href="#3-SPP-NET-FOR-IMAGE-CLASSIFICATION" class="headerlink" title="3. SPP-NET FOR IMAGE CLASSIFICATION"></a>3. SPP-NET FOR IMAGE CLASSIFICATION</h2><h2 id="4-SPP-NET-FOR-OBJECT-DETECTION"><a href="#4-SPP-NET-FOR-OBJECT-DETECTION" class="headerlink" title="4. SPP-NET FOR OBJECT DETECTION"></a>4. SPP-NET FOR OBJECT DETECTION</h2><ul>
<li>对于R-CNN来说，Feature extraction is the major timing bottleneck in testing.</li>
<li>对于我们的SPP-net来说，我们从一整张图片中值提取一次特征。</li>
<li>On the contrary, our method enables feature extraction in <strong>arbitrary windows</strong> from the deep convolutional feature maps.</li>
</ul>
<h3 id="4-1-Detection-Algorithm"><a href="#4-1-Detection-Algorithm" class="headerlink" title="4.1 Detection Algorithm"></a>4.1 Detection Algorithm</h3><p><img src="https://ww2.sinaimg.cn/large/006tKfTcgw1fbfnxjqg0ej30h50lfai3.jpg" alt=""></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/posts/4241353321/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Jacob Kong">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/uploads/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="JacobKong's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="JacobKong's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/posts/4241353321/" itemprop="url">
                  深度学习论文笔记：Rich feature hierarchies for accurate object detection and semantic segmentation
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2016-12-06T06:32:24+08:00">
                2016-12-06
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/4241353321/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="posts/4241353321/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>mAP: mean average precision，平均准确度</li>
<li><p>我们的方法结合两个关键的见解：</p>
<ul>
<li>第一：采用高容量的卷积神经网络来从上到下的进行region proposal，从而实现定位和分割物体。</li>
<li>当标记的训练数据稀缺时，可以先对辅助数据集（任务）进行受监督的预训练， 随后是基于域进行特定调整，产生显着的性能提升。</li>
</ul>
</li>
</ul>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul>
<li>关于各种视觉识别任务的上一个十年的进展主要基于SIFT和HOG的使用</li>
<li><p>实现这个结果需要解决两个问题：</p>
<ul>
<li>利用深度网络将对象定位</li>
<li>仅利用少量的注释检测数据来训练训练高容量模型。</li>
</ul>
</li>
<li><p>我们通过在“使用区域识别”范例内操作，来解决CNN定位问题</p>
</li>
<li>在测试时，我们的方法为输入图像生成大约2000个类别无关区域提案，使用CNN从每个proposal中提取固定长度的特征向量，然后使用类别特定的线性SVM对每个区域进行分类。</li>
<li>检测中面临的第二个挑战是标记的数据不足，目前可用的数据数量不足以训练大型CNN。这个问题的常规解决方案是使用无监督预训练，然后是监督 fine-tuning。</li>
<li>我们发现，对于CNN，有很大比例的参数（94%）可以在检测精度的适度降低的情况下被去除。</li>
<li>我们证明一个简单的 <strong>边界框回归方法（bounding box regression）</strong> 显着减少误定位，这是主要的误差模式(error mode)。</li>
<li>在开发技术细节之前，我们注意到，因为R-CNN在是区域上操作，所以很自然将其扩展到语义分割（semantic segmentation）的任务。</li>
</ul>
<h2 id="2-Object-detection-with-R-CNN"><a href="#2-Object-detection-with-R-CNN" class="headerlink" title="2. Object detection with R-CNN"></a>2. Object detection with R-CNN</h2><ul>
<li><p>我们的对象检测系统由三个模块组成:</p>
<ul>
<li>首先生成类别独立(category-independent)区域proposal。 这些proposal定义了可用于检测器的候选检测集合。</li>
<li>第二个模块是大卷积神经网络，从每个区域提取固定长度的特征向量。</li>
<li>第三个模块是一类特定类型的线性SVM。</li>
</ul>
</li>
</ul>
<h3 id="2-1-Module-design"><a href="#2-1-Module-design" class="headerlink" title="2.1. Module design"></a>2.1. Module design</h3><ul>
<li><p>Region proposals: 目前有很多用来生成category-independent的region proposal的方法：</p>
<ul>
<li>Objectness</li>
<li>selective search</li>
<li>category-independent object proposals</li>
<li>constrained parametric min-cuts (CPMC)</li>
<li>multi-scale combinatorial grouping</li>
<li>detect mitotic cells by applying a CNN to regularly-spaced square crops, which are a special case of region proposals.(通过将CNN应用于规则间隔的方形作物来检测有丝分裂细胞，这是区域提案的特殊情况。)</li>
</ul>
</li>
<li><p>虽然R-CNN与特定区域建议方法无关，但我们使用选择性搜索(selective search)来实现与先前检测工作的受控比较</p>
</li>
<li><p>Feature extraction:我们从每个区域提案中提取一个4096维特征向量，特征通过前向传播对227×227 RGB图像通过 <strong>五个卷积层和两个完全连接的层</strong> 计算。</p>
</li>
<li>无论候选区域的大小或宽高比如何，我们都会将其周围的紧密边界框中的所有像素装到所需的大小(227x227像素尺寸)。</li>
</ul>
<h3 id="2-2-Test-time-detection"><a href="#2-2-Test-time-detection" class="headerlink" title="2.2. Test-time detection"></a>2.2. Test-time detection</h3><ul>
<li>在测试时，我们对测试图像运行选择性搜索以提取大约2000个区域建议（我们在所有实验中使用选择性搜索的“快速模式（fast mode）”）。</li>
<li>给定图像中的所有得分区域，我们应用贪心非最大抑制(greedy non-maximum suppression)（对于每个类独立地），如果与的饭较高的区域有重叠，且IoU大于学习到的阈值，则该拒绝区域。</li>
<li><p>Run-time analysis.两个属性使检测更高校。</p>
<ul>
<li>首先，所有CNN参数在所有类别中共享。</li>
<li>第二，CNN计算的特征向量与其他常见方法（例如具有视觉词袋编码的空间棱金字塔）相比是 <strong>低维的</strong> 。</li>
<li>唯一的类特定(class-specific)计算是特征和SVM权重之间的点积和非最大抑制。</li>
</ul>
</li>
</ul>
<h3 id="2-3-Training"><a href="#2-3-Training" class="headerlink" title="2.3. Training"></a>2.3. Training</h3><ul>
<li>除了用随机初始化的21路分类层（对于20个VOC类加上背景）替换CNN的ImageNet特定的1000路分类层之外，CNN架构是不变的。</li>
<li>我们将所有region proposal与一个ground-truth重叠为IoU&gt;0.5，作为该框类的阳性，其余作为阴性。</li>
<li>我们以0.001的学习速率（初始预训练速率的1/10）开始SGD，这允许精细调整进行，而不是破坏初始化。</li>
<li>一旦提取特征并应用训练标签，我们对每个类优化一个线性SVM。</li>
<li>由于训练数据太大，无法记忆，我们采用标准 <strong>hard negative mining method</strong> 。</li>
</ul>
<h3 id="2-4-Results-on-PASCAL-VOC-2010-12"><a href="#2-4-Results-on-PASCAL-VOC-2010-12" class="headerlink" title="2.4. Results on PASCAL VOC 2010-12"></a>2.4. Results on PASCAL VOC 2010-12</h3><h2 id="3-Visualization-ablation-and-modes-of-error"><a href="#3-Visualization-ablation-and-modes-of-error" class="headerlink" title="3. Visualization, ablation, and modes of error"></a>3. Visualization, ablation, and modes of error</h2><h3 id="3-1-Visualizing-learned-features"><a href="#3-1-Visualizing-learned-features" class="headerlink" title="3.1. Visualizing learned features"></a>3.1. Visualizing learned features</h3><ul>
<li>pool-5，是网络第五个也是最后一个卷基层的max-pool层的输出。（是一个max-pooling层）</li>
<li>The pool-5 feature map is 6 × 6 × 256 = 9216维。</li>
<li>忽略边界效应，每个pool-5单元在原始227×227像素输入中具有195×195像素的接收场。</li>
</ul>
<h3 id="3-2-Ablation-studies"><a href="#3-2-Ablation-studies" class="headerlink" title="3.2. Ablation studies"></a>3.2. Ablation studies</h3><ul>
<li>Fc6与pool-5全连接，为了计算特征，他它将 <strong>4096×9216的权重矩阵乘以pool-5的feature map</strong> （重新形成为9216维矢量），然后添加偏差矢量。</li>
<li>Fc7是网络的最后一层，通过将由fc 6计算的特征乘以 <strong>4096×4096</strong> 权重矩阵，并类似地添加偏置矢量和应用半波整流来实现。</li>
<li>大多数CNN的表示能力来自它的卷积层，而不是来自大得多的密集连接的层。</li>
<li>All R-CNN variants strongly outperform the three DPM baselines</li>
</ul>
<h3 id="3-3-Detection-error-analysis"><a href="#3-3-Detection-error-analysis" class="headerlink" title="3.3. Detection error analysis"></a>3.3. Detection error analysis</h3><h3 id="3-4-Bounding-box-regression"><a href="#3-4-Bounding-box-regression" class="headerlink" title="3.4. Bounding box regression"></a>3.4. Bounding box regression</h3><h2 id="4-Semantic-segmentation"><a href="#4-Semantic-segmentation" class="headerlink" title="4. Semantic segmentation"></a>4. Semantic segmentation</h2><ul>
<li>full</li>
<li>fg</li>
<li>full+fg</li>
<li>The fg strategy slightly outperforms full, indicating that the masked region shape provides a stronger signal, matching our intuition.</li>
</ul>
<h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h2><ul>
<li>之前最好的性能系统是将多个低级图像特征与来自对象检测器和场景分类器的高级上下文组合在一起的复杂集合。</li>
<li>本文提出了一个简单和可扩展的对象检测算法，与PASCAL VOC 2012上的最佳以前的结果相比提供30％的相对改进。</li>
<li>我们推测“supervised pre-training/domain-speciﬁc ﬁne-tuning”范例将对各种数据缺乏的视觉问题高度有效。</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>


          
          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/avatar.jpg"
               alt="Jacob Kong" />
          <p class="site-author-name" itemprop="name">Jacob Kong</p>
          <p class="site-description motion-element" itemprop="description">欢迎来到孔伟杰（@JacobKong_Dev）的博客。 本人目前是北大信工的研一菜鸟一枚。 研究兴趣：计算机视觉|深度学习|行人检测。 欢迎大家一块儿交流！</p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">7</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/JacobKong" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/JacobKong_Dev" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/2232756824" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.jianshu.com/u/81a8d024af56" target="_blank" title="JianShu">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  JianShu
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jacob Kong</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"jacobkong"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  












  
  

  

  

  

  


</body>
</html>
