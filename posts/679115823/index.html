<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Deep Learning,深度学习,论文调研," />





  <link rel="alternate" href="/atom.xml" title="JacobKong's Blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="Visual object tracking
Learning Policies for Adaptive Tracking with Deep Feature Cascades
Our fundamental insight is to take an adaptive approach, where easy frames are processed with cheap features (">
<meta property="og:type" content="article">
<meta property="og:title" content="行为识别论文笔记：行为分类深度模型的总结.md">
<meta property="og:url" content="http://yoursite.com/posts/679115823/index.html">
<meta property="og:site_name" content="JacobKong's Blog">
<meta property="og:description" content="Visual object tracking
Learning Policies for Adaptive Tracking with Deep Feature Cascades
Our fundamental insight is to take an adaptive approach, where easy frames are processed with cheap features (">
<meta property="og:updated_time" content="2017-11-29T08:31:51.131Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="行为识别论文笔记：行为分类深度模型的总结.md">
<meta name="twitter:description" content="Visual object tracking
Learning Policies for Adaptive Tracking with Deep Feature Cascades
Our fundamental insight is to take an adaptive approach, where easy frames are processed with cheap features (">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '6269586705494312000',
      author: 'JacobKong_Dev'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/posts/679115823/"/>





  <title> 行为识别论文笔记：行为分类深度模型的总结.md | JacobKong's Blog </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  


<!-- hexo-inject:begin --><!-- hexo-inject:end --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-89889116-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?62f2b754cef72ea357bef905cd2ce0b4";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>








  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">JacobKong's Blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <p class="site-subtitle">路漫漫其修远兮......</p>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="st-search-show-outputs">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <form class="site-search-form">
  <input type="text" id="st-search-input" class="st-search-input st-default-search-input" />
</form>

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'qwpH936HXK5KmdfNnB1U','2.0.0');
</script>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://yoursite.com/posts/679115823/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Jacob Kong">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/uploads/avatar.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="JacobKong's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="JacobKong's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                行为识别论文笔记：行为分类深度模型的总结.md
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-11-25T15:51:24+08:00">
                2017-11-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/论文笔记/" itemprop="url" rel="index">
                    <span itemprop="name">论文笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/679115823/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="posts/679115823/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/posts/679115823/" class="leancloud_visitors" data-flag-title="行为识别论文笔记：行为分类深度模型的总结.md">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Visual-object-tracking"><a href="#Visual-object-tracking" class="headerlink" title="Visual object tracking"></a>Visual object tracking</h2><ul>
<li><h4 id="Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades"><a href="#Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades" class="headerlink" title="Learning Policies for Adaptive Tracking with Deep Feature Cascades"></a>Learning Policies for Adaptive Tracking with Deep Feature Cascades</h4><ul>
<li>Our fundamental insight is to take an adaptive approach, where easy frames are processed with cheap features (such as pixel values), while challenging frames are processed with invariant but expensive deep features.</li>
<li>Formulate the adaptive tracking problem as a decision-making process.</li>
<li>Learn an agent to decide whether to locate objects with high conﬁdence on an early layer, or continue processing subsequent layers of a network.</li>
<li>Signiﬁcantly reduces the feedforward cost.</li>
<li>Train the agent ofﬂine in a reinforcement learning fashion.</li>
<li>Obviously, the major computational burden comes from the forward pass through the entire network, and can be larger with deeper architectures.</li>
<li>However, when the object is visually distinct or barely moves, early layers are in most scenarios sufﬁcient for precise localization - offering the potential for substantial computational savings.</li>
<li>The agent learns to ﬁnd the target at each layer, and decides if it is conﬁdent enough to output and stop there.</li>
</ul>
</li>
<li><h4 id="Tracking-The-Untrackable-Learning-to-Track-Multiple-Cues-with-Long-Term-Dependencies"><a href="#Tracking-The-Untrackable-Learning-to-Track-Multiple-Cues-with-Long-Term-Dependencies" class="headerlink" title="Tracking The Untrackable: Learning to Track Multiple Cues with Long-Term Dependencies"></a>Tracking The Untrackable: Learning to Track Multiple Cues with <strong>Long-Term Dependencies</strong></h4><ul>
<li>Combine cues in a coherent end-to-end fashion over a <strong>long period of time.</strong></li>
<li>Present a structure of Recurrent Neural Networks (RNN) that jointly reasons on multiple cues over a temporal window.</li>
<li>We are able to <strong>correct many data association errors</strong> and recover observations from an occluded state.</li>
</ul>
</li>
<li><h4 id="Tracking-as-Online-Decision-Making-Learning-a-Policy-from-Streaming-Videos-with-Reinforcement-Learning"><a href="#Tracking-as-Online-Decision-Making-Learning-a-Policy-from-Streaming-Videos-with-Reinforcement-Learning" class="headerlink" title="Tracking as Online Decision-Making: Learning a Policy from Streaming Videos with Reinforcement Learning"></a>Tracking as Online Decision-Making: Learning a Policy from Streaming Videos with Reinforcement Learning</h4><ul>
<li>A tracking agent must follow an object despite ambiguous image frames and a limited computational budget.</li>
<li>The agent must decide<ul>
<li>where to look in the upcoming frames</li>
<li>when to reinitialize because it believes the target has been lost</li>
<li>when to update its appearance model for the tracked object</li>
</ul>
</li>
<li>Formulating tracking as a partially observable decision-making process (POMDP).</li>
<li><strong>Sparse rewards</strong> allow us to quickly train on massive datasets.</li>
<li>Challenges:<ul>
<li>First, the limited quantity of annotated video data impedes both training and evaluation.</li>
<li>Second, as vision (re)integrates with robotics, video processing must be done in an online, streaming fashion.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Face-detection"><a href="#Face-detection" class="headerlink" title="Face detection"></a>Face detection</h3><ul>
<li><h4 id="S3FD-Single-Shot-Scale-invariant-Face-Detector"><a href="#S3FD-Single-Shot-Scale-invariant-Face-Detector" class="headerlink" title="S3FD - Single Shot Scale-invariant Face Detector."></a>S3FD - Single Shot Scale-invariant Face Detector.</h4><ul>
<li>Use a single deep neural network, especially for small faces.</li>
<li>Contribution<ul>
<li>提出一个尺度公平的人脸检测框架来处理不同尺度的人脸。我们在各种各样的图层上拼贴anchor，以确保所有人脸的比例尺都具有足够的特征用于检测。基于有效接收域<strong>（effective receptive ﬁeld）</strong>和等比例区间原则<strong>（equal proportion interval principle）</strong>设计anchor</li>
<li>用尺度补偿anchor匹配策略<strong>（ a scale compensation anchor matching strategy）</strong>提高小脸的召回率; </li>
<li>通过最大化背景标签<strong>（ max-out background label）</strong>减少小脸的误报率。</li>
<li>effective receptive ﬁeld: Understanding the effective receptive ﬁeld in deep convolutional neural networks.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Salient-Object-Detection"><a href="#Salient-Object-Detection" class="headerlink" title="Salient Object Detection"></a>Salient Object Detection</h3><ul>
<li><h4 id="Amulet-Aggregating-Multi-level-Convolutional-Features-for-Salient-Object-Detection"><a href="#Amulet-Aggregating-Multi-level-Convolutional-Features-for-Salient-Object-Detection" class="headerlink" title="Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection"></a>Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection</h4><ul>
<li>How to better aggregate multi-level convolutional feature maps for salient object detection is <strong>underexplored</strong>.</li>
<li>Our framework:<ul>
<li>First integrates multi-level feature maps into multiple resolutions, which simultaneously incorporate coarse semantics and ﬁne details. </li>
<li>Then it adaptively learns to combine these feature maps at each resolution and <strong>predict saliency maps with the combined features.</strong> </li>
<li>Finally, the predicted results are efﬁciently fused to generate the <strong>ﬁnal saliency map</strong>.</li>
</ul>
</li>
<li>In addition, edge-aware maps and high-level predictions are embedded into the framework.</li>
</ul>
</li>
<li><h4 id="Learning-Uncertain-Convolutional-Features-for-Accurate-Saliency-Detection"><a href="#Learning-Uncertain-Convolutional-Features-for-Accurate-Saliency-Detection" class="headerlink" title="Learning Uncertain Convolutional Features for Accurate Saliency Detection"></a><a href="http://openaccess.thecvf.com/content_iccv_2017/html/Zhang_Learning_Uncertain_Convolutional_ICCV_2017_paper.html" target="_blank" rel="external">Learning Uncertain Convolutional Features for Accurate Saliency Detection</a></h4><ul>
<li>The key contribution of this work is to learn deep uncertain convolutional features (UCF), which encourage the robustness and accuracy of saliency detection.</li>
<li>我们提出了一种有效的混合上采样方法来减少我们的解码器网络中去卷积算子的棋盘伪影。</li>
<li>We ﬁnd that the actual cause of these artifacts is the upsampling mechanism, which generally utilizes the deconvolution operation.</li>
</ul>
</li>
</ul>
<h3 id="Action-Related"><a href="#Action-Related" class="headerlink" title="Action Related"></a>Action Related</h3><ul>
<li><h4 id="Encouraging-LSTMs-to-Anticipate-Actions-Very-Early"><a href="#Encouraging-LSTMs-to-Anticipate-Actions-Very-Early" class="headerlink" title="Encouraging LSTMs to Anticipate Actions Very Early"></a>Encouraging LSTMs to Anticipate Actions Very Early</h4><ul>
<li>Action anticipation - identify the action from only partially available videos.</li>
<li>To this end, we develop a <strong>multi-stage LSTM architecture</strong> that leverages <strong>context-aware</strong> and <strong>action-aware</strong> features, and introduce <strong>a novel loss function</strong> that encourages the model to predict the correct class as early as possible.</li>
<li>Intuitive: our loss models the intuition that some actions, such as running and high jump, are highly ambiguous after seeing only the ﬁrst few frames, and <strong>false positives</strong> should therefore not be penalized too strongly in the early stages.</li>
<li>We would like to predict a high probability for the correct class <strong>as early as possible</strong>, and thus penalize <strong>false negatives</strong> from the beginning of the sequence.</li>
<li>Contribute a novel multi-stage Long Short Term Memory (LSTM) architecture for action anticipation. This model effectively extracts and jointly exploits context- and action-aware features.</li>
<li>Existing method drawbacks:<ul>
<li>This is in contrast to existing methods that typically extract either <strong>global representations</strong> for the entire image or video sequence thus <strong>not focusing on the action itself, or localize the feature extraction process to the action itself</strong> via dense trajectories  optical ﬂow or actionness, thus <strong>failing to exploit contextual information.</strong></li>
<li>利用光流不允许这些方法在localization过程中明确地利用外观。</li>
<li>Computing optical ﬂow is typically expensive.</li>
</ul>
</li>
<li>In the future, we intend to study new ways to incorporate additional sources of information, such as <strong>dense trajectories</strong> and <strong>human skeletons</strong> in our framework.</li>
</ul>
</li>
<li><h4 id="Unsupervised-Action-Discovery-and-Localization-in-Videos"><a href="#Unsupervised-Action-Discovery-and-Localization-in-Videos" class="headerlink" title="Unsupervised Action Discovery and Localization in Videos"></a>Unsupervised Action Discovery and Localization in Videos</h4><ul>
<li>创新：无监督的action localization。</li>
<li>First to address the problem of unsupervised action localization in videos.</li>
<li><p>We propose a novel approach that:</p>
<ul>
<li>Discovers action class labels</li>
<li>Spatio-temporally localizes actions in videos.</li>
</ul>
</li>
<li><p>Method:</p>
<ul>
<li>It begins by computing local video features to apply <strong>spectral clustering</strong> on a set of unlabeled training videos.<ul>
<li>For each cluster of videos, an <strong>undirected graph</strong> is constructed to extract a dominant set, which are known for <strong>high internal homogeneity</strong> and <strong>in-homogeneity</strong> between vertices outside it.</li>
</ul>
</li>
<li>Next, a <strong>discriminative clustering approach</strong> is applied, by training a classiﬁer for each cluster, to iteratively select videos from the non-dominant set and obtain complete video action classes.<ul>
<li>Once classes are discovered, training videos within each cluster are selected to perform <strong>automatic spatio-temporal annotations</strong>, by ﬁrst over-segmenting videos in each discovered class into <strong>supervoxels</strong>（超体素） and constructing a <strong>directed graph</strong> （有向图）to apply a variant of knapsack problem with temporal constraints. （并构建有向图以应用具有时间约束的背包问题的变体。）</li>
</ul>
</li>
<li>背包优化联合收集超体素的一个子集，通过强制注释的动作进行时空连接，其体积是一个actor的大小。These annotations are used to train SVM action classiﬁers.</li>
</ul>
</li>
<li>在测试过程中，操作使用类似的背包方法来进行localize，在这种方法中将超体素分组在一起，并且使用来自发现的动作类的视频学习的SVM被用于识别这些动作。</li>
<li>However, supervised algorithms have some disadvantages compared to unsupervised approaches, due to the difﬁculty of video annotation.</li>
<li>Contributions：<ul>
<li>Automatic discovery of action class labels using <strong>a new discriminative clustering approach</strong> with dominant sets (Sec. 3).</li>
<li>We propose a novel <strong>Knapsack approach</strong> with <strong>graph-based temporal constraints</strong> to <strong>annotate actions</strong> in training videos</li>
<li>The annotations within each cluster of videos are jointly selected by <strong>Binary Integer Quadratic Programming (BIQP)</strong> optimization to train action classiﬁers.</li>
<li><strong>Structural SVM</strong> is used to learn the pairwise relations of supervoxels within foreground action and foreground-background, which enforces that the supervoxels belonging to the action to be simultaneously selected.</li>
<li>Lastly, we address a new problem of <strong>Unsupervised Action Localization</strong> (Sec. 5.2).</li>
</ul>
</li>
</ul>
</li>
<li><h4 id="Dense-Captioning-Events-in-Videos"><a href="#Dense-Captioning-Events-in-Videos" class="headerlink" title="Dense-Captioning Events in Videos"></a>Dense-Captioning Events in Videos</h4><ul>
<li>We introduce the task of dense-captioning events, which involves both detecting and describing events in a video.</li>
<li>Our model introduces a variant of an existing proposal module that is designed to capture both short as well as long events that span minutes.</li>
<li>To <strong>capture the dependencies between the events in a video</strong>, our model introduces a <strong>new captioning module</strong> that uses <strong>contextual information</strong> from past and future events to jointly describe all events.</li>
<li>While the success of these methods is encouraging, they all share one key limitation: <strong>detail.</strong></li>
<li>We introduce the task of <strong>dense-captioning events</strong>, which requires a model to generate a set of descriptions for multiple events occurring in the video and localize them in time.</li>
<li>However, we observe that densecaptioning events comes with its own set of challenges distinct from the image case.<ul>
<li>One observation is that events in videos can range across multiple time scales and can even overlap.<ul>
<li>Past captioning works have circumvented this problem by encoding the entire video sequence by <strong>mean-pooling</strong> [50] or by using a <strong>recurrent neural network (RNN)</strong> [49].</li>
<li>To overcome this limitation, we extend recent work on generating action proposals [10] to <strong>multi-scale detection of events.</strong></li>
</ul>
</li>
<li>Another key observation is that the events in a given video <strong>are usually related to one another.</strong> <ul>
<li>We introduce a <strong>captioning module</strong> that utilizes the context from all the events from our proposal module to generate each sentence.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><h4 id="Learning-long-term-dependencies-for-action-recognition-with-a-biologically-inspired-deep-network"><a href="#Learning-long-term-dependencies-for-action-recognition-with-a-biologically-inspired-deep-network" class="headerlink" title="Learning long-term dependencies for action recognition with a biologically-inspired deep network"></a>Learning long-term dependencies for action recognition with a biologically-inspired deep network</h4><ul>
<li>How to efﬁciently learn long-term dependencies from sequences still remains a pretty challenging task.</li>
<li>As one of the key models for sequence learning, <strong>recurrent neural network (RNN)</strong> and its variants such as <strong>long short term memory (LSTM)</strong> and <strong>gated recurrent unit (GRU)</strong> are still not powerful enough in practice.<ul>
<li>One possible reason is that they have only feedforward connections, which is different from the biological neural system that is typically composed of <strong>both feedforward and feedback connections.</strong>(既有前传，也有反馈)</li>
</ul>
</li>
<li>Propose <strong>shuttleNet technologically.</strong></li>
<li>The shuttleNet <strong>consists of several processors</strong>, each of which is a GRU while <strong>associated with multiple groups of cells and states.</strong></li>
<li><strong>Attention mechanism</strong> is then employed to select the best information ﬂow pathway.</li>
</ul>
</li>
<li><h4 id="Adaptive-RNN-Tree-for-Large-Scale-Human-Action-Recognition"><a href="#Adaptive-RNN-Tree-for-Large-Scale-Human-Action-Recognition" class="headerlink" title="Adaptive RNN Tree for Large-Scale Human Action Recognition"></a>Adaptive RNN Tree for Large-Scale Human Action Recognition</h4><ul>
<li>We present the RNN Tree (RNN-T), an adaptive learning framework for <strong>skeleton based human action recognition.</strong></li>
<li>Our method categorizes action classes and <strong>uses multiple Recurrent Neural Networks (RNNs)</strong> in a <strong>treelike hierarchy.</strong></li>
<li>在骨架表示中的行为是通过<strong>分层推理</strong>过程来识别的，在这个过程中，单独的RNN将细化的行为类别与增加的置信度</li>
<li>RNN-T effectively addresses two main challenges of large-scale action recognition:<ul>
<li>able to distinguish ﬁne-grained action classes that are intractable using a single network</li>
<li>adaptive to new action classes by augmenting an existing model.</li>
</ul>
</li>
</ul>
</li>
<li><h4 id="Ensemble-Deep-Learning-for-Skeleton-based-Action-Recognition-using-Temporal-Sliding-LSTM-networks"><a href="#Ensemble-Deep-Learning-for-Skeleton-based-Action-Recognition-using-Temporal-Sliding-LSTM-networks" class="headerlink" title="Ensemble Deep Learning for Skeleton-based Action Recognition using Temporal Sliding LSTM networks"></a>Ensemble Deep Learning for Skeleton-based Action Recognition using Temporal Sliding LSTM networks</h4><ul>
<li>Traditional methods generally use relative coordinate systems dependent on some joints, and model <strong>only the long-term dependency</strong>, while excluding <strong>short-term and medium term dependencies.</strong></li>
<li>We transform the skeletons into <strong>another coordinate system</strong> to obtain the robustness to scale, rotation and translation and then extract salient motion features from them.</li>
<li>We propose novel ensemble <strong>Temporal Sliding LSTM (TS-LSTM) networks</strong> for skeleton-based action recognition. The proposed network is composed of multiple parts containing <strong>short-term, medium-term and long-term TS-LSTM networks</strong>.</li>
<li>With a rapid development of 3D data acquisition over the past few decades, lots of researches on <strong>human activity recognition from 3D data</strong> can have been actively performed.</li>
</ul>
</li>
</ul>
<h3 id="Pedestrian-Related"><a href="#Pedestrian-Related" class="headerlink" title="Pedestrian Related"></a>Pedestrian Related</h3><ul>
<li><h4 id="HydraPlus-Net-Attentive-Deep-Features-for-Pedestrian-Analysis"><a href="#HydraPlus-Net-Attentive-Deep-Features-for-Pedestrian-Analysis" class="headerlink" title="HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis"></a>HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis</h4><ul>
<li>Learning of comprehensive features of pedestrians for ﬁne-grained tasks remains an open problem.</li>
<li>HydraPlus-Net: multi-directionally feeds the multi-level attention maps to different feature layers.</li>
<li>Advantages: <ul>
<li>(1) the model is capable of capturing <strong>multiple attentions</strong> from low-level to semantic-level</li>
<li>(2) it explores the <strong>multi-scale selectiveness of attentive features</strong> to enrich the ﬁnal feature representations for a pedestrian image.</li>
</ul>
</li>
<li>We demonstrate the effectiveness and generality of the proposed HP-net for pedestrian analysis on two tasks, i.e. <strong>pedestrian attribute recognition</strong> and <strong>person reidentiﬁcation.</strong></li>
<li>However, the learning of feature representation for pedestrian images, as the backbone for all those applications, still confronts critical challenges and needs profound studies.</li>
<li>However, existing arts merely extract global features [13, 24, 30] and are hardly effective to location-aware semantic pattern extraction.</li>
<li><strong>Multidirectional attention (MDA) modules</strong></li>
<li>Reliable 3D <strong>skeleton-based action recognition (SAR)</strong> is now feasible [1].</li>
<li>Although much progress has been achieved, these methods are still facing two challenges.<ul>
<li>We term the ﬁrst one as the <strong>discriminative challenge.</strong></li>
<li>We term the second challenge as <strong>adaptability</strong>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Pose-Estimation"><a href="#Pose-Estimation" class="headerlink" title="Pose Estimation"></a>Pose Estimation</h3><ul>
<li><h4 id="Towards-3D-Human-Pose-Estimation-in-the-Wild-A-Weakly-Supervised-Approach"><a href="#Towards-3D-Human-Pose-Estimation-in-the-Wild-A-Weakly-Supervised-Approach" class="headerlink" title="Towards 3D Human Pose Estimation in the Wild: A Weakly-Supervised Approach"></a><a href="http://openaccess.thecvf.com/content_iccv_2017/html/Zhou_Towards_3D_Human_ICCV_2017_paper.html" target="_blank" rel="external">Towards 3D Human Pose Estimation in the Wild: A Weakly-Supervised Approach</a></h4><ul>
<li>We propose a <strong>weakly-supervised transfer learning</strong> method that uses mixed 2D and 3D labels in a uniﬁed deep neutral network that presents two-stage cascaded structure.</li>
</ul>
</li>
</ul>
<h3 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h3><ul>
<li><h4 id="Flow-Guided-Feature-Aggregation-for-Video-Object-Detection"><a href="#Flow-Guided-Feature-Aggregation-for-Video-Object-Detection" class="headerlink" title="Flow-Guided Feature Aggregation for Video Object Detection"></a>Flow-Guided Feature Aggregation for Video Object Detection</h4><ul>
<li>Video object detection</li>
<li>The accuracy of detection suffers from degenerated object appearances in videos, e.g., motion blur, video defocus, rare poses, etc.</li>
<li>We present ﬂow-guided feature aggregation, an accurate and <strong>end-to-end</strong> learning framework for <strong>video object detection.</strong></li>
<li>It leverages <strong>temporal coherence</strong> on feature level instead.</li>
<li>它通过沿着运动路径聚集附近的特征来改进每帧特征，从而提高了视频识别的准确性。</li>
<li>Fast moving objects.</li>
</ul>
</li>
<li><h4 id="DeNet-Scalable-Real-time-Object-Detection-with-Directed-Sparse-Sampling"><a href="#DeNet-Scalable-Real-time-Object-Detection-with-Directed-Sparse-Sampling" class="headerlink" title="DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling"></a>DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling</h4><ul>
<li>We deﬁne the object detection from imagery problem as estimating a very large but <strong>extremely sparse bounding box</strong> dependent probability distribution. （我们将图像问题中的目标检测定义为估计非常大但极其稀疏的边界框相关概率分布。）</li>
<li>Two novelties:<ul>
<li>a <strong>corner based</strong> region-of-interest estimator</li>
<li>a <strong>deconvolution based</strong> CNN model</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Image-Recognition"><a href="#Image-Recognition" class="headerlink" title="Image Recognition"></a>Image Recognition</h3><ul>
<li><h4 id="Multi-label-Image-Recognition-by-Recurrently-Discovering-Attentional-Regions"><a href="#Multi-label-Image-Recognition-by-Recurrently-Discovering-Attentional-Regions" class="headerlink" title="Multi-label Image Recognition by Recurrently Discovering Attentional Regions"></a>Multi-label Image Recognition by Recurrently Discovering Attentional Regions</h4><ul>
<li>Current solutions for this task usually rely on an extra step of extracting hypothesis regions (i.e., region proposals), resulting in <strong>redundant computation</strong> and <strong>sub-optimal performance.</strong></li>
<li>Developing a recurrent memorized-attention module.</li>
<li>This module consists of two alternately performed components:<ul>
<li>a spatial transformer layer to locate attentional regions from the convolutional feature maps in a region-proposal-free way</li>
<li>an LSTM (Long-Short Term Memory) sub-network to sequentially predict semantic labeling scores on the located regions while capturing the global dependencies of these regions.</li>
</ul>
</li>
<li>Despite acknowledged successes, these methods take the redundant computational cost of extracting region proposals and usually over-simplify the contextual dependencies among foreground objects, leading to a sub-optimal performance in complex scenarios.</li>
</ul>
</li>
</ul>
<p>  ​<br>  ​<br>  ​<br>  ​    </p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
            <a href="/tags/论文调研/" rel="tag"># 论文调研</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/posts/679115822/" rel="next" title="行为识别论文笔记：行为分类深度模型的总结.md">
                <i class="fa fa-chevron-left"></i> 行为识别论文笔记：行为分类深度模型的总结.md
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <div class="ds-share flat" data-thread-key="posts/679115823/"
     data-title="行为识别论文笔记：行为分类深度模型的总结.md"
     data-content=""
     data-url="http://yoursite.com/posts/679115823/">
  <div class="ds-share-inline">
    <ul  class="ds-share-icons-16">

      <li data-toggle="ds-share-icons-more"><a class="ds-more" href="javascript:void(0);">分享到：</a></li>
      <li><a class="ds-weibo" href="javascript:void(0);" data-service="weibo">微博</a></li>
      <li><a class="ds-qzone" href="javascript:void(0);" data-service="qzone">QQ空间</a></li>
      <li><a class="ds-qqt" href="javascript:void(0);" data-service="qqt">腾讯微博</a></li>
      <li><a class="ds-wechat" href="javascript:void(0);" data-service="wechat">微信</a></li>

    </ul>
    <div class="ds-share-icons-more">
    </div>
  </div>
</div>
      
    </div>
  </div>

          
          </div>
          

  <p>热评文章</p>
  <div class="ds-top-threads" data-range="weekly" data-num-items="4"></div>


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="posts/679115823/"
           data-title="行为识别论文笔记：行为分类深度模型的总结.md" data-url="http://yoursite.com/posts/679115823/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/avatar.jpg"
               alt="Jacob Kong" />
          <p class="site-author-name" itemprop="name">Jacob Kong</p>
          <p class="site-description motion-element" itemprop="description">欢迎来到孔伟杰（@JacobKong_Dev）的博客。 本人目前是北大信工的研一菜鸟一枚。 研究兴趣：计算机视觉|深度学习|行人检测。 欢迎大家一块儿交流！</p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">28</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">15</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="mailto:kongweijiejacob@gmail.com" target="_blank" title="Email">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                  Email
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://github.com/JacobKong" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/JacobKong_Dev" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/2232756824" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.jianshu.com/u/81a8d024af56" target="_blank" title="简书">
                  
                    <i class="fa fa-fw fa-heartbeat"></i>
                  
                  简书
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Visual-object-tracking"><span class="nav-number">1.</span> <span class="nav-text">Visual object tracking</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Learning-Policies-for-Adaptive-Tracking-with-Deep-Feature-Cascades"><span class="nav-number">1.0.1.</span> <span class="nav-text">Learning Policies for Adaptive Tracking with Deep Feature Cascades</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tracking-The-Untrackable-Learning-to-Track-Multiple-Cues-with-Long-Term-Dependencies"><span class="nav-number">1.0.2.</span> <span class="nav-text">Tracking The Untrackable: Learning to Track Multiple Cues with Long-Term Dependencies</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tracking-as-Online-Decision-Making-Learning-a-Policy-from-Streaming-Videos-with-Reinforcement-Learning"><span class="nav-number">1.0.3.</span> <span class="nav-text">Tracking as Online Decision-Making: Learning a Policy from Streaming Videos with Reinforcement Learning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Face-detection"><span class="nav-number">1.1.</span> <span class="nav-text">Face detection</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#S3FD-Single-Shot-Scale-invariant-Face-Detector"><span class="nav-number">1.1.1.</span> <span class="nav-text">S3FD - Single Shot Scale-invariant Face Detector.</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Salient-Object-Detection"><span class="nav-number">1.2.</span> <span class="nav-text">Salient Object Detection</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Amulet-Aggregating-Multi-level-Convolutional-Features-for-Salient-Object-Detection"><span class="nav-number">1.2.1.</span> <span class="nav-text">Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Learning-Uncertain-Convolutional-Features-for-Accurate-Saliency-Detection"><span class="nav-number">1.2.2.</span> <span class="nav-text">Learning Uncertain Convolutional Features for Accurate Saliency Detection</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Action-Related"><span class="nav-number">1.3.</span> <span class="nav-text">Action Related</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Encouraging-LSTMs-to-Anticipate-Actions-Very-Early"><span class="nav-number">1.3.1.</span> <span class="nav-text">Encouraging LSTMs to Anticipate Actions Very Early</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Unsupervised-Action-Discovery-and-Localization-in-Videos"><span class="nav-number">1.3.2.</span> <span class="nav-text">Unsupervised Action Discovery and Localization in Videos</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dense-Captioning-Events-in-Videos"><span class="nav-number">1.3.3.</span> <span class="nav-text">Dense-Captioning Events in Videos</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Learning-long-term-dependencies-for-action-recognition-with-a-biologically-inspired-deep-network"><span class="nav-number">1.3.4.</span> <span class="nav-text">Learning long-term dependencies for action recognition with a biologically-inspired deep network</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adaptive-RNN-Tree-for-Large-Scale-Human-Action-Recognition"><span class="nav-number">1.3.5.</span> <span class="nav-text">Adaptive RNN Tree for Large-Scale Human Action Recognition</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Ensemble-Deep-Learning-for-Skeleton-based-Action-Recognition-using-Temporal-Sliding-LSTM-networks"><span class="nav-number">1.3.6.</span> <span class="nav-text">Ensemble Deep Learning for Skeleton-based Action Recognition using Temporal Sliding LSTM networks</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pedestrian-Related"><span class="nav-number">1.4.</span> <span class="nav-text">Pedestrian Related</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#HydraPlus-Net-Attentive-Deep-Features-for-Pedestrian-Analysis"><span class="nav-number">1.4.1.</span> <span class="nav-text">HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pose-Estimation"><span class="nav-number">1.5.</span> <span class="nav-text">Pose Estimation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Towards-3D-Human-Pose-Estimation-in-the-Wild-A-Weakly-Supervised-Approach"><span class="nav-number">1.5.1.</span> <span class="nav-text">Towards 3D Human Pose Estimation in the Wild: A Weakly-Supervised Approach</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Object-Detection"><span class="nav-number">1.6.</span> <span class="nav-text">Object Detection</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Flow-Guided-Feature-Aggregation-for-Video-Object-Detection"><span class="nav-number">1.6.1.</span> <span class="nav-text">Flow-Guided Feature Aggregation for Video Object Detection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DeNet-Scalable-Real-time-Object-Detection-with-Directed-Sparse-Sampling"><span class="nav-number">1.6.2.</span> <span class="nav-text">DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Image-Recognition"><span class="nav-number">1.7.</span> <span class="nav-text">Image Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Multi-label-Image-Recognition-by-Recurrently-Discovering-Attentional-Regions"><span class="nav-number">1.7.1.</span> <span class="nav-text">Multi-label Image Recognition by Recurrently Discovering Attentional Regions</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jacob Kong</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"jacobkong"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  












  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("YzdOPGD6UVitIPREYUH017Yt-gzGzoHsz", "W0g9MHOvC23K6PV4r2loD7d0");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  

  


</body>
</html>
