<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[论文笔记：A Comprehensive Survey on Graph Neural Networks]]></title>
    <url>%2Fblog%2F4197138744%2F</url>
    <content type="text"><![CDATA[GNN的发展 Spectral graph theory: The ﬁrst prominent research on GCNs is presented in Bruna et al. (2013), which develops a variant of graph convolution based on spectral graph theory Since that time, there have been increasing improvements, extensions, and approximations on spectral-based graph convolutional networks Spatial-based graph convolutional networks: As spectral methods usually handle the whole graph simultaneously and are difﬁcult to parallel or scale to large graphs, spatial-based graph convolutional networks have rapidly developed recently Together with sampling strategies, the computation can be performed in a batch of nodes instead of the whole graph [24], [27], which has the potential to improve the efﬁciency. Others: In addition to graph convolutional networks, many alternative graph neural networks have been developed in the past few years. These approaches include graph attention networks, graph autoencoders, graph generative networks, and graph spatial-temporal networks. GNN vs Network embedding 两者属于相交的关系，交集是Deep learning Network embedding aims to represent network vertices into a low-dimensional vector space, by preserving both network topology structure and node content information, so that any subsequent graph analytics tasks such as classiﬁcation, clustering, and recommendation can be easily performed by using simple off-the-shelf learning machine algorithm Many network embedding algorithms are typically unsupervised algorithms and they can be broadly classiﬁed into three groups [32] matrix factorization [38], [39] random walks [40] deep learning approaches The deep learning approaches for network embedding at the same time belong to graph neural networks, which include graph autoencoder-based algorithms (e.g., DNGR [41] and SDNE [42]) and graph convolution neural networks with unsupervised training(e.g., GraphSage [24]). GNN分类，5类]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Graph Neural Networks</tag>
        <tag>图神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记：行为预测(Action Prediction / Anticipation)相关论文略读笔记]]></title>
    <url>%2Fblog%2F1352025799%2F</url>
    <content type="text"><![CDATA[论文一：Part-Activated Deep Reinforcement Learning for Action Prediction 现有的许多行为预测的方法会用到整个帧的演化来对动作建模，这不能避免当前动作所带来的噪声，特别是在早期预测中。为了解决这个问题，我们设计了PA-DRL，通过在深层强化学习框架下提取骨架proposal来开发人体结构。具体而言，我们从人体的不同part单独提取特征，并激活特征中与动作相关的部分以增强表征。 我们的方法不仅利用了人体的结构信息，而且还考虑了表达动作的显着部分。 我们在三个流行的动作预测数据集上评估我们的方法：UT-Interaction，BIT-Interaction和UCF101。 我们的实验结果表明，我们的方法通过最先进的技术实现了性能。 Temporal Recurrent Networks for Online Action Detection以前的预测方法仅根据历史信息来进行预判，而不利用未来信息 动机：1）与仅仅关注过去相比，联合建模当前行动识别和未来在训练中的行动预期将迫使网络学习更具辨别力的表示；2）明确预测将来会发生什么作为额外时间背景的来源，将有助于在测试时对当前行动进行分类。实验也有验证这一点，方法架构图如下： 主要是重新设计了一个RNNcell，这样在原本RNN的基础上额外加入了未来信息来达到目的。 Action Prediction from Videos via Memorizing Hard-to-Predict Samples这是AAAI-2018的一篇关于行为预测的文章。文章有提出行为预测的一个困难点在于：在某些动作中，由于视觉相似性，开始的几帧特征是不具有足够的辨别力以进行准确分类。如何解决这个问题对于行为预测的性能也是至关重要的，这样可以帮助分类器尽早的对行为进行分类，虽然最近的研究表明，当观察到一半长度的视频时，预测性能通常就会变得稳定，但还是有必要越早发现具有判别性的特征越好。因此本文提出了加入Memory机制，从而在训练阶段记住难以预测的训练样本。 为了帮助更好的预测，本工作使用了未来的信息。在实现上是采用具有前向连接和后向连接的双层双向LSTM来表征时间动作演变并捕获用于预测的未来信息。这一点和Bidirectional Attentive Fusion With Context Gating for Dense Video Captioning这篇论文的想法有点像。]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>行为识别</tag>
        <tag>ECCV 2018</tag>
        <tag>Action Prediction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记：CVPR 2018 关于行为识别论文略读笔记（二）]]></title>
    <url>%2Fblog%2F2879360315%2F</url>
    <content type="text"><![CDATA[论文五：PoTion: Pose MoTion Representation for Action Recognition和上面两篇论文类似，这篇文章主要是利用人体关键点（Keypoint）来做行为识别。目前的许多方法主要是双流网络来分别处理外观（appearance）和动态（motion）。在本篇文章中，作者引入了一种新颖的表示方式，可以优雅地编码某些语义关键点的移动。我们使用人体关节作为这些关键点，编码后的维度固定的特征称为：PoTion，将该特征图输送到简单的CNN中即可用用来行为识别分类。方法框架图如下： 方法大致流程为：首先在每个帧中运行目前最先进的人体姿态估计器，并为每个人体关节获取热图。这些热图对每个像素的概率进行编码以包含特定的关节。我们使用取决于视频片段帧的相对时间的颜色对这些热度图进行着色。如下图所示的为不同通道下的随时间的上色机制： 对于每个关节，我们对所有帧上的彩色热图进行求和，以获得整个视频片段的PoTion表示。如下图所示为某一关节点聚合之后的色彩图，使用了不同的聚合方式： 给定这种表示形式，我们训练一个浅层CNN架构，包含6个卷积层和一个完全连接的层来执行动作分类，CNN结构如下： 整个这个网络可以从头开始训练，并胜过其他姿势表示商法。而且，由于网络很浅并且以整个视频clip的紧凑表示为输入，因此训练例如非常快速。在一台用于HMDB的GPU上只需要4个小时，而标准的双流方法则需要几天的培训和仔细的初始化[5,43]。另外，PoTion可以看做是标准外观和运动流的补充。与RGB和光学流程的I3D [5]结合使用时，我们在JHMDB，HMDB，UCF101上获得了最先进的性能。 论文六：Im2Flow: Motion Hallucination from Static Images for Action Recognition这是今年CVPR 2018中在静态图像中做行为识别的一篇文章。静态图像动作识别需要系统识别发生在单张照片中的行为。 该问题对于基于人类行为和事件组织照片集合（例如，在网络，社交媒体上的照片）具有实际意义。现有的一些方法都仅仅依据图像的表象特征——物体、场景和肢体姿势来区分单张静态图像中的动作，但是这样的方法会忽略图像中所包含的丰富的动态结构和动作。为了挖掘单张图片所包含的motion信息，本文提出了一种为单张图片产生类似光流的图像，表示图像中所蕴含的未来的可能的motion。 其中一个关键的想法是：从数千个未标记的视频中学习一个先验的短期动态，一次来在新的静态图像上推断的预期光流，然后训练利用RGB流和光流来进行的动作的识别。框架图如下： 方法大致流程为：首先通过观察上千个包含各种动作的未标注的视频中学习一个动作的先验知识（motion prior）；然后利用下图中的Image-to-image translation模型，去将一个RGB图像转化为上图右边类似的光流图，改光流图是3通道的，前两个通道是motion angle $\theta\in[0,2\pi]$，第三个通道是幅度M。最后通过RGB图像和得到的光流图像去共同进行行为识别。 本方法很重要的一个创新点：结合了GAN中的Image-to-image translation模型来仅仅为静态图像即可生成相应的光流图，所预测出的光流是十分准确的，同时也提升了行为识别的准确性，思想值得借鉴。 论文七：Compressed Video Action Recognition这是今年CVPR 2018中做行为识别的另一篇文章。本文很重要的一个创新点是：利用压缩视频作为输入来进行视频中的行为识别。原始的视频帧数据往往具有巨大的尺寸而且高时间冗余，关于动作的有用信息很容易淹没在许多不相关的背景数据中。利用压缩后的视频具有许多好处：首先由于视频压缩（使用H.264，HEVC等）可将原始视频信息量降低两个数量级；其次视频中压缩中的motion vector提供了额外的motion信息，这是RGB图像所不具备的；而且压缩视频排除了空间可变性，这样提高了模型的泛化性；最后压缩视频模型会更快更简单更准确；因此本文建议直接在压缩视频上训练深层网络。 大多数现代编解码器将视频分成I帧（内编码帧），P帧（预测帧）和零个或多个B帧（双向帧）。I帧是常规图像并且被压缩。P帧引用前面的帧并仅编码相对前一帧所需要的‘变化’。B帧可以被视为特殊的P帧，其中运动向量是双向计算的，并且只要在参考中没有循环就可以引用未来帧。 本文工作将视频压缩后得到的用于描述两帧之间变化的motion vector、residual帧图像（如下图所示）作为各种SOTA网络中输入进行行为识别，都得到了很明显的提升，而且更快速、更简单、更准确。 论文八：What have we learned from deep representations for action recognition?这是今年CVPR 2018中对行为识别任务时空特征研究的一篇探究性文章。这篇文章通过可视化双流网络具体所学习到的时空特征来探究视频中行为识别这一任务。我们展示了用于外观和运动物体的局部检测器，以形成用于识别人类行为的分布式表示。 目前的可视化方法有三类： Visualization for given inputs. Activation maximization. Generative Adversarial Networks (GANs). 本文的可视化方法是基于activation maximization并将它们扩展到时空域，以便在双流融合模型中找到各个单元的首选时空输入，我们将问题表述为在输入空间中搜索的（正则化的）基于梯度的优化问题，方法框架如下图所示。 方法流程为：随机初始化的输入呈现给我们模型的光流和外观路径。 我们计算特征图一直到我们想要可视化的特定图层。 选择单个目标特征通道c，并且执行激活最大化（activation maximization）以分两步产生优选输入（preferred input）。 首先，影响c的输入上的导数是通过将目标损耗（在所有位置上求和）反向计算到输入层来计算的。 其次，通过学习速率缩放传播的梯度并将其添加到当前输入。 这些操作由图2中的虚线红线说明。基于梯度的优化以自适应降低的学习速率迭代地执行这些步骤，直到输入收敛为止。 重要的是，在此优化过程中，网络权重不会改变，只有输入接收更改。 主要的结论有如下四点： 第一：相比于分开单独学习外观以及运动特征，cross-stream可以学习到真正的时空特征 第二：网络可以学习高度类别已知的本地表示，也可以学习适用于一系列类的通用表示。 第三：通过网络结构，特征变得更加抽象并且对于对获得期望的分布不重要的数据（比如不同速度间的motion模式）显示出逐渐增加的不变性。 第四：可视化不仅可用于揭示学习的表示，还可用于揭示训练数据的特性并解释系统的失败情况。 论文八：Non-local Neural Networks这是今年CVPR 2018中做利用Non-local方法去做行为识别的一篇文章。long-range的依赖捕捉是深度学习的核心问题，对于序列数据，recurrent操作是主要的方法，对于图像数据，long-range依赖通过多层的卷积形成的感受野来捕捉。但卷积和recurrent操作都是local的操作，在本文中，受启发于计算机视觉的经典non-local均值法，作者提出了进行non-local操作的通用构建块系列，来捕获long-range依赖性的。本文中的non-local操作将所有位置处的特征的加权和作为某一位置的响应。直观一点，如下图所示： 使用非本地操作有几个优点： （a）与循环和卷积操作的渐进行为相反，非本地操作通过计算任意两个位置之间的相互作用直接捕获长程依赖性，而不管它们的位置距离如何; （b）正如我们在实验中所表明的那样，非局部操作是有效的，即使只有几层（例如5）也能达到最佳效果; （c）最后，我们的非本地操作保持可变输入大小，并且可以很容易地与其他操作组合（例如，我们将使用的卷积）。 该non-local构建块可以集成到目前许多计算机视觉框架中。通过在视频分类任务上进行验证，我们的non-local模型也可以在Kinetics和Charades数据集上取得很有竞争力的结果；在静态图像识别中，我们的non-local模型提高了在COCO数据集上的对象检测/分割和姿态估计任务，证明了模型的有效性。 ​]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Action Recognition</tag>
        <tag>行为识别</tag>
        <tag>CVPR 2018</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行为识别论文笔记：Something about Temporal Reasoning]]></title>
    <url>%2Fblog%2F3309988052%2F</url>
    <content type="text"><![CDATA[在视频的行为识别中，影响性能很重要的一点：就是模型能否提取出强有力的时间信息。虽然有的行为光从单张图像的空间特征就能大概判断出其中所包含的动作是什么，但是还是有很多动作需要从其随时间的变化才能准确判断出来。最近看了几篇关于视频中时间推理（Temproal Reasioning）的文章，这里顺便整理一下。 论文一：Temporal Relational Reasoning in Videos时间关系推理是指连接物体或实体随时间有意义变化的能力。受启发于Relation Network，本文提出了一个时间关系网络（Temporal Relation Network ，TRN），用来在多时间尺度上学习并推理视频帧之间的时间依赖关系，该网络可以思想很简单，可以很容易的集成到现有的卷积神经网络中。 首先本文定义了时间关系（Temporal Relations）。给定一段视频V，选取n个有序的视频帧序列${f_1,f_2,…,f_n}$，则第$f_i$和$f_j$帧的关系定义如下： T_2(V)=h_\delta(\sum_{i]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Action Recognition</tag>
        <tag>行为识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记：CVPR 2018 关于行为识别论文略读笔记（一）]]></title>
    <url>%2Fblog%2F3799204522%2F</url>
    <content type="text"><![CDATA[论文一：Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition这是今年CVPR 2018中做行为识别的一篇文章，提出了一个叫做光流引导的特征（Optical Flow guided Feature，OFF）。时间信息是视频行为识别的关键，二光流可以很好的表征时间信息，其在视频分析领域已经被很多工作证明是一个很有用的特征。但是目前的双流网络Two-Stream在训练时其实还是比较麻烦的，因为需要单独对视频提取光流图，然后送到网络的另一至进行训练；而且如果数据集很大的话，光流图和RGB图像合起来得有原视频数据大小的好几倍，也十分消耗硬盘空间。因此思考如何利用单流网络同时利用RGB特征以及类似光流的特征去进行训练是一个值得思考的问题。本文从光流本身的定义出发，给了我们一个关于该问题很好的启发。该方法也在UCF-101逮到了96%的分类准确率，超过了不用Kinetics数据集预训练的I3D模型，可见该方法的有效性。 本文提出的光流引导特征（OFF），它使网络能够通过快速和稳健的方法提取时间信息。 OFF由光流的定义导出，并与光流正交。该特征由水平和垂直方向上的特征图的空间梯度以及从不同帧的特征图之间的差异获得的时间梯度组成，OFF操作是CNN特征上的像素级运算，而且所有操作都是可导的，因此整个过程是可以端到端训练的，而且可以应用到仅有RGB输入的网络中去同时有效提取空间和时间特征。 论文二：Recognize Actions by Disentangling Components of Dynamics这是今年CVPR 2018中做行为识别的另一篇文章。本文和第一篇论文的中心思想相似：都是想通过原始的RGB图像直接在网络中间接获得类似光流的特征，从而减少目前双流网络中计算光流模块导致的额外开销。因此本文提出了一个新的用于视频表征学习的ConvNet框架，其可以完全从原始视频帧中推导出动态信息，而不需进行额外的光流估计。具体网络框架如下： 大致流程为：给定一个连续的帧序列，该模型首先产生一些低级特征映射，然后将其馈入三个分支，分别是静态外观（Static Appearance，上），外观动态（Apparent Motion，中）和外观变化（Appearance Change，下）。 这些分支分别计算其对应的高级特征并进行预测。 最后，这些预测被合并为最终的预测。最后，3个组件预测出的结果将通过求平均的方式融合到一起生成最终的预测。 其中在静态外观分支，通过迭代地应用2D卷积，空间2D池化和时间1D池化来逐渐提取外观特征；在外观动态分支，主要提取视频帧中特征点的空间位移，主要第一次引入了Cost Volume来进行外观动态的估计；在外观变化分支中，由于不是所有的变化都能够通过外观动态表解释，诸如物体外观的固有变化或照明变化的其他因素也可能导致视频帧的变化，不同于以前使用RGB-diff的方法，本文提出了一个叫做warped differences的方法来表征外观变化。 通过在UCF101和Kinetics两个数据集上进行验证，本文的方法在仅使用RGB图像帧的前提下也能取得很有竞争力的结果，而且具有很高的效率，证明了方法的优越性和有效性。 论文三：2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning这是今年CVPR 2018中利用姿态做行为识别的一篇文章，主要突出了一个多任务网络来同时做2D和3D的姿态估计以及2D和3D的行为识别，同时利用姿态估计的结果来促进行为识别任务的性能。这也是解决问题的一个很好的出发点，就是利用两个任务来互相促进。 下图是网络的整体框架图，输入静态的RGB图像，同时进行姿态估计和行为识别。其中的姿态估计模型是利用基于回归的方法，其中利用了一个可微分的Softargmax来联合2D和3D的姿态估计。其中的动作识别方法分为两部分，一部分基于身体关节坐标序列，我们称之为基于姿态的识别，另一部分基于一系列视觉特征，我们称其为基于外观的识别。 将每个部分的结果组合起来估计最终的动作标签。 作者在MPII, Human3.6M, Penn Action 和 NTU四个数据集上进行了实验，验证了模型在两个任务上的有效性。 本文值得借鉴的一个思想就是：利用多任务之间的互相促进，来提升各自任务的有效性。 论文四：Deep Progressive Reinforcement Learning for Skeleton-based Action Recognition这是今年CVPR 2018中基于骨架（Skeleton-based）来做行为识别的一篇文章，但是一个重要的创新点是利用增强学习首先找到一段视频帧中最具动作代表性的帧，丢弃掉序列中的不明确帧，然后利用基于图的神经网络来捕捉关节连接点之间的依赖关系，从而达到行为识别的目的。框架图如下： 方法大致流程为：给定一个人体关节的视频，我们首先选择框架提取网络（FDNet）来提取视频中的关键帧，这是由提出的深度渐进式强化学习方法进行训练所得到。 我们根据两个重要因素逐步调整每个状态下的选定帧。 一个是所选帧用于动作识别的所具备的判别能力。 另一个是所选帧与整个动作序列的关系。然后，我们采用基于图的卷积神经网络（GCNN），它保留了人体关节之间的依赖关系，以处理所选关键帧以进行动作识别。 本文的方法在三个广泛使用的数据集上实现了非常有竞争力的性能。]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Action Recognition</tag>
        <tag>行为识别</tag>
        <tag>CVPR 2018</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行为检测论文笔记：One-shot Action Localization by Learning Sequence Matching Network]]></title>
    <url>%2Fblog%2F3531717168%2F</url>
    <content type="text"><![CDATA[这是今年CVPR 2018中接受为数不多的动作时间轴定位论文中的另一篇，基于学习的时间轴动作定位方法需要大量的训练数据。 然而，这样的大规模视频数据集不仅非常难以获得而且可能因为存在无数的动作类别而不实用。 当训练样本少且罕见时，当前方法的弊端就暴露出来了。为了解决这个挑战，本文的解决方案是采用匹配网络的One-shot学习技术，并利用相关性来挖掘和定位以前没有看过类别的行为。 本文在THUMOS14和ActivityNet数据集上评估了本文的one-shot动作定位方法。 背景 现在显存的基于深度学习的行为定位的方法都采用很强的监督学习策略，需要大量的标注数据，非常耗时去收集。 虽然转移学习或模型预训练可能在一定程度上缓解了这个问题，但处理新的动作类别和将学习的网络模型以高数据效率适应到新场景中仍然具有挑战性。 在本文中，考虑one(few)-shot的动作定位学习场景：给出一个（或几个）新动作类的例子，通常每个类一个例子，我们的目标是检测未修剪视频中所有出现的每个类。 目前很少有工作将one-shot learning应用到检测时空特征目标中。 目的为缓解目前用来训练的动作视频数据量稀少且难以获取的问题，利用one(few)-shot learning的方法，通过少量训练样本即可达到时间轴定位以及未见过的动作的预测，提高模型的泛化性能。 方法论文框架如下： 本文开发了一种新颖的元学习（meta-learning）策略，将视频序列匹配的任务级先验知识集成到学习动作定位中。 我们的一次学习策略的关键思想是动作视频的直观，结构化表示适合于用来匹配（部分）序列（matching sequences），以及一种相似性度量，它允许我们将动作示例的标签转换为未修剪视频中的动作提议。 本文提出了一个新的Matching Network结构，首先生成许多proposal，然后这些proposal送入到三个网络组件中进行动作标签预测： Video Encoder Network：它为每个行动建议和参考行动计算一个segment-based的动作表示，它维护行动的时间结构并用于准确定位。 该网络的性能依赖于候选proposal和参考视频之间的能否良好对应，为了实现准确对齐，我们打算在行动表示中保留动作视频的时间结构。 为此，我们开发了一个利用ranking LSTM来获得segment-based的视频表示，以便将每个动作实例编码为固定长度序列的视频片段特征。 Similarity Network: 将行动建议利用相似性网络与每个参考行动（reference action）进行比较，该网络在每个时间步骤生成一组相关分数。 类似于Matching Network，网络首先计算每个单独示例$x_i$相对于整个支持集（support set）的完整上下文嵌入 然后给定一个行动建议$\hat{x}$及其编码向量$g(x_i)$，相似性网络计算提议表示与所有示例之间的余弦距离： 接着基于上述距离，原始匹配网络使用关注机制和投票策略将测试数据分类到支持集中的一个类中 我们注意到，由于支持集只由前景类组成，这种分类方法不适用于定位任务，我们还必须区分前景和背景。 因此，在本文的 one-shot action localization 架构中，我们使用相似性网络计算相关分数，并设计一个单独的标签网络来推断每个提案的类别标签（包括背景）。 Labeling Network: 设置不同长度的时间窗口，根据时间窗口内的encoding vector和correlation scores，第三个网络会在每个时间步骤中预测提案的动作类别标签（作为前景类别或背景之一）。 标记网络直接应用于相关矩阵。 类似于通过比较余弦距离进行分类，我们通过在短暂的时间跨度上在相关矩阵上应用全连接层来比较相关矩阵的相同列的不同行，并且输出在前景和通过sigmoid激活的背景。 全连接网络沿时间维度滑过相关矩阵，为每个提案确定前景/背景。 在标签网络确定为前景的提议中，公式6应用于预测动作标签，如框架图的上半部分所示。 相关矩阵包含有关特定特定类的信息以及提案是属于背景还是属于前景。 例如，如果与一个示例的关联比其他示例的关联高得多，那么该提议很可能属于前景并且与示例具有相同的动作标签; 如果与所有例子的相关性都比较低，那么它可能属于背景。 标签网络通过训练学习这些标准。 还要注意标签网络在某种意义上独立于动作类别，因为它应用于相关矩阵而不是每个视频的特征表示。 这意味着它学到的标准应该适用于输入来自不同类别的视频，这使得它适用于一次性预测。 后处理阶段：我们结合多尺度 proposal-level 预测来获得frame-level的单帧级预测，并将相同标号的相邻帧分组以获得动作实例。 本文的定位系统是针对 one-shot action localization 而设计的，相应的 Meta Learning Formulation 可用于模型的训练。 系统的每个组成部分都是可导的，并且可以对系统进行端到端的培训。 然而，为了获得更好的初始化和性能，我们对Video Encoder Network和Similarity Network进行了预训练。 Meta Learning Formulation 在元学习中，模型在一组训练任务的元阶段进行训练，并对另一组测试任务进行评估 元学习的目标是在元测试任务分布中找到最小化损失的模型 Optimization for the Localization System 损失函数为： 其中，两个损失函数分别如下： Pretraining for Video Encoder &amp; Similarity Net 预训练两者的损失函数： 利用到了一个Ranking Loss，其直观的想法是：当给予越来越多的视频内容，分类器会生成越来越置信度高的预测 实验 实验在Thumos14和ActivityNet 1.2上进行评估 One-shot问题设置要求测试期间的类别不得存在在训练期间，因此在训练和测试过程中需要对两个数据集进行划分，从而达到这一要求 训练和测试阶段有很多细节，建议看论文，这里不再赘述 和强监督方法的比较，我们从表1和表2可以看出，虽然one-shot和全监督行为检测之间仍然存在性能差距，但我们的方法在以one-shot设置进行测试时，显着优于目前最先进的方法。 我们在Thumos14数据集上使用更多的训练数据，每类15个样本进一步测试我们的方法。 我们的结果表明，性能出现明显提升，而CDC只有很小的提升，这表明我们的方法相对于样本数量具有良好的可扩展性。 还有一些消融探究实验，这里也不再赘述 优点 针对目前视频数据集标注费时、难以获得等痛点，作者首次将one(few)-shot的方法引入了时间轴定位，很有创新点，和弱监督学习有异曲同工之妙 提出了一种基于匹配网络框架的动作定位问题的元学习方法，它能够捕获任务级别（task-level）的先验知识 后处理中的grouping策略可以要用在我之前的工作中 缺点 性能目前看来并不是特别理想，和全监督的方法还有差距，还有很多改进的空间，这可能需要多去了解一些one-shot learning或transfer learning等方向的论文。]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Action Detection</tag>
        <tag>行为检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行为检测论文笔记：Rethinking the Faster R-CNN Architecture for Temporal Action Localization]]></title>
    <url>%2Fblog%2F3697434189%2F</url>
    <content type="text"><![CDATA[这是今年CVPR 2018中接受为数不多的动作时间轴定位论文中的一篇，解决了目前现存方法中的3个问题：（1）Multi-scale的动作片段；（2）Temproal context的利用；（3）Multi-stream 特征融合。方法在THUMOS’ 14数据集上的提议和检测任务上达到目前最好的效果（mAP@tIoU=0.5达到42.8%），在ActivityNet数据及上取得了具有挑战性的效果。 背景 时间轴行为检测其实和目标检测相类似，因此目前许多行为检测的方法都受启发于目标检测的一些先进方法，比如R-CNN系列，先从整个视频中生成segments proposal，然后用分类器去对这些proposal进行分类。 目前有一些方法将Faster R-CNN迁移到时间轴行为检测中，然而直接迁移过来引入一些挑战，如下： 如何处理行动持续时间的巨大变化？ 因为行为会有许多时间长短不一的持续时间，从几秒到几分钟的行为片段都有，而Faster R-CNN利用anchor提proposal会在特征的temporal scope和anchor的span之间产生misalignment现象。我们提出了一个multi-tower网络和利用扩张时间卷积（dilated temporal convolutions）来解决alignment的问题。 如何利用时间上下文信息？ 动作实例之前和之后的时刻包含关于定位和分类的关键信息（可以说比对象的空间上下文更重要）。Faster R-CNN没有利用时间上下文信息。我们建议通过扩展提案生成和动作分类中的感受野来明确地编码时间上下文。 如何最好的去融合multi-stream的特征？ 对于Faster R-CNN探索这种RGB和Flow特征融合方面的工作有限。 我们提出了一个后期融合方案，并且经验性地证明了它在一般的早期融合方案上的优势。 目的解决Faster R-CNN直接引入到时间轴行为检测中的上述3个挑战,并以此来提升Faster R-CNN在行为检测中的性能. 方法论文框架如下： 本文提出了TAL-Net，有三个创新的结构改变： Receptive Field Alignment 传统的anchor机制有一个缺点：每个时间点的锚点分类都有相同的单一的感受野。 为了解决这个问题，我们建议将每个锚点的感受野与它的时间跨度对齐。 这是通过两个关键因素实现的：multi-tower网络和扩张时间卷积（dilated temporal convolutions）。 给定一个一维feature map，我们的Segment Proposal Network 由K个temproal ConvNets 组成，每个K网络负责对特定比例的锚段进行分类.最重要的是，每个时间ConvNet都经过精心设计，使得其接受的字段大小与相关的锚点尺度一致。 在每个ConvNet结束时，我们分别应用两个核心大小为1的平行卷积层进行锚定分类和边界回归。 另一问题：如何设计具有可控感受野s的时间卷积？ 方法一：如果s=2L+1，则叠加L层卷积层得到相应的感受野。缺点是层数L随着s线性增加，很容易增加参数数量使网络过拟合。 方法二：在每一层卷积层后添加一个kernel size为2的pooling层，则感受野$s=2^{(L+1)}-1$，此时层数随着s成log变化，但是添加pooling层会减小输出feature map的分辨率，会影响定位准确率。 方法三：使用扩充时间卷积，这种卷积可以在扩充感受野的同时不损失分辨率。在我们的Segment Proposal Network中，每一个temporal ConvNet都只由2个dilated convolutional layers组成。为了获得一个目标感受野s，则第一层的dilated convolutional layers的dilation rate $r_1=s/6, r_2=(s/6)\times2$. Context Feature Extraction 时间轴上下文信息十分重要 为了确保上下文特征用于锚定分类和边界回归，感受野必须覆盖时间轴上下文信息区域，可以通过将dilation rate加倍，即$r_1=s/6\times2, r_2=(s/6)\times2\times2$，如下： 在动作分类阶段，我们要利用SoI pooling来为每个proposal提取一个固定尺寸的feature map Late Feature Fusion 目前许多方法都在使用RGB和光流特征 本文为双流特征提出了一个后融合的机制 们首先使用两个不同的网络分别从RGB帧和叠加的光流中提取两个一维特征映射。 我们通过一个不同的Segment Proposal Network来处理每个feature ma，该网络并行地生成锚定分类和边界回归的逻辑。 我们使用来自两个网络的logits的元素平均值作为最终的逻辑来生成提议。 对于每个提案，我们在两个特征映射上并行执行SoI池，并在每个输出上应用不同的DNN分类器。 实验 基于TensorFlow目标检测API 9个anchor，scales为{1, 2, 3, 4, 5, 6, 8, 11, 16} NMS阈值为0.7去筛选proposal，保留前300个proposal用于分类 THUMOS’ 14检测结果 ActivityNet v1.3在验证集的检测结果 优点相比于R-C3D，本文的方法解决了Multi-scale的问题，利用了上下文信息以及额外的光流信息，解决了目前许多方法中存在的大大小小的缺陷，组合成了一个较为完整的框架，因此在THUMOS’ 14数据集上检测效果达到最好，在ActivityNet数据集上也取得了很有竞争力的结果，但是还是不如SSN的结果。文中分析：THUMOS’ 14是一个更好的用来评估行为定位的数据集，因为其每段视频中包含有更多的行为实例，并且每段视频包含大量的背景活动。 缺点我认为除了第一点创新：利用dilated temporal convlutional组成感受野可控的multi-tower网络来解决multi-scale问题比较有创新外，另外两点创新其实不算特别有新意。]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Action Detection</tag>
        <tag>行为检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文调研：ICCV 2017论文调研]]></title>
    <url>%2Fblog%2F679115822%2F</url>
    <content type="text"><![CDATA[Visual object tracking Learning Policies for Adaptive Tracking with Deep Feature Cascades Our fundamental insight is to take an adaptive approach, where easy frames are processed with cheap features (such as pixel values), while challenging frames are processed with invariant but expensive deep features. Formulate the adaptive tracking problem as a decision-making process. Learn an agent to decide whether to locate objects with high conﬁdence on an early layer, or continue processing subsequent layers of a network. Signiﬁcantly reduces the feedforward cost. Train the agent ofﬂine in a reinforcement learning fashion. Obviously, the major computational burden comes from the forward pass through the entire network, and can be larger with deeper architectures. However, when the object is visually distinct or barely moves, early layers are in most scenarios sufﬁcient for precise localization - offering the potential for substantial computational savings. The agent learns to ﬁnd the target at each layer, and decides if it is conﬁdent enough to output and stop there. Tracking The Untrackable: Learning to Track Multiple Cues with Long-Term Dependencies Combine cues in a coherent end-to-end fashion over a long period of time. Present a structure of Recurrent Neural Networks (RNN) that jointly reasons on multiple cues over a temporal window. We are able to correct many data association errors and recover observations from an occluded state. Tracking as Online Decision-Making: Learning a Policy from Streaming Videos with Reinforcement Learning A tracking agent must follow an object despite ambiguous image frames and a limited computational budget. The agent must decide where to look in the upcoming frames when to reinitialize because it believes the target has been lost when to update its appearance model for the tracked object Formulating tracking as a partially observable decision-making process (POMDP). Sparse rewards allow us to quickly train on massive datasets. Challenges: First, the limited quantity of annotated video data impedes both training and evaluation. Second, as vision (re)integrates with robotics, video processing must be done in an online, streaming fashion. Face detection S3FD - Single Shot Scale-invariant Face Detector. Use a single deep neural network, especially for small faces. Contribution 提出一个尺度公平的人脸检测框架来处理不同尺度的人脸。我们在各种各样的图层上拼贴anchor，以确保所有人脸的比例尺都具有足够的特征用于检测。基于有效接收域（effective receptive ﬁeld）和等比例区间原则（equal proportion interval principle）设计anchor 用尺度补偿anchor匹配策略（ a scale compensation anchor matching strategy）提高小脸的召回率; 通过最大化背景标签（ max-out background label）减少小脸的误报率。 effective receptive ﬁeld: Understanding the effective receptive ﬁeld in deep convolutional neural networks. Salient Object Detection Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection How to better aggregate multi-level convolutional feature maps for salient object detection is underexplored. Our framework: First integrates multi-level feature maps into multiple resolutions, which simultaneously incorporate coarse semantics and ﬁne details. Then it adaptively learns to combine these feature maps at each resolution and predict saliency maps with the combined features. Finally, the predicted results are efﬁciently fused to generate the ﬁnal saliency map. In addition, edge-aware maps and high-level predictions are embedded into the framework. Learning Uncertain Convolutional Features for Accurate Saliency Detection The key contribution of this work is to learn deep uncertain convolutional features (UCF), which encourage the robustness and accuracy of saliency detection. 我们提出了一种有效的混合上采样方法来减少我们的解码器网络中去卷积算子的棋盘伪影。 We ﬁnd that the actual cause of these artifacts is the upsampling mechanism, which generally utilizes the deconvolution operation. Action Related Encouraging LSTMs to Anticipate Actions Very Early Action anticipation - identify the action from only partially available videos. To this end, we develop a multi-stage LSTM architecture that leverages context-aware and action-aware features, and introduce a novel loss function that encourages the model to predict the correct class as early as possible. Intuitive: our loss models the intuition that some actions, such as running and high jump, are highly ambiguous after seeing only the ﬁrst few frames, and false positives should therefore not be penalized too strongly in the early stages. We would like to predict a high probability for the correct class as early as possible, and thus penalize false negatives from the beginning of the sequence. Contribute a novel multi-stage Long Short Term Memory (LSTM) architecture for action anticipation. This model effectively extracts and jointly exploits context- and action-aware features. Existing method drawbacks: This is in contrast to existing methods that typically extract either global representations for the entire image or video sequence thus not focusing on the action itself, or localize the feature extraction process to the action itself via dense trajectories optical ﬂow or actionness, thus failing to exploit contextual information. 利用光流不允许这些方法在localization过程中明确地利用外观。 Computing optical ﬂow is typically expensive. In the future, we intend to study new ways to incorporate additional sources of information, such as dense trajectories and human skeletons in our framework. Unsupervised Action Discovery and Localization in Videos 创新：无监督的action localization。 First to address the problem of unsupervised action localization in videos. We propose a novel approach that: Discovers action class labels Spatio-temporally localizes actions in videos. Method: It begins by computing local video features to apply spectral clustering on a set of unlabeled training videos. For each cluster of videos, an undirected graph is constructed to extract a dominant set, which are known for high internal homogeneity and in-homogeneity between vertices outside it. Next, a discriminative clustering approach is applied, by training a classiﬁer for each cluster, to iteratively select videos from the non-dominant set and obtain complete video action classes. Once classes are discovered, training videos within each cluster are selected to perform automatic spatio-temporal annotations, by ﬁrst over-segmenting videos in each discovered class into supervoxels（超体素） and constructing a directed graph （有向图）to apply a variant of knapsack problem with temporal constraints. （并构建有向图以应用具有时间约束的背包问题的变体。） 背包优化联合收集超体素的一个子集，通过强制注释的动作进行时空连接，其体积是一个actor的大小。These annotations are used to train SVM action classiﬁers. 在测试过程中，操作使用类似的背包方法来进行localize，在这种方法中将超体素分组在一起，并且使用来自发现的动作类的视频学习的SVM被用于识别这些动作。 However, supervised algorithms have some disadvantages compared to unsupervised approaches, due to the difﬁculty of video annotation. Contributions： Automatic discovery of action class labels using a new discriminative clustering approach with dominant sets (Sec. 3). We propose a novel Knapsack approach with graph-based temporal constraints to annotate actions in training videos The annotations within each cluster of videos are jointly selected by Binary Integer Quadratic Programming (BIQP) optimization to train action classiﬁers. Structural SVM is used to learn the pairwise relations of supervoxels within foreground action and foreground-background, which enforces that the supervoxels belonging to the action to be simultaneously selected. Lastly, we address a new problem of Unsupervised Action Localization (Sec. 5.2). Dense-Captioning Events in Videos We introduce the task of dense-captioning events, which involves both detecting and describing events in a video. Our model introduces a variant of an existing proposal module that is designed to capture both short as well as long events that span minutes. To capture the dependencies between the events in a video, our model introduces a new captioning module that uses contextual information from past and future events to jointly describe all events. While the success of these methods is encouraging, they all share one key limitation: detail. We introduce the task of dense-captioning events, which requires a model to generate a set of descriptions for multiple events occurring in the video and localize them in time. However, we observe that densecaptioning events comes with its own set of challenges distinct from the image case. One observation is that events in videos can range across multiple time scales and can even overlap. Past captioning works have circumvented this problem by encoding the entire video sequence by mean-pooling [50] or by using a recurrent neural network (RNN) [49]. To overcome this limitation, we extend recent work on generating action proposals [10] to multi-scale detection of events. Another key observation is that the events in a given video are usually related to one another. We introduce a captioning module that utilizes the context from all the events from our proposal module to generate each sentence. Learning long-term dependencies for action recognition with a biologically-inspired deep network How to efﬁciently learn long-term dependencies from sequences still remains a pretty challenging task. As one of the key models for sequence learning, recurrent neural network (RNN) and its variants such as long short term memory (LSTM) and gated recurrent unit (GRU) are still not powerful enough in practice. One possible reason is that they have only feedforward connections, which is different from the biological neural system that is typically composed of both feedforward and feedback connections.(既有前传，也有反馈) Propose shuttleNet technologically. The shuttleNet consists of several processors, each of which is a GRU while associated with multiple groups of cells and states. Attention mechanism is then employed to select the best information ﬂow pathway. Adaptive RNN Tree for Large-Scale Human Action Recognition We present the RNN Tree (RNN-T), an adaptive learning framework for skeleton based human action recognition. Our method categorizes action classes and uses multiple Recurrent Neural Networks (RNNs) in a treelike hierarchy. 在骨架表示中的行为是通过分层推理过程来识别的，在这个过程中，单独的RNN将细化的行为类别与增加的置信度 RNN-T effectively addresses two main challenges of large-scale action recognition: able to distinguish ﬁne-grained action classes that are intractable using a single network adaptive to new action classes by augmenting an existing model. Ensemble Deep Learning for Skeleton-based Action Recognition using Temporal Sliding LSTM networks Traditional methods generally use relative coordinate systems dependent on some joints, and model only the long-term dependency, while excluding short-term and medium term dependencies. We transform the skeletons into another coordinate system to obtain the robustness to scale, rotation and translation and then extract salient motion features from them. We propose novel ensemble Temporal Sliding LSTM (TS-LSTM) networks for skeleton-based action recognition. The proposed network is composed of multiple parts containing short-term, medium-term and long-term TS-LSTM networks. With a rapid development of 3D data acquisition over the past few decades, lots of researches on human activity recognition from 3D data can have been actively performed. For the modeling of human actions, recent researches show that Long Short-Term Memory (LSTM) networks are superior to temporal pyramids and hidden markov models. Overall Method： Firstly, we transform the coordinates of input skeleton sequences so that the data can be robust to scale, rotation and translation. Secondly, instead of using the simple joint positions, we employ the motion features in terms of temporal differences, which help our networks to be focused on the actual skeleton movements. Thirdly, the motion features are processed with multi-term LSTMs containing short-term, medium-term and long-term LSTMs, which allow robustness to variable temporal dynamics. Finally, the multi-term LSTMs capture a variety of action dynamics through ensemble. What Actions are Needed for Understanding Human Actions in Videos? We analyze the current state of human activity understanding in videos. The goal of this paper is to examine datasets, evaluation metrics, algorithms, and potential future directions. The results demonstrate that while there is inherent ambiguity in the temporal extent of activities, current datasets still permit effective benchmarking. 我们发现，当与时间推理相结合时，对物体和姿态的细粒度理解很可能在算法精度上产生实质性的改善。 Some questions: What is an activity and how should we represent it? Do activities have well-deﬁned spatial and temporal extent? What role do goals and intentions play in deﬁning and understanding activities? What does the data show about the right categories for recognition in case of activities? Do existing approaches scale with increasing complexity of activities categories, video data, or temporal relationships between activities? Are the hypothesized new avenues of studying context, objects, or intentions worthwhile: Do these really help in understanding videos? This paper provides an in-depth analysis of the new generation of video datasets, human annotators, activity categories, recognition approaches, and above all possible new cues for video understanding. We found that people considered verbs to be relatively more ambiguous. This suggests that despite boundary ambiguity, current datasets allow us to understand, learn from, and evaluate the temporal extents of activities. That is, a perfect classiﬁer would automatically do 5 times better than current state-of-the-art [30] on activity localization. This suggests that focusing our attention on gaining more insight into activity classiﬁcation would naturally yield signiﬁcant improvements in localization accuracy as well. Having concluded that: (1) we should be reasoning about activities as (verb,object) pairs rather than just verb, (2) temporal boundaries of activities are ambiguous but nevertheless meaningful, and (3) classiﬁcation of short videos is a reasonable proxy for temporal localization This suggests that moving forward ﬁne-grained discrimination between activities with similar objects and verbs is needed. Lattice Long Short-Term Memory for Human Action Recognition However, naively applying RNNs to video sequences in a convolutional manner implicitly assumes that motions in videos are stationary across different spatial locations. This assumption is valid for short-term motions but invalid when the duration of the motion is long. In this work, we propose Lattice-LSTM (L^2STM), which extends LSTM by learning independent hidden state transitions of memory cells for individual spatial locations. Additionally, we introduce a novel multi-modal training procedure for training our network. An accurate action recognition should: (1) have a high capacity for learning and capturing as many motion dynamics as possible (2) when an action appears in sequential images, the neurons should properly decide what kind of spatio-temporal dynamics should be encoded into the memory for distinguishing actions. Common Action Discovery and Localization in Unconstrained Videos In this work, we tackle the problem of common action discovery and localization in unconstrained videos, where we do not assume to know the types, numbers or locations of the common actions in the videos. To perform automatic discovery and localization in such challenging scenarios, we ﬁrst generate action proposals using human prior. By building an afﬁnity graph among all action proposals, we formulate the common action discovery as a subgraph density maximization problem to select the proposals containing common actions. 为了避免在指数级大的解空间中枚举，我们提出了一个有效的多项式时间优化算法。 It solves the problem up to a user speciﬁed error bound with respect to the global optimal solution. Action discovery的困难： 首先，由于我们事先不知道在给定的数据集中常见的动作类型或位置，我们必须同时进行发现和定位。 给定一组未标记的视频，我们需要自动识别一组捕获常见操作的时空边界框。 其次，类似的行为也可能由于视点变化，尺度变化或相机运动而出现不同。 自动关联这些常见操作并不是一项简单的任务。 最后，除了常见的动作之外，视频还可能包含动态背景或不常见的动作，因此将这种“noisy motions”与常见动作区分开来是至关重要的。 Pedestrian Related HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis Learning of comprehensive features of pedestrians for ﬁne-grained tasks remains an open problem. HydraPlus-Net: multi-directionally feeds the multi-level attention maps to different feature layers. Advantages: (1) the model is capable of capturing multiple attentions from low-level to semantic-level (2) it explores the multi-scale selectiveness of attentive features to enrich the ﬁnal feature representations for a pedestrian image. We demonstrate the effectiveness and generality of the proposed HP-net for pedestrian analysis on two tasks, i.e. pedestrian attribute recognition and person reidentiﬁcation. However, the learning of feature representation for pedestrian images, as the backbone for all those applications, still confronts critical challenges and needs profound studies. However, existing arts merely extract global features [13, 24, 30] and are hardly effective to location-aware semantic pattern extraction. Multidirectional attention (MDA) modules Reliable 3D skeleton-based action recognition (SAR) is now feasible [1]. Although much progress has been achieved, these methods are still facing two challenges. We term the ﬁrst one as the discriminative challenge. We term the second challenge as adaptability. Pose Estimation Towards 3D Human Pose Estimation in the Wild: A Weakly-Supervised Approach We propose a weakly-supervised transfer learning method that uses mixed 2D and 3D labels in a uniﬁed deep neutral network that presents two-stage cascaded structure. Object Detection Flow-Guided Feature Aggregation for Video Object Detection Video object detection The accuracy of detection suffers from degenerated object appearances in videos, e.g., motion blur, video defocus, rare poses, etc. We present ﬂow-guided feature aggregation, an accurate and end-to-end learning framework for video object detection. It leverages temporal coherence on feature level instead. 它通过沿着运动路径聚集附近的特征来改进每帧特征，从而提高了视频识别的准确性。 Fast moving objects. DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling We deﬁne the object detection from imagery problem as estimating a very large but extremely sparse bounding box dependent probability distribution. （我们将图像问题中的目标检测定义为估计非常大但极其稀疏的边界框相关概率分布。） Two novelties: a corner based region-of-interest estimator a deconvolution based CNN model Image Recognition Multi-label Image Recognition by Recurrently Discovering Attentional Regions Current solutions for this task usually rely on an extra step of extracting hypothesis regions (i.e., region proposals), resulting in redundant computation and sub-optimal performance. Developing a recurrent memorized-attention module. This module consists of two alternately performed components: a spatial transformer layer to locate attentional regions from the convolutional feature maps in a region-proposal-free way an LSTM (Long-Short Term Memory) sub-network to sequentially predict semantic labeling scores on the located regions while capturing the global dependencies of these regions. Despite acknowledged successes, these methods take the redundant computational cost of extracting region proposals and usually over-simplify the contextual dependencies among foreground objects, leading to a sub-optimal performance in complex scenarios. ​ ​ ​ ​]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
        <tag>论文调研</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行为识别论文笔记：行为分类深度模型的总结.md]]></title>
    <url>%2Fblog%2F679115822%2F</url>
    <content type="text"><![CDATA[本次主要总结了目前常见一些经典的基于深度学习的行为分类模型。其中的主要内容来自于论文《Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset》中的Related Work部分的总结。 虽然近年来图像表示体系结构的发展已经迅速成熟，但视频的前端运行架构仍然不够清晰。当前视频体系结构中的一些主要差异在于convolutional and layers operators是使用2D（基于图像的）还是3D（基于视频的）kernels; 无论网络输入是RGB视频或者还是包含预先计算的光流，在2D ConvNets的情况下，对于信息如何跨帧传播，这可以通过使用诸如LSTM之类的temporally-recurrent layers或者随着时间的推移进行特征聚合来完成。 图1显示了我们评估的五种体系结构的图形概述。 模型1：ConvNet+LSTM图像分类网络的高性能使得我们尝试只需很少的改变就能将其重用于视频中。 通过使用它们针对每个独立帧提取特征，然后集中在整个视频中来提取预测结果来实现这一目标。 这是bag of words图像建模方法的精神; 但是在实践中使用方便的同时，还存在完全忽略时间结构的问题（例如，模型不能很好的区分开门和关门）。 理论上，更令人满意的方法是向模型添加一个recurrent layer，例如可以对状态进行编码的LSTM，并捕获时间顺序和长程依赖性。 本文在有512个隐藏单元的Inception-V1的最后的平均池化层之后防止了一个有着BN的LSTM层， 一个全连接层被添加到分类器的顶部。 该模型对所有时间步骤的输出上使用交叉熵损失进行训练。 在测试过程中，我们只考虑最后一帧的输出。 模型2：3D ConvNets3D ConvNets似乎是一种更自然的视频建模方法，其就像标准的卷积网络一样，但是具有时空卷积核。它们有一个非常重要的特征：它们直接创建时空数据的分层表示。这些模型的一个问题是，由于附加的内核维度，它们比2D ConvNets有更多的参数，这使得它们更难以训练。另外，它们似乎排除了ImageNet预训练的好处，因此以前的工作定义了相对较浅的架构，并且都是train from scratch。基准测试的结果具有提升的前景，但是与最新的技术水平相比还不具有竞争性，使得这种类型的模型成为评估我们大型数据集的好选择。 在本文中，我们实现了一个C3D模型的小变体，它在顶部有8个卷积层，5个池化层和2个完全连接的层。模型的输入与原始实现相同，使用112×112像素共16帧。与原始C3D不同的是，我们在所有卷积和全连接层之后使用了batch normalization。另一个区别在于对于第一个池化层，我们使用stride=2而不是stride=1，这减少了内存占用，并允许更大批量 - 这对于批量标准化非常重要。使用这一步，我们能够使用标准的K40 GPU在每个GPU上每批处理15个视频。 模型3：Two-Stream Networks来自ConvNets最后层的特征，LSTM可以模拟高层次的变化，但是可能无法捕获在许多情况下非常关键的精细的low-level动作。训练也是昂贵的，因为它需要通过多帧来展开网络以便反向传播。 Simonyan和Zisserman介绍了一种不同的非常实用的方法，在通过两个副本ImageNet预先训练的ConvNet后，通过对来自单个RGB帧的预测和10个外部计算的光流帧的预测进行平均，来对视频的短时间快照进行建模。光流 stream有一个自适应的输入卷积层，输入通道的数量是光流帧的两倍（因为流量有两个通道，水平和垂直），在测试时，多个快照从视频中采样，并对动作预测进行平均。这被证明在现有的基准测试中得到了非常高的性能，同时非常有效地进行训练和测试。 最近的一个扩展[8]将最后一个网络卷积层之后的空间流和光流融合起来，显示出对HMDB的一些改进，同时需要较少的测试时间增量（快照采样）。我们的实现大致使用了Inception-V1。网络的输入是5个连续的RGB帧，相隔10帧，以及相应的光流片段。 Inception-V1（5×7×7特征网格，对应于时间x和y维度）的最后一个平均汇聚层之前的空间和运动特征通过具有512个输出通道的3×3×3的3D卷积层，然后是3×3×3的3D最大池层，并通过最终的完全连接层。这些新层的权重用高斯噪声初始化。 两种模型（原始双流和3D融合版本）都是端对端训练（包括原始模型中的双流平均流程）。]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
        <tag>Action Recognition</tag>
        <tag>行为识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习论文笔记：DSSD]]></title>
    <url>%2Fblog%2F2938514597%2F</url>
    <content type="text"><![CDATA[Abstract 本文的主要贡献在于在当前最好的通用目标检测器中加入了额外的上下文信息。 为实现这一目的：我们通过将ResNet-101与SSD结合。然后，我们用deconvolution layers来丰富了SSD + Residual-101，以便在物体检测中引入额外的large-scale的上下文，并提高准确性，特别是对于小物体，从而称之为DSSD。 我们通过仔细的加入额外的learned transformations阶段，具体来说是一个用于在deconvolution中前向传递连接的模块，以及一个新的输出模型，使得这个新的方法变得可行，并为之后的研究提供一个潜在的道路。 我们的DSSD具有513×513的输入，在VOC2007测试中达到81.5％de的mAP，VOC2012测试为80.0％de的mAP，COCO为33.2％的mAP，在每个数据集上优于最先进的R-FCN 。 Introduction 最近的一些目标检测方法回归到了滑动窗口技术，这种技术随着更强大的整合了深度学习的机器学习框架而回归。 Faster RCNN -&gt; YOLO -&gt; SSD. 回顾最近的这些优秀的目标检测框架，要想提高检测准确率，一个很明显的目标就是：利用更好的特征网络并且添加更多的上下文，特别是对于小物体，另外还要提高边界框预测过程的空间分辨率。 在目标检测之外，最近有一个集成上下文的工作，利用所谓的“encoder-decoder”网络。该网络中间的bottleneck layer用于编码关于输入图像的信息，然后逐渐地更大的层将其解码到整个图像的map中。所形成的wide，narrow，wide的网络结构通常被称为沙漏。 但是有必要仔细构建用于集成反卷积的组合模块和输出模块，以在训练期间隔绝ResNet-101层，从而允许有效的学习。 Related Work SPPnet, Fast R-CNN, Faster R-CNN, R-FCN, YOLO：使用卷积网络的最上面的层来进行不同尺度的物体检测。 通过在ConvNet中开发多层来提高检测精度的方法有多重。 第一种方法：组合了ConvNet不同层的特征图，并使用组合特征图进行预测。 ION利用L2 normalization来结合多个VGGNet和池化层的特征来进行目标检测。 HyperNet也是使用类似于ION的方法。 但是这种结合多层特征的方法不仅增加内存，而且降低了模型的速度。 第二种方法：使用ConvNet中的不同层来预测不同尺度的对象。 因为不同层中的节点具有不同的接收域，所以自然会从具有大型接收场的层预测大对象，并使用具有小接收场的层来预测小物体。 SSD将不同尺度的默认框扩展到ConvNet中的多个层，并强制执行每一层专注于预测一定规模的对象。 S-CNN [2]在ConvNet的多层应用去卷积，以在使用层去学习region proposal和pool feature之前增加feature maps的分辨率。 然而，为了很好地检测小物体，这些方法需要从具有小的接收场和密集特征图的浅层中使用一些信息，这可能导致在检测小对象性能较低，因为浅层具有较少的关于对象的语义信息。 通过使用deconvolution layers和skip connections,，我们可以在密集（去卷积）特征图中注入更多的信息，从而有助于预测小物体。 另外还有一个工作方法，尽量去包括预测的上下文信息。 Multi-Region CNN Deconvolutional (DSSD) model Single Shot DetectionSSD SSD构建于base network之上，添加了一些逐渐见效的卷积层，如上图蓝色部分。 每个添加的层和一些较早的基本网络层用于预测某些预定义的边界框的分数和偏移。 这些预测由3x3x＃个通道维数的滤波器执行，一个滤波器用于产生每个类别分数，一个用于回归边界框的每个维度。 它使用非最大抑制（NMS）对预测进行后处理，以获得最终检测结果。 Using Residual-101 in place of VGG将Base Network从VGG16换为ResNet-101并未提升结果，但是添加额外的prediction module会显著地提成性能。 prediction module 在原始SSD中，目标函数直接应用于所选择的特征图，并且由于梯度的大幅度，使用L2标准化层用于conv4 3层。 MS-CNN指出，改进每个任务的子网可以提高准确性，按照这个原则，我们为每个预测层添加一个残差块，如图2变体（c）所示。 我们还尝试了原始SSD方法（a）和具有跳过连接（b）的残余块的版本以及两个顺序的残余块（d）。 我们注意到，ResNet-101和预测模块似乎显著优于对于较高分辨率输入图像没有预测模块的VGG。 Deconvolutional SSD 为了在检测中包含更多的高层次上下文，我们将prediction module转移到在原始SSD设置之后放置的一系列去卷积层中，有效地制作了非对称沙漏网络结构。 添加额外的去卷积层，以连续增加feature maps layers的分辨率。为了加强特征，我们采用了沙漏模型中“跳跃连接”的想法。 尽管沙漏模型在编码器和解码器阶段均包含对称层，但由于两个原因，我们使解码器阶段非常浅。 Deconvolution Module 为了帮助整合早期特征图和去卷积层的信息，我们引入了一个去卷积模块，如图3所示。 首先，在每个卷积层之后添加BN层。 第二，我们使用学习的去卷积层代替双线性上采样。 最后，我们测试不同的组合方法：element-wise sum and element-wise product。 Traning 我们遵循与SSD相同的训练政策。 在原始SSD模型中，长宽比为2和3的boxes从实验中证明是有用的。为了了解训练数据（PASCAL VOC 2007和2012年 trainval）中边界框的纵横比，我们以training box运行K-means聚类，以方格平方根为特征。我们从两个集群开始，如果错误可以提高20％以上，就会增加集群的数量。经过试验因此，我们决定在每个预测层添加一个宽高比1.6，并使用（1.6, 2.0, 3.0）。 Experiments PASCAL VOC2007 test detection results. PASCAL 2012 test detection results. COCO test-dev2015 detection results. Comparison of Speed &amp; Accuracy on PASCAL VOC2007 test.]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>目标检测</tag>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习论文笔记：Deep Residual Learning for Image Recognition]]></title>
    <url>%2Fblog%2F3085218970%2F</url>
    <content type="text"><![CDATA[Abstract 本文是何凯明大神的又一篇CVPR最佳论文。 网络越深越难训练，所以我们提出一个residual learning framework从而减轻网络的训练，该网络比以前使用的网络要深得多。 我们明确地将参考层的输入来作为学习残差函数，而不是学习无参考的函数（unreferenced functions）。 我们提供全面的经验证据，表明这些残留网络更容易优化，并可以从显着增加的深度中获得准确性。 这些残留网络的集合在ImageNet测试集上达到3.57％的误差。 该结果在ILSVRC 2015分类任务中荣获第一名。 深度对于许多CV领域的任务都十分重要的。由于我们网络很深，我们在COCO对象检测数据集上获得了28％的相对改进。我们还荣获了ImageNet检测，ImageNet定位，COCO检测和COCO分割任务的第一名。 Introduction 深层网络自然地将低/中/高层特征和分类器以端到端多层方式进行集成，并且特征的“级别”可以通过堆叠层数（深度）来丰富。网络的深度有着十分重要的作用。 随着网络深度的增加，带来一个问题：学习更好的网络是否和堆叠更多的层一样简单？回答这个问题的障碍是：逐渐消失的梯度问题。 当较深的网络能够开始收敛时，暴露了一个退化问题：随着网络深度的增加，精度饱和，然后迅速下降。这种下降不是由于过拟合，添加多层会导致更高的训练错误。 从浅到深的一个解决方案： 附加层：设置为“恒等”（identity） 原始层：由一个已经学会的较浅模型复制得来。 这种解决方案的存在表明，较深的模型不应该产生比较浅的模型更高的训练误差。至少具有相同的训练误差。 优化难题：随着网络层数不断加深，求解器不能找到解决途径。 为了解决这个问题，本文提出了深度残差学习框架。 平原网络： H(x)是任意一种理想的映射 平原网络希望第2层权重层能够与H(x)拟合。 残差网络： H(x)是任意一种理想的映射 残差网络希望第2类权重层能够与F(x)拟合使得H(x) = F(x) + x F(x)是一个残差映射w.r.t 恒 如果说恒等是理想，很容易将权重值设定为0； 如果理想化映射更接近于恒等映射，便更容易发现微小波动。 我们假设优化残差映射比优化原始的，无参考映射(unreferenced mapping)更容易。在极端情况下，如果一个identity mapping是最佳的，那么将残差推到零比通过一堆非线性层的identity mapping更容易。]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测论文笔记：R-FCN]]></title>
    <url>%2Fblog%2F3678248031%2F</url>
    <content type="text"><![CDATA[Abstract 提出了一个region-based, fully convolutional的网络来准确高效的进行物体检测。 不同于Fast/Faster R-CNN，其应用了计算成本很高的每个区域子网络数百次，本论文的region-based detector是完全卷积化的，几乎一张图像上所有的计算都是共享的。 为了实现这一目标，我们提出position-sensitive score maps，以解决在图像分类的平移不变性（translation-invariance）和物体检测中的平移可变性（translation-variance）之间的困境。 Introduction 最近流行的用于目标检测的深度学习框架依据RoI层的不同可以分为两大subnetworks： 一类是独立于RoIs的、共享的、fully convolutional的subnetwork。 另一类是RoI-wise的subnetwork，不共享计算。 在图像分类网络中，一个convolutional subnetwork会以一个sptial pooling layer跟随着几个fully-connected layer最为结尾，所以图像分类中的sptial pooling layer自然转化为目标检测中的RoI pooling layer。 ResNet和GoogleLeNets都被设计成fully convolutional的。 在ResNet论文中，Faster R-CNN中的RoI pooling layer被不自然的插入到两个卷积层集之间，带来了准确率的提升，但是速度由于unshared per-RoI计算降低。 对于图像分类任务来说：更倾向于平移不变性。对于图像检测任务来说：更倾向于平移变换性。 假设图像分类网络中更深层的卷积层对translation不敏感，所以为了解决translation invariance和translation variance之间的困难，ResNet将RoI pooling layer插入到了卷积神经网络之间。这个区域特定的操作打破了平移不变性，并且在不同区域之间进行评估时，RoI之后的卷积层不再是平移不变的。 为了将translation variance结合到FCN中，我们通过使用一组专用卷积层作为FCN输出来构造一组位置敏感得分图（position-sensitive score maps）。每一个得分图将相对于相对空间位置（例如，“在对象的左边”）的位置信息进行编码。在这个FCN之上，我们附加一个位置敏感的RoI池层（position-sensitive RoI pooling layer），从这些得分图中获取信息，没有跟随的权重的（卷积/ fc）层。 Our approach 本论文的方法参考R-CNN，也是使用two-stage的目标检测策略。 region proposal region classification 虽然不依赖于region proposal的目标检测方法确实存在，如SSD何YOLO，但是region-based system依旧在几个基准上保持领先的准确性。 Overall architecture of R-FCN: 用RPN来提出candidate RoIs，然后这些RoIs被应用到score maps，在RPN和R-FCN之间共享特征。 给定一个RoI，R-FCN架构对RoI进行分类（分为物体类别或者背景）。所有可学习权值的层都是卷积层，并且是在整张图片上计算得到的权重。最后卷积层为每个类别产生一个$k^2$个position-sensitive score maps，因此具有带有C个对象类别（背景为+1）的$k^2(C + 1)$通道的输出层。 每一个category有一个$k^2$的score map，对于这里来说k=3，所以最后RoI pooling层产生3x3x(C+1)维的feature map。 RPN以一个position-sensitive RoI pooling layer结束，该层聚合最后卷积层的输出并产生每个RoI的分数。我们的位置敏感的RoI pooling layer进行选择性合并，each of the k × k bin aggregates responses from only one score map out of the bank of k × k score maps。利用端到端训练，这个RoI层管理最后的卷积层以学习专门的position-sensitive score maps。 Backbone architecture 本论文R-FCN基于ResNet-101。 ResNet-101具有100个卷积层，后面是global average pooling和一个1000-class的fc层。我们移去了average pooling layer and the fc layer，仅使用convolutional layer来计算feature maps。 我们使用ResNet-101，在ImageNet上进行预训练，ResNet-101中的最后一个卷积块是2048-d，并且我们附加随机初始化的1024-d 1×1卷积层以减小尺寸。然后，我们应用$k^2(C + 1)$通道卷积层来生成分数图，如下所述。 Position-sensitive score maps &amp; Position-sensitive RoI pooling. 为了将位置信息显式编码到每个RoI中，我们将RoI矩形划分为k x k个bins。 最后的卷积层为每个类别产生的$k^2$个分数图。在第(i, j)个bin内，我们定义了一个位置敏感的RoI池化操作，从而只在第(i, j)个score map上进行池化： r_c(i, j|\theta)=\sum_{(x,y)\in bin(i,j)}{z_{i,j,c}(x+x_0,y+y_0|\theta)/n} 其中$r_c(i, j)$表示从c-th类别中得到的(i,j)-th bin的池化响应。 $z_{i,j,c}$是$k^2(C+1)$个score maps中的一个score map。 $(x_0,y_0)$表示一个RoI的左上角。 $n$表示这个bin中的像素的数量。 $\theta$表示这个网络中所有的可学习权重。 该pooling属于AVE，也可以用MAX。 对每个类别k*k的score map进行平均，最后每个RoI得到一个C+1维的向量。然后求loss，它们用于评估训练期间的交叉熵损失和推理期间的RoIs排名。 bounding boxes regression。对每个RoI产生一个$4k^2$维的向量。然后通过average voting将其聚合成4维向量。 在RoI层之后没有可学习的层次，实现了几乎无成本地区的计算、加速训练和推理。 Training 利用提前计算好的region proposal，很容易来端到端的R-FCN架构训练。 loss function: L(s,t_{x,y,w,h})=L_{cls}(s_{c^*})+\lambda[c^*>0]L_{reg}(t,t^*) 本框架很容易在训练的时候使用online hard example mining (OHEM)。 我们的per-RoI计算可以进行几乎cost-free的example mining。 在前向传播中：假设每张图片N个proposals。我们计算所有N个proposal的loss，排序，选择最高的B个RoIs。然后在选中的proposal上进行反向传播。 decay：0.0005 momentum：0.9 single-scale training。 B=128 lr = 0.001 ~20k, 0.0001 ~ 10k 同Faster R-CNN一趟，使用4步alternating training，在训练RPN和训练R-FCN之间。 Inference 为公平期间，在300个RoIs上进行评估，结果之后用NMS进行处理，IoU阈值0.3 À trous and stride 将ResNet-10的有效stride从32减为16像素，提高了score map的分辨率。 conv4阶段之前（stride=16）的所有层都没有改变。 第一个conv5块儿的stride从2改为1，并且conv5阶段的卷积核都被改为hole algorithm，（Algorithme à trous）以补偿减少的步幅。 为了公平比较，RPN在conv4之上进行计算。从而RPN不被à trous影响。 Related WorkR-CNN已经说明了带深度网络的区域候选的有效性。R-CNN计算那些关于裁剪不正常的覆盖区域的卷积网络，并且计算在区域直接是不共享的。SPPnet，Fast R-CNN和Faster R-CNN是半卷积的（semi-convolutional），在卷积子网络中是计算共享的，在另一个子网络是各自计算独立的区域。 物体检测器可以被认为是全卷积模型。OverFeat 检测物体通过在convolutional feature maps上进行多尺度的窗口滑动。 在某些情况下，可以将单精度的滑动窗口改造成一个单层的卷积层。在Faster R-CNN中的RPN组件是一个全卷积检测器，用来预测是一个关于多尺寸的参考边框的实际边框。原始的RPN是class-agnostic（class无关的）。但是对应的class-specific是可应用的。 另一个用于物体检测的是fc layer（fully-connected）用来基于整幅图片的完整物体检测。]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>目标检测</tag>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习论文笔记：SSD]]></title>
    <url>%2Fblog%2F3118967289%2F</url>
    <content type="text"><![CDATA[知识点 Jaccard overlap, Jaccard similarity:Jaccard coefficient: J(A,B)=\frac{|A\cap B|}{|A\cup B|}A,B分别代表符合某种条件的集合：两个集合交集的大小/两个集合并集的大小，交集=并集意味着2个集合完全重合。所以Jaccard overlap其实就是IoU。 Abstract SSD: 利用单个深度神经网络的目标检测方法。将边界框的输出空间离散化为一组默认框，在每个feature map位置上有着不同的宽高比和尺度。 在预测的时候，网络针对每个默认框中的每个存在的对象类别产生分数，并且对框的进行调整以更好地匹配对象形状。 在多尺度图像处理方面，网络组合来自具有不同分辨率的多个feature map的预测，以自然地处理各种尺寸的对象。 相比于基于object proposal的方法，SSD是简单地，因为它能够完全消除proposal generation和后续的像素或者特征重冲采样阶段，所有的计算都封装在单独的网络中。 Introduction 目前的目标检测系统是以下方法的变体：假设边界框（bounding box），对每个框进行像素或特征重取样，采用高质量分类器。 评估速度方法：SPF (seconds per frame). 提出第一个基于深度网络的不需要为BB进行resample pixels or features的目标检测器，并能够同样达到高准确率。 本论文的贡献（具体看论文）： 引入了SSD。 SSD的核心。 为了实现高检测准确率，引入了在不同尺度和横纵比的feature maps上进行预测。 End-to-end training 以及高准确率，机试在低分辨率图片。 在PASCAL VOC、COCO和ILSVRC上进行试验，具有很强的竞争力。 The Single Shot Detector (SSD)Model 基于前向卷积神经网络，产生固定尺寸的BB集，以及这些BB中存在物体的分数，之后跟随者一个非极大值抑制步骤来产生最终检测。 网络前面的几层是基于标准的用于产生高质量图像分类的架构，我们成为基础网络。我们给网络然后添加了辅助的结构来产生检测结构。辅助网络具备以下关键特征： 用于检测的多尺寸特征图。在基础网络后面添加额外几个卷积层，在尺寸上逐层递减，从而能够在不同尺寸上检测。（Overfeat和YOLO都只是在单独尺寸的feature map上进行操作。） 用来预测的卷积预测器（Convolutional predictors）。 默认的boxes和aspect ratios。我们将一组默认边界框与每个feature map单元关联，用于网络顶部的多个特征映射。在每个feature map单元格中，我们预测相对于单元格中的默认框形状的偏移，以及指示每个框中存在类实例的每类分数。（本论文中的default boxes类似于Faster R-CNN中的anchor boxes，然而我们将他用于不同分辨率的几个feature maps中） Training 训练SSD和训练使用region proposals的典型检测器之间的关键区别是：ground truth信息需要分配给固定的检测器输出集合中的特定输出。 训练涉及到： choosing the set of default boxes and scales for detection。 hard negative mining。 data augmentation strategies（数据增加策略）。 Matching strategy在训练过程中，对于每一个ground truth，我们都从默认框中选择每个不同的位置、aspect ratio、scale的bounding boxes。首先匹配最好的jaccard overlap的default box（类似于MultiBox），但与MultiBox不同的是，我们然后匹配default box与任何ground truth，只要jaccard overlap高于阈值（0.5）。这样简化了学习问题。 Training objective整体的代价函数是localization loss和confidence loss之和： L(x,c,l,g)=\frac{1}{N}(L_{conf}(x,c)+\alpha L_{loc}(x,l,g)) N是匹配的default boxes的数量，N=0时，loss=0。 localization loss是predicted box和ground truth box之间的Smooth L1 loss（类似于Faster R-CNN）。我们预测default box的中心$(cx,cy)$，以及宽度$(w)$和长度$(h)$。 confidence loss是多个类confidence$(c)$之间的softmax loss。 权值$\alpha$通过交叉验证设为1。 Choosing scales and aspect ratios for default boxes 不同于将照片处理为不同尺寸再结合结果的方法，本论文通过利用单个神经网络中不同层的feature maps，可以达到同样的效果，同时可以在所有尺寸中共享权值。 利用较低层的feature maps可以提高semantic segmentation质量，应为较低层往往可以捕捉到更精细的细节。 我们同时使用较低和较高层的feature maps来进行检测。 网络中不同层的feature maps有着不同的接受域的尺寸。 假设我们想要使用m个feature maps用来检测，则每个feature map的default boxes的scale可以这样计算： 通过结合在许多feature maps上所有位置上的所有的有着不同scale和aspect ratio的default boxes，我们可以产生对不同物体大小和形状的各种预测。 如下图中，狗在8x8的feature map中没有匹配的default box，因此在训练中会被作为负样本，但是在4x4的feature map中有着匹配的feature map。 Hard negative mining 通过匹配阶段后，default boxes中会产生大量的negatives，尤其是当可能的default boxes数量非常大时。这导致positive和negative时间严重的不平衡。 我们将negative examples的default boxes通过其最高的confidence loss进行排序，然后选择较高的几个，使negative examples和positive examples之间的比例保持在3:1之间。这样会更快的优化和更稳定的训练。 Experimental Results 所有的实验都是基于VGG16。 将fc6和fc7转化为卷积层，从fc6和fc7中取样子参数。 将pool5从2x2-s2转化为3x3-s1。 使用a trous algorithm来填补“holes”。 移去了所有的dropout层和fc8层。 用SGD进行微调。 学习率$10^{-3}$，动量0.9，weight decay是0.0005，batch size是32. PASCAL VOC2007 “xavier” method来初始化新加入的层的参数。 通过detection analysis tool分析后，显示SSD有着更少的localization错误，因为其能够直接去学习regress物体的形状，并分类，而非使用两个互相解耦的步骤。 然而，SSD对于相似的物体会有更多的混淆，特别是animals，一部分原因。 SSD对bounding boxes尺寸是十分敏感的。提升输入尺寸可能会提升小物体检测，但依旧有许多空间提升。 Model analysis Data augmentation很重要。 更多的default boxes的形状可能会更好。 Atrous is faster：如果不使用更改后的VGG-16，虽然结果一样，但是速度回降低20%。 不同分辨率中多个输出层会更好。SSD的主要贡献是在不同输出层上使用不同尺度的默认框。 PASCAL VOC2012 和PASCAL VOC2007一样的实验设置。 在2012 trainval+2007 trainval+2007 test上进行训练，在2012 test上进行测试。 COCO COCO中的物体比PASCAL VOC中的物体小，所以我们在所有层上使用更小的default boxes Data Augmentation for Small Object Accuracy 对SSD来说，对小物体分类的任务相对Faster R-CNN来说会更难。 Data augmentation对于特高性能是十分显著的，尤其是小数据及。 改进SSD的另一种方法是设计更好的平铺默认框（tiling of default boxes），使其位置和尺度更好地与特征图上每个位置的接收场对准。 Inference time 考虑到从我们的方法生成的大量框，有必要在推理期间有效地执行非最大抑制（nms）。 通过使用0.01的限制阈值，我们可以过滤大多数bounding boxes。 然后我们应用nms，每个类别的jaccard重叠0.45，并保持每个图像前200个检测。 80%的前向传递时间被花费在了base network，所以使用一个更快的base network可以提高速度。 Related Work 有两种已建立的用于图像中的对象检测的方法类别，一种基于滑动窗口，另一种基于region proposal classification。 Conclusion 我们模型的一个关键特性是使用多尺度卷积边界框输出附加到网络顶部的多个特征图。]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>目标检测</tag>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习论文笔记：YOLO9000]]></title>
    <url>%2Fblog%2F2102833929%2F</url>
    <content type="text"><![CDATA[Abstract YOLO9000: a state-of-the-art, real-time 的目标检测系统，可以检测超过9000种的物体分类。 本论文提出两个模型，YOLOv2和YOLO9000。 YOLOv2： 是对YOLO改进后的提升模型。 利用新颖的，多尺度训练的方法，YOLOv2模型可以在多种尺度上运行，在速度与准确性上更容易去trade off。 YOLO9000： 是提出的一种联合在检测和分类数据集上训练的模型，这种联合训练的方法使得YOLO9000能够为没有标签的检测数据目标类预测。 可以检测超过9000个类。 Introduction 目前，许多检测方法依旧约束在很小的物体集上。 目前，目标检测数据集相比于用于分类和标注的数据集来说，是有限制的。 最常见的检测数据集包含数十到数十万的图像，具有几十到几百个标签，比如Pascal、CoCo、ImageNet。 分类数据集具有数以百万计的图像，具有数万或数十万种类别，如ImageNet。 目标检测数据集永远不会达到和分类数据集一样的等级。 本论文提出一种方法，利用分类数据集来作为检测数据集，将两种截然不同的数据集结合。 本论文提出一个在目标检测和分类数据集上联合训练的方法。此方法利用标记的检测图像学习精确定位对象，而它使用分类图像增加其词汇和鲁棒性。 Better YOLO产生很多的定位错误。而且YOLO相比于region proposal-based方法有着相对较低的recall（查全率）。所以主要任务是在保持分类准确率的前提下，提高recall和减少定位错误。 我们从过去的工作中融合了我们自己的各种新想法，以提高YOLO的性能。 结果的摘要可以在表中找到： Batch Normalization 得到2%的mAP的提升，使用Batch Normalization，我们可以从模型中删除dropout，而不会出现过度缺陷。 High Resolution Classiﬁer 对于YOLOv2，我们首先在ImageNet对全部448×448分辨率图像上进行10epochs的微调来调整分类网络。 然后我们在检测时调整resulting network。 这种高分辨率分类网络使我们增加了近4％的mAP。 Convolutional With Anchor Boxes YOLO利用卷积特征提取器最顶端的全连接层来直接预测BB的坐标。而Faster R-CNN是利用首选的priors来预测BB。 预测BB的偏移而不是坐标可以简化问题，并使网络更容易学习。本论文从YOLO中移去了全连接层，并且利用anchor box来预测BB。 我们移去了pooling层，使得网络的卷积层的输出有更高的像素。 同时将网络缩减到在416*416像素的图片上操作。 我们这样做是因为我们想要特征图中具有奇数个位置，因此存在单个中心单元。 当我们移动到anchor boxes时，我们也将class prediction机制与空间位置解耦，而是为每个anchor box预测的类和对象。 同YOLO一样，objectness prediction仍然预测ground truth和所提出的框的IOU，并且class predictions预测该类的条件概率，假定存在对象。(没太懂) Dimension Clusters 将YOLO与anchor boxes结合有两个问题，第一个是anchor box的长宽是认为选定的。 我们不是手动选择先验（priors），而是在训练集边界框上运行k-means聚类，以自动找到好的先验。 我们真正想要的是导致良好的IOU分数的priors，这是独立于盒子的大小。 因此，对于我们的distance metric，我们使用： d(box, centroid) = 1-IOU(box,centroid) 我们选择k = 5作为模型复杂性和高召回率之间的良好权衡。这样非常不同于相比于人工选择的boxes。更多的又高又瘦的boxes。 Direct location prediction 将YOLO与anchor boxes结合有两个问题，第二个模型不稳定，特别是在早期迭代中。 并非预测偏移，我们遵循YOLO的方法并预测相对于网格单元的位置的位置坐标。 这将ground truth限制在0和1之间。我们使用逻辑激活来约束网络的预测落在该范围内。 网络为每一个BB预测5个坐标：$t_x, t_y, t_w, t_h, t_o$. 结合Dimension Clusters和Direct location prediction，YOLO提升5%的mAP。 Fine-Grained Features 修改后的YOLO在1313的feature map上进行检测。 虽然这对于大对象是足够的，但是它可以从用于定位较小对象的*细粒度特征中受益。 添加一个传递层，将分辨率从前面的层变为从26 x 26分辨率。 Multi-Scale Training 我们希望YOLOv2可以足够鲁邦在不同尺寸的images上进行训练。 并非使用固定的输入图像尺寸，我们在每几次迭代后改变网络。每10batches，我们的网络随机选择一个新的图像尺寸。 Faster 大多数检测框架依赖VGG-16作为基本特征提取器。VGG-16是一个强大、准确的分类网络，但是也很复杂。 YOLO框架使用的基于Googlenet架构的修改后的网络。比VGG-16快速，但是准确性比VGG-16稍差。 Darknet-19 供YOLOv2使用的新的分类模型。 最终模型叫做darknet-19，有着19个卷积层和5个maxpooling层。 Darknet-19处理每张图片只需要5.58 billion的操作。 Training for classification 在标准的ImageNet 1000类的数据集上利用随机梯度下降训练160 epochs。开始学习率为0.1，polynomial rate decay 是4，weight decay是0.0005，动量是0.9。 在训练期间，我们使用标准的数据增加技巧，包括随机裁剪，旋转，以及色调，饱和度和曝光偏移。 在224x224分辨率的图像上进行预训练，然后在448x448分辨率的图像上进行微调。 Training for detection 我们通过去除最后的卷积层来修改这个网络，并且替代地增加具有1024个滤波器的三个3×3卷积层，每个跟随着具有我们需要检测所需的输出数量的最后的1×1卷积层。 passthrough层的添加：使网络能够使用fine grain feature。 Stronger 本论文提出一种机制，用来将分类和检测数据结合起来再一起训练。 在训练过程中，当看到用于检测的被标注的图片，我们会使用基于YOLOv2的代价函数进行反向传播。 在训练过程中，当看到分类图片，我们只从框架中用来分类部分来传递损失。 这种方法的challenge： 检测数据集中的标签是大分类，而分类数据集的标签是小分类，所以我们需要找一个方法来融合这些标签。 用来分类的许多方法都是使用softmax层来计算最后的概率分布，使用softmax层会假设类之间是互斥的，但是如何用本方法融合数据集，类之间本身不是互斥的。 我们所以使用multi-label模型来结合数据集，不假设类之间互斥。这种方法忽略了我们已知的数据的结构。 Hierarchical classification ImageNet标签是从WordNet中得来，一种结构化概念和标签之间如何联系的语言数据库。 WordNet是连接图结构，而非树。我们相反并不实用整个图结构，我们将问题简化成从ImageNet的概念中构建有结构的树。 WordTree Dataset combination with WordTree 我们可以使用WordTree来介个数据集。 将数据集中分类映射成树中的下义词。 举例：将ImageNet和COCO数据集结合： WordNet十分多样化，所以我们可以利用这种技术到大多数数据集。 Joint classiﬁcation and detection 将COCO数据集和ImageNet数据集结合，训练处一个特别大规模的检测器。 对应的WordTree有9418个类。 ImageNet是一个更大的数据集，因此我们通过对COCO进行过采样来平衡数据集，使ImageNet只以4：1的倍数来增大。 当我们的网络看见一张用来检测的图片，我们正常反向传播loss。对于分类loss，我们只在该label对应层次之上反向传播loss。比如：如果标签是“dog”，我们会在树中的“German Shepherd”和“Golden Retriever”中进一步预测错误，因为我们没有这些信息。 当我们的网络看见一张用来分来的照片，我们只反向传递分类loss。 使用这种联合训练，YOLO 9000使用COCO中的检测数据学习找到图像中的对象，并使用ImageNet中的数据学习分类各种各样的对象。 在ImageNet上利用YOLO9000来做detection，从而进行评估。ImageNet和COCO只有44个相同的类分类，意味着YOLO9000在利用部分监督来进行检测。 Conclusion 本论文提出两个模型，YOLOv2和YOLO9000。 YOLOv2：是对YOLO改进后的提升模型。更快更先进。此外，它可以以各种图像大小运行，以提供速度和精度之间的权衡。 YOLO9000：是提出的一种联合在检测和分类数据集上训练的模型，可以为没有任何标注检测标签的数据进行检测。可以检测超过9000个类。使用WordTree技术来组合不同来源的数据。 我们创造出许多目标检测之外的技术： WordTree representation. Dataset combination. Multi-scale training. 下一步工作：我们希望利用相似的技术来进行weakly supervised image segmentation.]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>目标检测</tag>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习论文笔记：YOLO]]></title>
    <url>%2Fblog%2F2094641206%2F</url>
    <content type="text"><![CDATA[Abstract 之前的物体检测的方法是使用分类器来进行检测。 相反，本论文将对象检测作为空间分离的边界框和相关类概率的回归问题。 本论文的YOLO模型能达到45fps的实时图像处理效果。 Fast YOLO：小型的网络版本，可达到155fps。 与目前的检测系统相比，YOLO会产生更多的定位错误，但是会更少的去在背景中产生false positive。 Introduction DPM: use a sliding window approach where the classiﬁer is run at evenly spaced locations over the entire image. R-CNN: use region proposal methods to ﬁrst generate potential bounding boxes in an image and then run a classiﬁer on these proposed boxes. 具有slow和hard to optimize的缺点。 本论文将目标检测问题重新组织成single regression problem. 从图像像素转为bounding box coordinates和class probabilities. YOLO框架： A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. YOLO模型的优势： First, YOLO is extremely fast. regression problem. no batch processing on a Titan X. Second, YOLO reasons globally about the image when making predictions. YOLO makes less than half the number of background errors compared to Fast R-CNN. Third, YOLO learns generalizable representations of objects. YOLO在准确性方面依旧落后与其他先进的检测系统，但是可以快速的标注图片中的物体，特别是小物体。 Unified Detection 本论文将物体检测中单独的组件统一到一个单一的神经网络中。网络利用整个图像的各个特征来预测每一个BB。而且同时为一张图片中所有的类预测所用的BB。 YOLO可以end-to-end来训练，而且能在保持高平均准确率的同时达到实时要求。 系统将输入图片分为$S*S$的网格单元。如果物体的中心落入某个格子，那么这个格子将会用来检测这个物体。 每个网格单元会预测B个bounding box以及这些框的置信值。 每个bounding box会有5个预测值：$x,y,w,h$和置信值confidence，$confidence = Pr(Object)*IOU^{truth}_{pred}$. 每个网格单元也预测C个条件类概率，$Pr(Class_i|Object)$，在一个网格单元包含一个物体的前提下，它属于某个类的概率。我们只为每个网格单元预测一组类概率，而不考虑框B的数量。 在测试的时候，通过如下公式来给出对某一个box来说某一类的confidence score： Pr(Class_{i}|Object)*Pr(Object)*IOU^{truth}_{pred}=Pr(Class_{i})*IOU^{truth}_{pred} Model示例： 每个grid cell预测B个bounding boxes，每个框的confidence和C个类概率。 Network Design YOLO网络结构图： 起初的卷积层用来从图像中提取特征。 全连接层用来预测输出的概率和坐标。 24个卷积层，之后跟着2个全连接层 最终输出是7 x 7 x 30的张量。 Fast YOLO和YOLO之间所有的训练和测试参数一样。 在ImageNet上进行卷积层的预训练。 Training 在ImageNet上预训练卷积层。预训练前20层卷积层，之后跟随者一个average-pooling layer和一个fully connected layer. 将预训练的模型用来检测，论文Ren et al.显示给与训练好的模型添加卷积和连接层能够提高性能。所以添加了额外的4个卷积层和2个全连接层，其权值随机初始化。 将像素从224x224提升到448x448。 最后一层同时预测class probabilities和bounding box coordinates. 其中涉及到BB的长宽规范化。 由于sum-squared error的缺点，增加边界框坐标预测的损失，并减少对不包含对象的框的置信度预测的损失。 large boxes中的偏差matter less than 与small boxes中的偏差。 YOLO为每一个网格单元预测多个BB，但是在测试期间，我们只想每一个物体有一个BB预测框来做响应，我们选择具有最高IOU的BB来作为响应框。 总的loss function: 135 epochs batch size：64 动量：0.9 decay：0.0005 为防止过拟合，我们使用dropout和extensive data augmentation技术。 Inference 在测试图像中预测检测只需要一个网络评估，与一般的classifier-based methods不同。 Non-maximal suppression可以用来修复multiple detections。 Comparison to Other Detection Systems 检测流水线往往开始于提取健壮特征集（Haar, SIFT, HOG, convolutional features）,然后分类器或者定位器用来识别特征空间的物体，这些分类器或者定位器往往在整个图像上或者在图像的子区域中滑动窗口。 与DPM的比较。 与R-CNN的比较。每个图片值预测98个bounding boxes。 与其他快速检测器的比较。相比于单类检测器，YOLO可以同时检测多种物体。 与Deep MultiBox的比较。YOLO是一个完整的检测系统。 与OverFeat的比较。OverFeat是一个disjoint的系统，OverFeat优化定位，而非检测性能。需要大量的后处理。 与MultiGrasp的比较。执行比目标检测更简单的任务。]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>目标检测</tag>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习实践经验：用Faster R-CNN训练Caltech数据集——训练检测]]></title>
    <url>%2Fblog%2F464905881%2F</url>
    <content type="text"><![CDATA[前言前面已经介绍了如何准备数据集，以及如何修改数据集读写接口来操作数据集，接下来我来说明一下怎么来训练网络和之后的检测过程。 修改模型文件faster rcnn有两种各种训练方式: Alternative training(alt-opt) Approximate joint training(end-to-end) 两种方法有什么不同，可以参考我这篇博客，推荐使用第二种，因为第二种使用的显存更小，而且训练会更快，同时准确率差不多，两种方式需要修改的代码是不一样的，同时faster rcnn提供了三种训练模型，小型的ZF model，中型的VGG_CNN_M_1024和大型的VGG16,论文中说VGG16效果比其他两个好，但是同时占用更大的GPU显存(~11GB) 我使用的是VGG model + alternative training，需要检测的类别只有一类，加上背景所以总共是两类(background + person)。 下面修改模型文件： py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/stage1_fast_rcnn_train.pt 123456789101112131415layer &#123; name: &apos;data&apos; type: &apos;Python&apos; top: &apos;data&apos; top: &apos;rois&apos; top: &apos;labels&apos; top: &apos;bbox_targets&apos; top: &apos;bbox_inside_weights&apos; top: &apos;bbox_outside_weights&apos; python_param &#123; module: &apos;roi_data_layer.layer&apos; layer: &apos;RoIDataLayer&apos; param_str: &quot;&apos;num_classes&apos;: 2&quot; #按训练集类别改，该值为类别数+1 &#125; &#125; 1234567891011121314151617181920212223layer &#123; name: &quot;cls_score&quot; type: &quot;InnerProduct&quot; bottom: &quot;fc7&quot; top: &quot;cls_score&quot; param &#123; lr_mult: 1.0 &#125; param &#123; lr_mult: 2.0 &#125; inner_product_param &#123; num_output: 2 #按训练集类别改，该值为类别数+1 weight_filler &#123; type: &quot;gaussian&quot; std: 0.01 &#125; bias_filler &#123; type: &quot;constant&quot; value: 0 &#125; &#125; &#125; 1234567891011121314151617181920212223layer &#123; name: &quot;bbox_pred&quot; type: &quot;InnerProduct&quot; bottom: &quot;fc7&quot; top: &quot;bbox_pred&quot; param &#123; lr_mult: 1.0 &#125; param &#123; lr_mult: 2.0 &#125; inner_product_param &#123; num_output: 8 #按训练集类别改，该值为（类别数+1）*4，四个顶点坐标 weight_filler &#123; type: &quot;gaussian&quot; std: 0.001 &#125; bias_filler &#123; type: &quot;constant&quot; value: 0 &#125; &#125; &#125; py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/stage1_rpn_train.pt 123456789101112layer &#123; name: &apos;input-data&apos; type: &apos;Python&apos; top: &apos;data&apos; top: &apos;im_info&apos; top: &apos;gt_boxes&apos; python_param &#123; module: &apos;roi_data_layer.layer&apos; layer: &apos;RoIDataLayer&apos; param_str: &quot;&apos;num_classes&apos;: 2&quot; #按训练集类别改，该值为类别数+1 &#125; &#125; py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/stage2_fast_rcnn_train.pt 123456789101112131415layer &#123; name: &apos;data&apos; type: &apos;Python&apos; top: &apos;data&apos; top: &apos;rois&apos; top: &apos;labels&apos; top: &apos;bbox_targets&apos; top: &apos;bbox_inside_weights&apos; top: &apos;bbox_outside_weights&apos; python_param &#123; module: &apos;roi_data_layer.layer&apos; layer: &apos;RoIDataLayer&apos; param_str: &quot;&apos;num_classes&apos;: 2&quot; #按训练集类别改，该值为类别数+1 &#125; &#125; 1234567891011121314151617181920212223layer &#123; name: &quot;cls_score&quot; type: &quot;InnerProduct&quot; bottom: &quot;fc7&quot; top: &quot;cls_score&quot; param &#123; lr_mult: 1.0 &#125; param &#123; lr_mult: 2.0 &#125; inner_product_param &#123; num_output: 2 #按训练集类别改，该值为类别数+1 weight_filler &#123; type: &quot;gaussian&quot; std: 0.01 &#125; bias_filler &#123; type: &quot;constant&quot; value: 0 &#125; &#125; &#125; 1234567891011121314151617181920212223layer &#123; name: &quot;bbox_pred&quot; type: &quot;InnerProduct&quot; bottom: &quot;fc7&quot; top: &quot;bbox_pred&quot; param &#123; lr_mult: 1.0 &#125; param &#123; lr_mult: 2.0 &#125; inner_product_param &#123; num_output: 8 #按训练集类别改，该值为（类别数+1）*4,四个顶点坐标 weight_filler &#123; type: &quot;gaussian&quot; std: 0.001 &#125; bias_filler &#123; type: &quot;constant&quot; value: 0 &#125; &#125; &#125; py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/stage2_rpn_train.pt 123456789101112layer &#123; name: &apos;input-data&apos; type: &apos;Python&apos; top: &apos;data&apos; top: &apos;im_info&apos; top: &apos;gt_boxes&apos; python_param &#123; module: &apos;roi_data_layer.layer&apos; layer: &apos;RoIDataLayer&apos; param_str: &quot;&apos;num_classes&apos;: 2&quot; #按训练集类别改，该值为类别数+1 &#125; &#125; py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt 12345678910111213141516171819layer &#123; name: &quot;cls_score&quot; type: &quot;InnerProduct&quot; bottom: &quot;fc7&quot; top: &quot;cls_score&quot; inner_product_param &#123; num_output: 2 #按训练集类别改，该值为类别数+1 &#125; &#125; layer &#123; name: &quot;bbox_pred&quot; type: &quot;InnerProduct&quot; bottom: &quot;fc7&quot; top: &quot;bbox_pred&quot; inner_product_param &#123; num_output: 84 #按训练集类别改，该值为（类别数+1）*4,四个顶点坐标 &#125; &#125; 训练测试训练前还需要注意几个地方： cache问题： 假如你之前训练了官方的VOC2007的数据集或其他的数据集，是会产生cache的问题的，建议在重新训练新的数据之前将其删除。 py-faster-rcnn/output py-faster-rcnn/data/cache 训练参数 参数放在如下文件: py-faster-rcnn/models/pascal_voc/VGG16/faster_rcnn_alt_opt/stage_fast_rcnn_solver*.pt 12345base_lr: 0.001lr_policy: &apos;step&apos;step_size: 30000display: 20.... 迭代次数在文件py-faster-rcnn/tools/train_faster_rcnn_alt_opt.py中进行修改: 1max_iters = [80000, 40000, 80000, 40000] 分别对应rpn第1阶段，fast rcnn第1阶段，rpn第2阶段，fast rcnn第2阶段的迭代次数，自己修改即可，不过注意这里的值不要小于上面的solver里面的step_size的大小，大家自己修改吧 开始训练首先修改experiments/scripts/faster_rcnn_alt_opt.sh成如下，修改地方已标注： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#!/bin/bash# Usage:# ./experiments/scripts/faster_rcnn_alt_opt.sh GPU NET DATASET [options args to &#123;train,test&#125;_net.py]# DATASET is only pascal_voc for now## Example:# ./experiments/scripts/faster_rcnn_alt_opt.sh 0 VGG_CNN_M_1024 pascal_voc \# --set EXP_DIR foobar RNG_SEED 42 TRAIN.SCALES "[400, 500, 600, 700]"set -xset -eexport PYTHONUNBUFFERED="True"GPU_ID=$1NET=$2NET_lc=$&#123;NET,,&#125;DATASET=$3array=( $@ )len=$&#123;#array[@]&#125;EXTRA_ARGS=$&#123;array[@]:3:$len&#125;EXTRA_ARGS_SLUG=$&#123;EXTRA_ARGS// /_&#125;case $DATASET in caltech) # 这里将pascal_voc改为caltech TRAIN_IMDB="caltech_train" # 改为与factor.py中命名的name格式相同，为caltech_train TEST_IMDB="caltech_test" # 改为与factor.py中命名的name格式相同，为caltech_test PT_DIR="caltech" # 这里将pascal_voc改为caltech ITERS=40000 ;; coco) echo "Not implemented: use experiments/scripts/faster_rcnn_end2end.sh for coco" exit ;; *) echo "No dataset given" exit ;;esacLOG="experiments/logs/faster_rcnn_alt_opt_$&#123;NET&#125;_$&#123;EXTRA_ARGS_SLUG&#125;.txt.`date +'%Y-%m-%d_%H-%M-%S'`"exec &amp;&gt; &gt;(tee -a "$LOG")echo Logging output to "$LOG"time ./tools/train_faster_rcnn_alt_opt.py --gpu $&#123;GPU_ID&#125; \ --net_name $&#123;NET&#125; \ --weights data/imagenet_models/$&#123;NET&#125;.v2.caffemodel \ --imdb $&#123;TRAIN_IMDB&#125; \ --cfg experiments/cfgs/faster_rcnn_alt_opt.yml \ $&#123;EXTRA_ARGS&#125;set +xNET_FINAL=`grep "Final model:" $&#123;LOG&#125; | awk '&#123;print $3&#125;'`set -xtime ./tools/test_net.py --gpu $&#123;GPU_ID&#125; \ --def models/$&#123;PT_DIR&#125;/$&#123;NET&#125;/faster_rcnn_alt_opt/faster_rcnn_test.pt \ --net $&#123;NET_FINAL&#125; \ #--net output/faster_rcnn_alt_opt/train/ZF_faster_rcnn_final.caffemodel \ --imdb $&#123;TEST_IMDB&#125; \ --cfg experiments/cfgs/faster_rcnn_alt_opt.yml \ $&#123;EXTRA_ARGS&#125; 调用如下命令进行训练及测试，从上面代码可以看出，该shell文件在训练完后会接着进行测试，但是我的测试集没有标注，所以测试的时候会报错，但是由于Caltech数据集的测试结果有专门的评估代码，所以我不用faster r-cnn提供的代码进行测试，而是直接进行检测生成坐标，用专门的评估代码进行检测。 12cd py-faster-rcnn./experiments/scripts/faster_rcnn_alt_opt.sh 0 VGG16 caltech 参数1：指定gpu_id。 参数2：指定网络模型参数。 参数3：数据集名称，目前只能为pascal_voc。 在训练过程中，会调用py_faster_rcnn/tools/train_faster_rcnn_alt_opt.py文件开始训练网络。 可能会出现的BugsAssertionError: assert (boxes[:, 2] &gt;= boxes[:, 0]).all()问题重现在训练过程中可能会出现如下报错： 1234File &quot;/py-faster-rcnn/tools/../lib/datasets/imdb.py&quot;, line 108, in append_flipped_images assert (boxes[:, 2] &gt;= boxes[:, 0]).all() AssertionError 问题分析检查自己数据发现，左上角坐标 (x, y) 可能为0，或标定区域溢出图片（即坐标为负数），而faster rcnn会对Xmin,Ymin,Xmax,Ymax进行减一操作，如果Xmin为0，减一后变为65535，从而在左右翻转图片时导致如上错误发生。 问题解决 修改lib/datasets/imdb.py中的append_flipped_images()函数： 数据整理，在一行代码为 boxes[:, 2] = widths[i] - oldx1 - 1下加入代码： 123for b in range(len(boxes)): if boxes[b][2]&lt; boxes[b][0]: boxes[b][0] = 0 修改lib/datasets/caltech.py，_load_pascal_annotation()函数，将对Xmin,Ymin,Xmax,Ymax减一去掉，变为： 1234567891011121314# Load object bounding boxes into a data frame. for ix, obj in enumerate(objs): bbox = obj.find('bndbox') # Make pixel indexes 0-based # 这里我把‘-1’全部删除掉了，防止有的数据是0开始，然后‘-1’导致变为负数，产生AssertError错误 x1 = float(bbox.find('xmin').text) y1 = float(bbox.find('ymin').text) x2 = float(bbox.find('xmax').text) y2 = float(bbox.find('ymax').text) cls = self._class_to_ind[obj.find('name').text.lower().strip()] boxes[ix, :] = [x1, y1, x2, y2] gt_classes[ix] = cls overlaps[ix, cls] = 1.0 seg_areas[ix] = (x2 - x1 + 1) * (y2 - y1 + 1) （可选）如果1和2可以解决问题，就没必要用方法3。修改lib/fast_rcnn/config.py，不使图片实现翻转，如下改为： 12# Use horizontally-flipped images during training? __C.TRAIN.USE_FLIPPED = False 如果如上三种方法都无法解决该问题，那么肯定是你的数据集坐标出现小于等于0的数，你应该一一排查。 训练fast rcnn时出现loss=nan的情况。问题重现 问题分析这是由于模型不收敛，导致loss迅速增长。 而我出现以上现象的原因主要是因为我在出现AssertionError的时候直接使用了第三种方法导致的。也就是禁用图片翻转。 问题解决启用图片翻转。 训练结果训练后的模型放在output/faster_rcnn_alt_opt/train/VGG16_faster_rcnn_final.caffemodel，该模型可以用于之后的检测。 检测检测步骤经过以上训练后，就可以用得到的模型来进行检测了。检测所参考的代码是tools/demo.py，具体步骤如下： 将output/faster_rcnn_alt_opt/train/VGG16_faster_rcnn_final.caffemodel，拷贝到data/faster_rcnn_models下，命名为VGG16_Caltech_faster_rcnn__final.caffemodel 进入tools/文件夹中，拷贝demo.py为demo_caltech.py。 修改demo_caltech.py代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154#!/usr/bin/env python# --------------------------------------------------------# Faster R-CNN# Copyright (c) 2015 Microsoft# Licensed under The MIT License [see LICENSE for details]# Written by Ross Girshick# --------------------------------------------------------import matplotlibmatplotlib.use('Agg');"""Demo script showing detections in sample images.See README.md for installation instructions before running."""import _init_pathsfrom fast_rcnn.config import cfgfrom fast_rcnn.test import im_detectfrom fast_rcnn.nms_wrapper import nmsfrom utils.timer import Timerimport matplotlib.pyplot as pltimport numpy as npimport scipy.io as sioimport caffe, os, sys, cv2import argparseCLASSES = ('__background__', # 这里改为自己的类别 'person')NETS = &#123;'vgg16': ('VGG16', 'VGG16_Caltech_faster_rcnn_final.caffemodel'), #这里需要修改为训练后得到的模型的名称 'zf': ('ZF', 'ZF_Caltech_faster_rcnn_final.caffemodel')&#125; #这里需要修改为训练后得到的模型的名称def vis_detections(im, image_name, class_name, dets, thresh=0.5): """Draw detected bounding boxes.""" inds = np.where(dets[:, -1] &gt;= thresh)[0] if len(inds) == 0: return im = im[:, :, (2, 1, 0)] fig, ax = plt.subplots(figsize=(12, 12)) ax.imshow(im, aspect='equal') for i in inds: bbox = dets[i, :4] score = dets[i, -1] ax.add_patch( plt.Rectangle((bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1], fill=False, edgecolor='red', linewidth=3.5) ) ax.text(bbox[0], bbox[1] - 2, '&#123;:s&#125; &#123;:.3f&#125;'.format(class_name, score), bbox=dict(facecolor='blue', alpha=0.5), fontsize=14, color='white') ax.set_title(('&#123;&#125; detections with ' 'p(&#123;&#125; | box) &gt;= &#123;:.1f&#125;').format(class_name, class_name, thresh), fontsize=14) plt.axis('off') plt.tight_layout() plt.draw() plt.savefig('/home/jk/py-faster-rcnn/output/faster_rcnn_alt_opt/test/'+image_name) #将检测后的图片保存到相应的路径def demo(net, image_name): """Detect object classes in an image using pre-computed object proposals.""" # Load the demo image im_file = os.path.join(cfg.DATA_DIR, 'VOCdevkit/Caltech/JPEGImages', image_name) im = cv2.imread(im_file) # Detect all object classes and regress object bounds timer = Timer() timer.tic() scores, boxes = im_detect(net, im) timer.toc() print ('Detection took &#123;:.3f&#125;s for ' '&#123;:d&#125; object proposals').format(timer.total_time, boxes.shape[0]) # Visualize detections for each class CONF_THRESH = 0.85 # 设置权值，越低检测出的框越多 NMS_THRESH = 0.3 for cls_ind, cls in enumerate(CLASSES[1:]): cls_ind += 1 # because we skipped background cls_boxes = boxes[:, 4*cls_ind:4*(cls_ind + 1)] cls_scores = scores[:, cls_ind] dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])).astype(np.float32) keep = nms(dets, NMS_THRESH) dets = dets[keep, :] vis_detections(im, image_name, cls, dets, thresh=CONF_THRESH)def parse_args(): """Parse input arguments.""" parser = argparse.ArgumentParser(description='Faster R-CNN demo') parser.add_argument('--gpu', dest='gpu_id', help='GPU device id to use [0]', default=0, type=int) parser.add_argument('--cpu', dest='cpu_mode', help='Use CPU mode (overrides --gpu)', action='store_true') parser.add_argument('--net', dest='demo_net', help='Network to use [vgg16]', choices=NETS.keys(), default='vgg16') args = parser.parse_args() return argsif __name__ == '__main__': cfg.TEST.HAS_RPN = True # Use RPN for proposals args = parse_args() prototxt = os.path.join(cfg.MODELS_DIR, NETS[args.demo_net][0], 'faster_rcnn_alt_opt', 'faster_rcnn_test.pt') caffemodel = os.path.join(cfg.DATA_DIR, 'faster_rcnn_models', NETS[args.demo_net][1]) if not os.path.isfile(caffemodel): raise IOError(('&#123;:s&#125; not found.\nDid you run ./data/script/' 'fetch_faster_rcnn_models.sh?').format(caffemodel)) if args.cpu_mode: caffe.set_mode_cpu() else: caffe.set_mode_gpu() caffe.set_device(args.gpu_id) cfg.GPU_ID = args.gpu_id net = caffe.Net(prototxt, caffemodel, caffe.TEST) print '\n\nLoaded network &#123;:s&#125;'.format(caffemodel) # Warmup on a dummy image im = 128 * np.ones((300, 500, 3), dtype=np.uint8) for i in xrange(2): _, _= im_detect(net, im) testfile_path = '/home/jk/py-faster-rcnn/data/VOCdevkit/Caltech/ImageSets/Main/test.txt' with open(testfile_path) as f: im_names = [x.strip()+'.jpg' for x in f.readlines()] # 从test.txt文件中读取图片文件名，找到相应的图片进行检测。也可以使用如下的方法，把项检测的图片存到tools/demo/文件夹下进行读取检测 #im_names = ['set06_V002_I00023.jpg', 'set06_V002_I00072.jpg', 'set06_V002_I00097.jpg', # 'set06_V002_I00151.jpg', 'set07_V010_I00247.jpg'] for im_name in im_names: print '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~' print 'Demo for data/demo/&#123;&#125;'.format(im_name) demo(net, im_name) plt.show() 在命令行中输入一下命令进行检测： 1python tools/demo_caltech.py 检测结果放几张检测后的结果图，感觉检测效果并不是很好，很多把背景当成行人的错误： 参考博客 使用Faster-Rcnn进行目标检测(实践篇) Train Fast-RCNN on Another Dataset]]></content>
      <categories>
        <category>经验</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>目标检测</tag>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习实践经验：用Faster R-CNN训练Caltech数据集——修改读写接口]]></title>
    <url>%2Fblog%2F4113466123%2F</url>
    <content type="text"><![CDATA[前言这部分主要讲如何修改Faster R-CNN的代码，来训练自己的数据集，首先确保你已经编译安装了py-faster-rcnn，并且准备好了数据集，具体可参考我上一篇文章。 py-faster-rcnn文件结构 caffe-fast-rcnn这里是caffe框架目录，用来进行caffe编译安装 data用来存放pre trained模型，比如ImageNet上的，要训练的数据集以及读取文件的cache缓存。 experiments存放配置文件，运行的log文件，另外这个目录下有scripts 用来获取imagenet的模型，以及作者训练好的fast rcnn模型，以及相应的pascal-voc数据集 lib用来存放一些python接口文件，如其下的datasets主要负责数据库读取，config负责cnn一些训练的配置选项 matlab放置matlab与python的接口，用matlab来调用实现detection models里面存放了三个模型文件，小型网络的ZF，大型网络VGG16，中型网络VGG_CNN_M_1024 output这里存放的是训练完成后的输出目录，默认会在default文件夹下 tools里面存放的是训练和测试的Python文件 修改训练代码所要操作文件结构介绍所有需要修改的训练代码都放到了py-faster-rcnn/lib文件夹下，我们进入文件夹，里面主要用到的文件夹有： datasets：该目录下主要存放读写数据接口。 fast-rcnn：该目录下主要存放的是python的训练和测试脚本，以及训练的配置文件。 roi_data_layer：该目录下主要存放一些ROI处理操作文件。 utils：该目录下主要存放一些通用操作比如非极大值nms，以及计算bounding box的重叠率等常用功能。 读写数据接口都放在datasets/文件夹下，我们进入文件夹，里面主要文件有： factory.py：这是个工厂类，用类生成imdb类并且返回数据库共网络训练和测试使用。 imdb.py：这是数据库读写类的基类，分装了许多db的操作，但是具体的一些文件读写需要继承继续读写 pascal_voc.py：这是imdb的子类，里面定义许多函数用来进行所有的数据读写操作。 从上面可以看出，我们主要对pascal_voc.py文件进行修改。 pascal_voc.py文件代码分析我们主要是基于pasca_voc.py这个文件进行修改，里面有几个重要的函数需要介绍： 123456789101112131415161718192021def __init__(self, image_set, devkit_path=None): # 这个是初始化函数，它对应着的是pascal_voc的数据集访问格式。 def image_path_at(self, i): # 根据第i个图像样本返回其对应的path，其调用image_path_from_index(self, index):作为其具体实现。 def image_path_from_index(self, index): # 实现了 image_path的具体功能def _load_image_set_index(self): # 加载了样本的list文件，根据ImageSet/Main/文件夹下的文件进行image_index的加载。 def _get_default_path(self): # 获得数据集地址def gt_roidb(self): # 读取并返回ground_truth的db def rpn_roidb(self): # 加载rpn产生的roi，调用_load_rpn_roidb(self, gt_roidb):函数作为其具体实现 def _load_rpn_roidb(self, gt_roidb): # 加载rpn_file def _load_pascal_annotation(self, index): # 这个函数是读取gt的具体实现 def _write_voc_results_file(self, all_boxes): # 将voc的检测结果写入到文件 def _do_python_eval(self, output_dir = 'output'): # 根据python的evluation接口来做结果的分析 修改pascal_voc.py文件要想对自己的数据集进行读取，我们主要是进行pascal_voc.py文件的修改，但是为了不破坏源文件，我们可以将pascal_voc.py进行拷贝复制，从而进行修改。这里我将pascal_voc.py文件拷贝成caltech.py文件： 1cp pascal_voc.py caltech.py 下面我们对caltech.py文件进行修改，在这里我会一一列举每个我修改过的函数。这里按照文件中的顺序排列。。 init函数修改这里是原始的pascal_voc的init函数，在这里，由于我们自己的数据集往往比voc的数据集要更简单的一些，在作者额代码里面用了很多的路径拼接，我们不用去迎合他的格式，将这些操作简单化即可。 原始的函数123456789101112131415161718192021222324252627282930313233def __init__(self, image_set, year, devkit_path=None): imdb.__init__(self, 'voc_' + year + '_' + image_set) self._year = year self._image_set = image_set self._devkit_path = self._get_default_path() if devkit_path is None \ else devkit_path self._data_path = os.path.join(self._devkit_path, 'VOC' + self._year) self._classes = ('__background__', # always index 0 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor') self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes))) self._image_ext = '.jpg' self._image_index = self._load_image_set_index() # Default to roidb handler self._roidb_handler = self.selective_search_roidb self._salt = str(uuid.uuid4()) self._comp_id = 'comp4' # PASCAL specific config options self.config = &#123;'cleanup' : True, 'use_salt' : True, 'use_diff' : False, 'matlab_eval' : False, 'rpn_file' : None, 'min_size' : 2&#125; assert os.path.exists(self._devkit_path), \ 'VOCdevkit path does not exist: &#123;&#125;'.format(self._devkit_path) assert os.path.exists(self._data_path), \ 'Path does not exist: &#123;&#125;'.format(self._data_path) 修改后的函数123456789101112131415161718192021222324252627def __init__(self, image_set, devkit_path=None):# initial function，把year删除 imdb.__init__(self, image_set) # imageset is train.txt or test.txt self._image_set = image_set self._devkit_path = devkit_path # devkit_path = '~/py-faster-rcnn/data/VOCdevkit' self._data_path = os.path.join(self._devkit_path, 'Caltech') # _data_path = '~/py-faster-rcnn/data/VOCdevkit/Caltech' self._classes = ('__background__', # always index 0 'person') # 我只有‘background’和‘person’两类 self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes))) self._image_ext = '.jpg' self._image_index = self._load_image_set_index() # Default to roidb handler self._roidb_handler = self.selective_search_roidb self._salt = str(uuid.uuid4()) self._comp_id = 'comp4' # PASCAL specific config options self.config = &#123;'cleanup' : True, 'use_salt' : True, 'use_diff' : True, # 我把use_diff改为true了，因为我的数据集xml文件中没有&lt;difficult&gt;标签，否则之后训练会报错 'matlab_eval' : False, 'rpn_file' : None, 'min_size' : 2&#125; assert os.path.exists(self._devkit_path), \ 'VOCdevkit path does not exist: &#123;&#125;'.format(self._devkit_path) assert os.path.exists(self._data_path), \ 'Path does not exist: &#123;&#125;'.format(self._data_path) _load_image_set_index函数修改原始的函数12345678910111213def _load_image_set_index(self): """ Load the indexes listed in this dataset's image set file. """ # Example path to image set file: # self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt image_set_file = os.path.join(self._data_path, 'ImageSets', 'Main', self._image_set + '.txt') assert os.path.exists(image_set_file), \ 'Path does not exist: &#123;&#125;'.format(image_set_file) with open(image_set_file) as f: image_index = [x.strip() for x in f.readlines()] return image_index 修改后的函数1234567891011121314def _load_image_set_index(self): """ Load the indexes listed in this dataset's image set file. """ # Example path to image set file: # self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt # /home/jk/py-faster-rcnn/data/VOCdevkit/Caltech/ImageSets/Main/train.txt image_set_file = os.path.join(self._data_path, 'ImageSets', 'Main', self._image_set + '.txt') assert os.path.exists(image_set_file), \ 'Path does not exist: &#123;&#125;'.format(image_set_file) with open(image_set_file) as f: image_index = [x.strip() for x in f.readlines()] return image_index 其实没改，只是加了一行注释，从而更好理解路径问题。 _get_default_path函数修改直接注释即可 _load_pascal_annotation函数修改原始的函数123456789101112131415161718192021222324252627282930313233343536373839404142434445def _load_pascal_annotation(self, index): """ Load image and bounding boxes info from XML file in the PASCAL VOC format. """ filename = os.path.join(self._data_path, 'Annotations', index + '.xml') tree = ET.parse(filename) objs = tree.findall('object') if not self.config['use_diff']: # Exclude the samples labeled as difficult non_diff_objs = [ obj for obj in objs if int(obj.find('difficult').text) == 0] # if len(non_diff_objs) != len(objs): # print 'Removed &#123;&#125; difficult objects'.format( # len(objs) - len(non_diff_objs)) objs = non_diff_objs num_objs = len(objs) boxes = np.zeros((num_objs, 4), dtype=np.uint16) gt_classes = np.zeros((num_objs), dtype=np.int32) overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32) # "Seg" area for pascal is just the box area seg_areas = np.zeros((num_objs), dtype=np.float32) # Load object bounding boxes into a data frame. for ix, obj in enumerate(objs): bbox = obj.find('bndbox') # Make pixel indexes 0-based x1 = float(bbox.find('xmin').text) - 1 y1 = float(bbox.find('ymin').text) - 1 x2 = float(bbox.find('xmax').text) - 1 y2 = float(bbox.find('ymax').text) - 1 cls = self._class_to_ind[obj.find('name').text.lower().strip()] boxes[ix, :] = [x1, y1, x2, y2] gt_classes[ix] = cls overlaps[ix, cls] = 1.0 seg_areas[ix] = (x2 - x1 + 1) * (y2 - y1 + 1) overlaps = scipy.sparse.csr_matrix(overlaps) return &#123;'boxes' : boxes, 'gt_classes': gt_classes, 'gt_overlaps' : overlaps, 'flipped' : False, 'seg_areas' : seg_areas&#125; 修改后的函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546def _load_pascal_annotation(self, index): """ Load image and bounding boxes info from XML file in the PASCAL VOC format. """ filename = os.path.join(self._data_path, 'Annotations', index + '.xml') tree = ET.parse(filename) objs = tree.findall('object') if not self.config['use_diff']: # Exclude the samples labeled as difficult non_diff_objs = [ obj for obj in objs if int(obj.find('difficult').text) == 0] # if len(non_diff_objs) != len(objs): # print 'Removed &#123;&#125; difficult objects'.format( # len(objs) - len(non_diff_objs)) objs = non_diff_objs num_objs = len(objs) boxes = np.zeros((num_objs, 4), dtype=np.uint16) gt_classes = np.zeros((num_objs), dtype=np.int32) overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32) # "Seg" area for pascal is just the box area seg_areas = np.zeros((num_objs), dtype=np.float32) # Load object bounding boxes into a data frame. for ix, obj in enumerate(objs): bbox = obj.find('bndbox') # Make pixel indexes 0-based # 这里我把‘-1’全部删除掉了，防止有的数据是0开始，然后‘-1’导致变为负数，产生AssertError错误 x1 = float(bbox.find('xmin').text) y1 = float(bbox.find('ymin').text) x2 = float(bbox.find('xmax').text) y2 = float(bbox.find('ymax').text) cls = self._class_to_ind[obj.find('name').text.lower().strip()] boxes[ix, :] = [x1, y1, x2, y2] gt_classes[ix] = cls overlaps[ix, cls] = 1.0 seg_areas[ix] = (x2 - x1 + 1) * (y2 - y1 + 1) overlaps = scipy.sparse.csr_matrix(overlaps) return &#123;'boxes' : boxes, 'gt_classes': gt_classes, 'gt_overlaps' : overlaps, 'flipped' : False, 'seg_areas' : seg_areas&#125; main函数修改原始的函数 12345if __name__ == '__main__': from datasets.pascal_voc import pascal_voc d = pascal_voc('trainval', '2007') res = d.roidb from IPython import embed; embed() 修改后的函数 12345if __name__ == '__main__': from datasets.caltech import caltech # 导入caltech包 d = caltech('train', '/home/jk/py-faster-rcnn/data/VOCdevkit')#调用构造函数，传入imageset和路径 res = d.roidb from IPython import embed; embed() 至此读取接口修改完毕，该文件中的其他函数并未修改。 修改factory.py文件当网络训练时会调用factory里面的get方法获得相应的imdb，首先在文件头import 把pascal_voc改成caltech 在这个文件作者生成了多个数据库的路径，我们自己数据库只要给定根路径即可，修改主要有以下4个 函数之后有两个多级的for循环，也将其注释 直接定义devkit。 利用创建自己的训练和测试的imdb set，这里的name的格式为caltech_{}。 原始的代码123456789101112131415161718192021222324252627282930313233343536373839404142# --------------------------------------------------------# Fast R-CNN# Copyright (c) 2015 Microsoft# Licensed under The MIT License [see LICENSE for details]# Written by Ross Girshick# --------------------------------------------------------"""Factory method for easily getting imdbs by name."""__sets = &#123;&#125;from datasets.pascal_voc import pascal_vocfrom datasets.coco import cocoimport numpy as np# Set up voc_&lt;year&gt;_&lt;split&gt; using selective search "fast" modefor year in ['2007', '2012']: for split in ['train', 'val', 'trainval', 'test']: name = 'voc_&#123;&#125;_&#123;&#125;'.format(year, split) __sets[name] = (lambda split=split, year=year: pascal_voc(split, year))# Set up coco_2014_&lt;split&gt;for year in ['2014']: for split in ['train', 'val', 'minival', 'valminusminival']: name = 'coco_&#123;&#125;_&#123;&#125;'.format(year, split) __sets[name] = (lambda split=split, year=year: coco(split, year))# Set up coco_2015_&lt;split&gt;for year in ['2015']: for split in ['test', 'test-dev']: name = 'coco_&#123;&#125;_&#123;&#125;'.format(year, split) __sets[name] = (lambda split=split, year=year: coco(split, year))def get_imdb(name): """Get an imdb (image database) by name.""" if not __sets.has_key(name): raise KeyError('Unknown dataset: &#123;&#125;'.format(name)) return __sets[name]()def list_imdbs(): """List all registered imdbs.""" return __sets.keys() 修改后的文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# --------------------------------------------------------# Fast R-CNN# Copyright (c) 2015 Microsoft# Licensed under The MIT License [see LICENSE for details]# Written by Ross Girshick# --------------------------------------------------------"""Factory method for easily getting imdbs by name."""__sets = &#123;&#125;from datasets.caltech import caltech # 导入caltech包#from datasets.coco import coco#import numpy as npdevkit = '/home/jk/py-faster-rcnn/data/VOCdevkit'# Set up voc_&lt;year&gt;_&lt;split&gt; using selective search "fast" mode#for year in ['2007', '2012']:# for split in ['train', 'val', 'trainval', 'test']:# name = 'voc_&#123;&#125;_&#123;&#125;'.format(year, split)# __sets[name] = (lambda split=split, year=year: pascal_voc(split, year))# Set up coco_2014_&lt;split&gt;#for year in ['2014']:# for split in ['train', 'val', 'minival', 'valminusminival']:# name = 'coco_&#123;&#125;_&#123;&#125;'.format(year, split)# __sets[name] = (lambda split=split, year=year: coco(split, year))# Set up coco_2015_&lt;split&gt;#for year in ['2015']:# for split in ['test', 'test-dev']:# name = 'coco_&#123;&#125;_&#123;&#125;'.format(year, split)# __sets[name] = (lambda split=split, year=year: coco(split, year))# Set up caltech_&lt;split&gt;for split in ['train', 'test']: name = 'caltech_&#123;&#125;'.format(split) __sets[name] = (lambda imageset=split, devkit=devkit: caltech(imageset, devkit))def get_imdb(name): """Get an imdb (image database) by name.""" if not __sets.has_key(name): raise KeyError('Unknown dataset: &#123;&#125;'.format(name)) return __sets[name]()def list_imdbs(): """List all registered imdbs.""" return __sets.keys() 修改init.py文件在行首添加上 from .caltech import caltech 总结 坐标的顺序我再说一次，要左上右下，并且x1必须要小于x2，这个是基本，反了会在坐标水平变换的时候会出错，坐标从0开始，如果已经是0，则不需要再-1。 训练图像的大小不要太大，否则生成的OP也会太多，速度太慢，图像样本大小最好调整到500，600左右，然后再提取OP 如果读取并生成pkl文件之后，实际数据内容或者顺序还有问题，记得要把data/cache/下面的pkl文件给删掉。 参考博客 Fast RCNN训练自己的数据集 （2修改读写接口） Faster R-CNN教程]]></content>
      <categories>
        <category>经验</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>目标检测</tag>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习实践经验：用Faster R-CNN训练行人检测数据集Caltech——准备工作]]></title>
    <url>%2Fblog%2F2093106769%2F</url>
    <content type="text"><![CDATA[前言Faster R-CNN是Ross Girshick大神在Fast R-CNN基础上提出的又一个更加快速、更高mAP的用于目标检测的深度学习框架，它对Fast R-CNN进行的最主要的优化就是在Region Proposal阶段，引入了Region Proposal Network (RPN)来进行Region Proposal，同时可以达到和检测网络共享整个图片的卷积网络特征的目标，使得region proposal几乎是cost free的。 关于Faster R-CNN的详细介绍，可以参考我上一篇博客。 Faster R-CNN的代码是开源的，有两个版本：MATLAB版本(faster_rcnn)，Python版本(py-faster-rcnn)。 这里我主要使用的是Python版本，Python版本在测试期间会比MATLAB版本慢10%，因为Python layers中的一些操作是在CPU中执行的，但是准确率应该是差不多的。 准备工作1——py-faster-rcnn的编译安装测试py-faster-rcnn的编译安装 克隆Faster R-CNN仓库： 1git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git 一定要加上--recursive标志，假设克隆后的文件夹名字叫py-faster-rcnn 编译Cython模块： 12cd py-faster-rcnn/libmake 编译里面的Caffe和pycaffe： 12345cd py-faster-rcnn/caffe-fast-rcnn# 按照编译Caffe的方法，进行编译# 注意Makefile.config的修改，这里不再赘述Caffe的安装# 编译make -j8 &amp;&amp; make pycaffe 这里贴上我的Makefile.config文件代码，根据你的情况进行相应修改 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114## Refer to http://caffe.berkeleyvision.org/installation.html# Contributions simplifying and improving our build system are welcome!# cuDNN acceleration switch (uncomment to build with cuDNN).USE_CUDNN := 1# CPU-only switch (uncomment to build without GPU support).# CPU_ONLY := 1# uncomment to disable IO dependencies and corresponding data layers# USE_OPENCV := 0# USE_LEVELDB := 0# USE_LMDB := 0# uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)# You should not set this flag if you will be reading LMDBs with any# possibility of simultaneous read and write# ALLOW_LMDB_NOLOCK := 1# Uncomment if you're using OpenCV 3OPENCV_VERSION := 3# To customize your choice of compiler, uncomment and set the following.# N.B. the default for Linux is g++ and the default for OSX is clang++# CUSTOM_CXX := g++# CUDA directory contains bin/ and lib/ directories that we need.CUDA_DIR := /usr/local/cuda# On Ubuntu 14.04, if cuda tools are installed via# "sudo apt-get install nvidia-cuda-toolkit" then use this instead:# CUDA_DIR := /usr# CUDA architecture setting: going with all of them.# For CUDA &lt; 6.0, comment the *_50 lines for compatibility.CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \-gencode arch=compute_20,code=sm_21 \-gencode arch=compute_30,code=sm_30 \-gencode arch=compute_35,code=sm_35 \-gencode arch=compute_50,code=sm_50 \-gencode arch=compute_50,code=compute_50# BLAS choice:# atlas for ATLAS (default)# mkl for MKL# open for OpenBlasBLAS :=mkl# Custom (MKL/ATLAS/OpenBLAS) include and lib directories.# Leave commented to accept the defaults for your choice of BLAS# (which should work)!# BLAS_INCLUDE := /path/to/your/blas# BLAS_LIB := /path/to/your/blas# Homebrew puts openblas in a directory that is not on the standard search path# BLAS_INCLUDE := $(shell brew --prefix openblas)/include# BLAS_LIB := $(shell brew --prefix openblas)/lib# This is required only if you will compile the matlab interface.# MATLAB directory should contain the mex binary in /bin.MATLAB_DIR := /usr/local/MATLAB/R2016b# MATLAB_DIR := /Applications/MATLAB_R2012b.app# NOTE: this is required only if you will compile the python interface.# We need to be able to find Python.h and numpy/arrayobject.h.# PYTHON_INCLUDE := /usr/include/python2.7 \/usr/lib/python2.7/dist-packages/numpy/core/include# Anaconda Python distribution is quite popular. Include path:# Verify anaconda location, sometimes it's in root.ANACONDA_HOME := $(HOME)/anacondaPYTHON_INCLUDE := $(ANACONDA_HOME)/include \$(ANACONDA_HOME)/include/python2.7 \$(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \$ /usr/include/python2.7# Uncomment to use Python 3 (default is Python 2)# PYTHON_LIBRARIES := boost_python3 python3.5m# PYTHON_INCLUDE := /usr/include/python3.5m \# /usr/lib/python3.5/dist-packages/numpy/core/include# We need to be able to find libpythonX.X.so or .dylib.# PYTHON_LIB := /usr/libPYTHON_LIB := $(ANACONDA_HOME)/lib# Homebrew installs numpy in a non standard path (keg only)# PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include# PYTHON_LIB += $(shell brew --prefix numpy)/lib# Uncomment to support layers written in Python (will link against Python libs)WITH_PYTHON_LAYER := 1# Whatever else you find you need goes here.# INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include# LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/libINCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial# If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies# INCLUDE_DIRS += $(shell brew --prefix)/include# LIBRARY_DIRS += $(shell brew --prefix)/lib# Uncomment to use `pkg-config` to specify OpenCV library paths.# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)# USE_PKG_CONFIG := 1# N.B. both build and distribute dirs are cleared on `make clean`BUILD_DIR := buildDISTRIBUTE_DIR := distribute# Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171# DEBUG := 1# The ID of the GPU that 'make runtest' will use to run unit tests.TEST_GPUID := 0# enable pretty build (comment to see full commands)Q ?= @ Demo运行为了检验你的py-faster-rcnn是否成功安装，作者给出了一个demo，可以利用在PASCAL VOC2007数据集上体现训练好的模型，来进行demo的运行，步骤如下： 下载预训练好的Faster R-CNN检测器： 12cd py-faster-rcnn./data/scripts/fetch_faster_rcnn_models.sh 这条命令会自动下载名为faster_rcnn_models.tgz的文件，解压后会创建data/faster_rcnn_models文件夹，里面会有两个模型： ZF_faster_rcnn_final.caffemodel：在ZF网络模型下训练所得 VGG16_faster_rcnn_final.caffemodel：在VGG16网络模型下训练所得。 运行demo： 12cd py-faster-rcnn./tools/demo.py demo会检测5张图片，这5张图片放在data/demo/文件夹下，其中一张的检测结果如下： 至此如果上述过程没有出错，那么py-faster-rcnn算是成功编译安装。 若出现报错如下： 1ImportError: /xx/xx/xx/py-faster-rcnn/tools/../lib/nms/cpu_nms.so: undefined symbol: PyFPE_jbuf 需要将lib/fast_rcnn/nms_wrapper.py文件中的from nms.cpu_nms import cpu_nms注释掉即可。 准备工作2——Caltech数据集由于Faster R-CNN的一部分实验是在PASCAL VOC2007数据集上进行的，所以要想用Faster R-CNN训练我们自己的数据集，首先应该搞清楚PASCAL VOC2007数据集中的目录、图片、标注格式，这样我们才能用自己的数据集制作出类似于PASCAL VOC2007类似的数据集，供Faster R-CNN来进行训练及测试。 获取PASCAL VOC2007数据集这一部分不是必须的，如果你需要PASCAL VOC2007数据集，可以利用以下命令获取数据集，但我们下载VOC数据集的目的主要是观察他的文件结构和文件内容，以便于我们构建符合要求的自己的数据集。 创建一个专门用来存数据集的地方，假设是$HOME/data文件夹。 下载PASCAL VOC2007的训练、验证和测试数据集： 123cd $HOME/datawget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tarwget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar 下载完后用以下命令解压： 12tar xvf VOCtrainval_06-Nov-2007.tartar xvf VOCtest_06-Nov-2007.tar 会得到如下文件结构： 123456$HOME/data/VOCdevkit/ # 根文件夹$HOME/data/VOCdevkit/VOC2007 # VOC2007文件夹$HOME/data/VOCdevkit/VOC2007/Annotations # 标记文件夹$HOME/data/VOCdevkit/VOC2007/ImageSets # 供train.txt、test.txt、val.txt等文件存放的文件夹$HOME/data/VOCdevkit/VOC2007/JPEGImages # 存放图片文件夹# ... 以及其他的文件夹及子文件夹 ... 创建快捷方式symlinks来连接到VOC数据集存放的地方： 12cd py-faster-rcnn/dataln -s $HOME/data/VOCdevkit/ VOCdevkit 这里需要把$HOME/data/VOCdevkit/改为你存放VOCdevkit文件夹的路径 最好使用symlinks来在共享同一份数据集，防止数据集多处拷贝，占用空间。 至此VOC数据集创建完毕。 PASCAL VOC数据集的分析PASCAL VOC数据集的文件结构，如下： 12345678910└── VOCdevkit └── VOC2007 ├── Annotations ├── ImageSets │ ├── Layout │ ├── Main │ └── Segmentation ├── JPEGImages ├── SegmentationClass └── SegmentationObject Annotations该文件夹主要用来存放图片标注（即为ground truth），文件是.xml格式，每张图片都有一个.xml文件与之对应。选取其中一个文件进行如下分析： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;annotation&gt; &lt;folder&gt;VOC2007&lt;/folder&gt; # 必须有，父文件夹的名称 &lt;filename&gt;000005.jpg&lt;/filename&gt; # 必须有 &lt;source&gt; # 可有可无 &lt;database&gt;The VOC2007 Database&lt;/database&gt; &lt;annotation&gt;PASCAL VOC2007&lt;/annotation&gt; &lt;image&gt;flickr&lt;/image&gt; &lt;flickrid&gt;325991873&lt;/flickrid&gt; &lt;/source&gt; &lt;owner&gt; # 可有可无 &lt;flickrid&gt;archintent louisville&lt;/flickrid&gt; &lt;name&gt;?&lt;/name&gt; &lt;/owner&gt; &lt;size&gt; # 表示图像大小 &lt;width&gt;500&lt;/width&gt; &lt;height&gt;375&lt;/height&gt; &lt;depth&gt;3&lt;/depth&gt; &lt;/size&gt; &lt;segmented&gt;0&lt;/segmented&gt; # 用于分割 &lt;object&gt; # 目标信息，类别，bbox信息，图片中每个目标对应一个&lt;object&gt;标签 &lt;name&gt;chair&lt;/name&gt; &lt;pose&gt;Rear&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;263&lt;/xmin&gt; &lt;ymin&gt;211&lt;/ymin&gt; &lt;xmax&gt;324&lt;/xmax&gt; &lt;ymax&gt;339&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;chair&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;1&lt;/truncated&gt; &lt;difficult&gt;1&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;5&lt;/xmin&gt; &lt;ymin&gt;244&lt;/ymin&gt; &lt;xmax&gt;67&lt;/xmax&gt; &lt;ymax&gt;374&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt;&lt;/annotation&gt; 需要注意的，对于我们自己准备的xml标记文件中，每个&lt;object&gt;标签中的&lt;xmin&gt;和&lt;ymin&gt;标签中所对应的坐标值最好大于0，千万不能为负数，否则在训练过程中会报错：AssertionError: assert (boxes[:, 2]) &gt;= boxes[:, 0]).all()，如下： 所以为了能够顺利训练，一定要仔细检查自己的xml文件中的左上角的坐标是否都为正。我被这个bug卡了一两天，最终把自己标记中所有的错误坐标找出来，才得以顺利训练。 ImageSetsImageSets文件夹下有三个子文件夹，这里我们只需关注Main文件夹即可。Main文件夹下主要用到的是train.txt、val.txt、test.txt、trainval.txt文件，每个文件中写着供训练、验证、测试所用的文件名的集合，如下： JPEGImagesJPEGImages文件夹下主要存放着所有的.jpg文件格式的输入图片，不在赘述。 制作VOC类似的Caltech数据集经过以上对PASCAL VOC数据集文件结构的分析，我们仿照其，创建首先创建类似的文件结构即可： 1234567└── VOCdevkit └── VOC2007 └── Caltech ├── Annotations ├── ImageSets │ └── Main └── JPEGImages 我建议将Caltech文件创建一个symlinks链接到VOCdevkit文件夹之下，因为这样会方便之后训练代码的修改。 至于Caltech数据集如何从.seq文件转化为一张张.jpg图片，这里可以参考这里。 至于Annotations中一个个.xml标记文件是实验室师兄给我的，上面提到的方法也可以转化，但是并不符合要求。 至于ImageSets中的train.txt是根据.xml文件得来的，test.txt是每个seq中每隔30帧取一帧图片得来的。 以上所有和Caltech数据集有关的文件，都可以直接邮件与我联系，我直接发给你，可以省下不少制作数据集的时间。 参考博客 FastRCNN 训练自己数据集 (1编译配置) 目标检测—Faster RCNN2]]></content>
      <categories>
        <category>经验</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>目标检测</tag>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习论文笔记：Faster R-CNN]]></title>
    <url>%2Fblog%2F3802700508%2F</url>
    <content type="text"><![CDATA[Abstract Region Proposal的计算是基于Region Proposal算法来假设物体位置的物体检测网络比如：SPPnet, Fast R-CNN运行时间的瓶颈。 Faster R-CNN引入了Region Proposal Network（RPN）来和检测网络共享整个图片的卷积网络特征，因此使得region proposal几乎是cost free的。 RPN-&gt;预测物体边界（object bounds）和在每一位置的分数（objectness score） 通过在一个网络中共享RPN和Fast R-CNN的卷积特征来融合两者——使用“attention”机制。 300 proposals pre image. Introduction RP是当前许多先进检测系统的瓶颈。 Region proposal methods: Selective Search: one of the most popular method EdgeBoxes: trade off between proposal quality and speed. region proposal这一步依旧和检测网络花费同样多的时间。 Fast R-CNN生成的feature map 也能用来生成RP。在这些卷积特征之上我们通过这样的方式构建RPN：通过添加几个额外的卷积层来模拟一个regular grid上每一个位置的regress region bounds和objectness scores。所以RPN也是一种fully convolutional network(FCN)，从而可以端到端训练来产生detection proposals。 anchor boxes：references at multiple scales and aspect ratios. 我们的方法可以看成pyramid of regression reference，从而避免枚举多尺寸、多横纵比的images或者filters Related Work R-CNN主要是一个分类器，他不能预测object bounds，他的准确性依赖于Region proposal模块的表现 Faster R-CNN 由两个模块组成： 第一个模块：A deep fuuly convolutional network that proposes regions，用来proposes regions. 第二个模块：Fast R-CNN检测器，使用第一模块提出的regions。 Attention mechanisms：RPN module告诉Fast R-CNN module 往哪里看（where to look） Region Proposal Networks 输入：一张任意尺寸的图片。 输出：一组矩形object proposal，每个proposal都有一个score。 是一个fully convolutional network（FCN），由于我们需要在RPN和Fast RCNN之间共享权值，所以我们假设两个网络共享一组共同的卷积层。 为了生成region proposals，我们在最后一层共享卷积层输出的feature map上滑动一个微型网络。这个微型网络将输入的feature map上的nxn的空间窗口作为输入。每一个滑动窗口被映射为一个低维特征(ZF: 256-d, VGG: 512-d, 之后跟着ReLU层)。这些特征然后被送到两个sibling全连接层中——一个box-regression(reg)层和一个box-classification(cls)层。 注意：因为微型网络以滑动窗口方式操作，所以完全连接层在所有空间位置上共享。 这种结构自然地通过一个n×n卷积层，后面是两个同级1×1卷积层（分别用于rpn_reg和rpn_cls）来实现。 生成region proposal的思路： rpn网络结构定义如下： ​ ​ Anchors 假设每个位置最大可能的proposal的数量为k，在每个sliding-window位置，同时预测几个RP： reg layer：有4k个输出 cls layer：有2k个输出，指出该每一个proposal是否是object，estimate probability of object or not object for each proposal。 k个proposal相对于k个参考框（reference boxes）而参数化，我们将参考框称为anchor。 一个anchor位于sliding window的中间，同时关联着一个scale和aspect ration。 Translation-Invariant Anchors(平移不变性) 如果移动了一张图像中的一个物体，这proposal应该也移动了，而且相同的函数可以预测出热议未知的proposal。MultiBox不具备如此功能 平移不变性可以减少模型大小。 Multi-Scale Anchor as Regression References Two popular ways for multi-scale predictions: 第一种：based on image/feature pyramids, 如：DPM and CNN-based methods。图像被resized成不同尺寸，然后为每一种尺寸计算feature maps(HOG或者deep convolutional features)。这种方法比较费时。 第二种：use sliding windows of multiple scales (and/or aspect ratios) on the feature maps.——filters金字塔。第二种方法经常和第一种方法联合使用 本论文的方法：anchor金字塔——more cost-efficient，只依靠单尺寸的图像和feature map。 The design of multiscale anchors is a key component for sharing features without extra cost for addressing scales. Loss Function 为了训练RPN，我们给每个anchor设置了一个二元标签（是物体或者不是物体） 两类anchor是有正标签（is object）的： anchor/anchors with highest IoU overlap with a ground-truth box。 an anchor that has IoU overlap higher than 0.7 with any ground-truth box. 第二种方法更好检测正样本，在第二种情况下如果找不到正样本，那么使用第一种。 如果一个anchor和任何ground-truth boxes的IoU值小于0.3，那么该anchor为负标签 非正非负样本对training objective没有用。 Loss Function： ​ $N{cls}=256,N{reg}=256*9=2304,\lambda=10$，这样两个loss就可以权重基本相当了。 Bounding box regression 这个可以考虑为从anchor box回归到附近的ground truth box。 和R-CNN和Fast R-CNN的bounding box regression方法不同的是： 前两种的回归是在从任意大小RoI中提取的特征进行回归的，所以regression weights在所有尺寸中共享。 在我们的方法中，用于回归的特征都是同一个3x3的空间特征。考虑到变化的尺寸，有k个不同的bounding boxe回归器去学习，每一个回归器负责去学习一个尺寸一个衡重比的anchor。所以k个回归器是不共享权值的。所以得益于anchor的设计，即使特征规定，我们依旧可以去预测不同尺寸的box。 Training RPNs image-centric sampling strategy mini-batch: arises from a single image that contains many positive and negative example anchors. 随机在一张图片中采样256个anchors来计算一个mini-batch的loss function。正负anchors = 1:1. all new layers的权值初始化：高斯分布$(\mu = 0, \sigma = 0.01)$，all other layers（比如共享卷积层）用ImageNet来权值初始化。用ZF net来进行进行微调。 学习率：0.001(60k)-&gt;0.0001(20k) 动量：0.9 weight decay: 0.0005 Sharing Feature for RPN and Fast R-CNN sharing convolutional layers between the two networks, rather than learning two separate networks 三种特征共享的方法： Alternating training：迭代，先训练PRN，然后用proposal去训练Fast R-CNN。被Fast R-CNN微调的网络然后用来初始化PRN，以此迭代。本论文所有的实现都是使用该方法。 Approximate joint training： RPN和Fast R-CNN融合到一个网络中进行训练。在每次SGD迭代过程中： 前向传递：RPN产生region proposals，这些proposals被当做固定的、提前计算好的proposal来训练Fast R-CNN检测器。 反向传递：对于共享层来说，来自RPN的loss和Fast R-CNN的loss结合. 但是这种方法不考虑Bounding Boxes，忽略了proposal boxes的坐标也是网络的输出。所以这种方法叫做approximate Non-approximate joint training: 考虑Bounding Boxes。 4-step Alternating Training: Step 1: train the RPN, initialized with an ImageNet-pre-trained model and ﬁne-tuned end-to-end for the region proposal task. Step 2: train a separate detection network by Fast R-CNN using the proposals generated by the step-1 RPN. 同样使用ImageNet-pre-trained model来初始化。此时两个网络并没有共享卷积层。 Step 3: use the detector network to initialize RPN training but we ﬁx the shared convolutional layers and only ﬁne-tune the layers unique to RPN. 现在两个网络共享卷积层 Step 4: keeping the shared convolutional layers ﬁxed, we ﬁne-tune the unique layers of Fast R-CNN. Implementation Details Multi-scale与speed-accuracy之间的trade-off To reduce redundancy, we adopt non-maximum suppression (NMS) on the proposal regions based on their cls scores.]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>目标检测</tag>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习论文笔记：Fast R-CNN]]></title>
    <url>%2Fblog%2F1679631826%2F</url>
    <content type="text"><![CDATA[知识点 mAP：detection quality. Abstract 本文提出一种基于快速区域的卷积网络方法（快速R-CNN）用于对象检测。 快速R-CNN采用多项创新技术来提高训练和测试速度，同时提高检测精度。 采用VGG16的网络：VGG: 16 layers of 3x3 convolution interleaved with max pooling + 3 fully-connected layers Introduction 物体检测相对于图像分类是更复杂的，应为需要物体准确的位置。 首先，必须处理许多候选对象位置（通常称为“proposal”）。 其次，这些候选者只提供粗略的定位，必须进行精确定位才能实现精确定位。 这些问题的解决方案经常损害 速度 ， 准确性 或 简单性 。 R-CNN and SPPnet R-CNN(Region-based Convolution Network)具有几个显著的缺点： 训练是一个多级管道。 训练在空间和时间上是昂贵的。 物体检测速度很慢。 R-CNN是慢的，因为它对每个对象proposal执行ConvNet正向传递，而不共享计算（sharing computation）。 Spatial pyramid pooling networks（SPPnets），利用sharing computation对R-CNN进行了加速，但是SPPnets也具有明显的缺点，像R-CNN一样，SPPnets也需要： 训练是一个多阶段流程， 涉及提取特征， 用对数损失精简网络 训练SVM 赋予边界框回归。 特征也需要也写入磁盘。 但与R-CNN 不同 ，在[11]中提出的fine-tuning算法不能更新在空间金字塔池之前的卷积层。 不出所料，这种限制（固定的卷积层）限制了非常深的网络的精度。 Contributions Fast R-CNN优点： 比R-CNN，SPPnet更高的检测质量（mAP） 训练是单阶段的，使用多任务损失（multi-task loss） 训练可以更新所有网络层 特征缓存不需要磁盘存储 Fast R-CNN architecture and training 整体框架 快速R-CNN网络将整个图像和一组object proposals作为输入。 网络首先使用几个卷积（conv）和最大池层来处理整个图像，以产生conv feature map。 然后，对于每个对象proposal， 感兴趣区域（RoI）池层 从特征图中抽取固定长度的特征向量。 每个特征向量被馈送到完全连接（fc）层序列，其最终分支成两个同级输出层： 一个产生对K个对象类加上全部捕获的“背景”类的softmax概率估计(one that produces softmax probability estimates over K object classes plus a catch-all “background” class) 另一个对每个K对象类输出四个实数，每组4个值编码提炼定义K个类中的一个的的边界框位置。(another layer that outputs four real-valued numbers for each of the K object classes. Each set of 4 values encodes reﬁned bounding-box positions for one of the K classes.) The RoI pooling layer Rol pooling layer的作用主要有两个： 一个是将image中的RoI定位到feature map中对应patch 另一个是用一个单层的SPP layer将这个feature map patch下采样为大小固定的feature再传入全连接层。 RoI池层使用最大池化将任何有效的RoI区域内的特征转换成具有H×W（例如，7×7）的固定空间范围的小feature map，其中H和W是层超参数 它们独立于任何特定的RoI。 在本文中，RoI是conv feature map中的一个矩形窗口。 每个RoI由定义其左上角（r，c）及其高度和宽度（h，w）的四元组（r，c，h，w）定义。 RoI层仅仅是Sppnets中的spatial pyramid pooling layer的特殊形式，其中只有一个金字塔层. Initializing from pre-trained networks 用了3个预训练的ImageNet网络（CaffeNet/ VGG_CNN_M_1024 /VGG16）。预训练的网络初始化Fast RCNN要经过三次变形： 最后一个max pooling层替换为RoI pooling层，设置H’和W’与第一个全连接层兼容。 最后一个全连接层和softmax（原本是1000个类）替换为softmax的对K+1个类别的分类层，和bounding box 回归层。 输入修改为两种数据：一组N个图形，R个RoI，batch size和ROI数、图像分辨率都是可变的。 Fine-tuning for detection 利用反向传播算法进行训练所有网络的权重是Fast R-CNN很重要的一个能力。 我们提出了一种更有效的训练方法，利用在训练期间的特征共享（feature sharing during training）。 在Fast R-CNN训练中， 随机梯度下降（SGD）小批量分层采样 ，首先通过采样N个图像，然后通过从每个图像采样 R/N个 RoIs。 关键的是，来自同一图像的RoI在向前和向后传递中 共享计算 和存储。 此外为了分层采样，Fast R-CNN使用了一个流水线训练过程，利用一个fine-tuning阶段来联合优化一个softmax分类器和bounding box回归，而非训练一个softmax分类器，SVMs，和regression在三个独立的阶段。 Multi-task loss： 两个sibling输出层： 第一层：输出离散概率分布（针对每个RoIs），$p=(p_0,…,p_K)$，分别对应$K+1$个类。p是在一个全连接层的$K+1$个输出上的softmax。 第二层：输出bounding-box的回归偏移(bounding-box regression offsets)，针对K object classes中的每一个类，计算$t^k=(t^k_x,t^k_y,t^k_w,t^k_h)$，具体见R-CNN得补充材料，里面有很详细的介绍bounding box regression。 每一个训练RoIs被标注一个ground truth类$u$，和一个ground truth bounding box 回归目标$v$。 两个loss，以下分别介绍： 对于分类loss，是一个N+1路的softmax输出，其中的N是类别个数，1是背景。 对于回归loss，是一个4xN路输出的regressor，也就是说对于每个类别都会训练一个单独的regressor的意思，比较有意思的是，这里regressor的loss不是L2的，而是一个平滑的L1，形式如下： 我们利用一个multi-task loss L 在每个被标注的RoI上来联合训练分类器和bounding box regression Mini-batch sampling：在微调时，每个SGD的mini-batch是随机找两个图片，R为128，因此每个图上取样64个RoI。从object proposal中选25%的RoI，就是和ground-truth交叠至少为0.5的。剩下的作为背景。 Back-propagation through RoI pooling layers： RoI pooling层计算损失函数对每个输入变量x的偏导数，如下： y是pooling后的输出单元，x是pooling前的输入单元，如果y由x pooling而来，则将损失L对y的偏导计入累加值，最后累加完R个RoI中的所有输出单元。下面是我理解的x、y、r的关系： Scale invariance 这里讨论object的scale问题，就是网络对于object的scale应该是要不敏感的。这里还是引用了SPP的方法，有两种: brute force （single scale），也就是简单认为object不需要预先resize到类似的scale再传入网络，直接将image定死为某种scale，直接输入网络来训练就好了，然后期望网络自己能够学习到scale-invariance的表达。 image pyramids （multi scale），也就是要生成一个金字塔，然后对于object，在金字塔上找到一个大小比较接近227x227的投影版本，然后用这个版本去训练网络。 可以看出，2应该比1更加好，作者也在5.2讨论了，2的表现确实比1好，但是好的不算太多，大概是1个mAP左右，但是时间要慢不少，所以作者实际采用的是第一个策略，也就是single scale。 这里，FRCN测试之所以比SPP快，很大原因是因为这里，因为SPP用了2，而FRCN用了1。 Fast R-CNN detection 大型全连接层很容易的可以通过将他们与 truncated SVD(奇异值分解) 压缩来加速计算。 Main results All Fast R-CNN results in this paper using VGG16 ﬁne-tune layers conv3 1 and up; all experments with models S and M ﬁne-tune layers conv2 and up. Design evaluationDo we need more training data? 在训练期间，作者做过的唯一一个数据增量的方式是水平翻转。 作者也试过将VOC12的数据也作为拓展数据加入到finetune的数据中，结果VOC07的mAP从66.9到了70.0，说明对于网络来说， 数据越多就是越好的。]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>目标检测</tag>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习论文笔记：Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition]]></title>
    <url>%2Fblog%2F3054155989%2F</url>
    <content type="text"><![CDATA[Abstract 现有的深卷积神经网络（CNN）需要固定尺寸（例如，224×224）的输入图像。 新的网络结构，称为SPP-net，可以生成固定长度的表示，而不管图像大小/规模。 使用SPP-net，我们从整个图像只计算一次特征图，然后在任意区域（子图像）中池特征以生成固定长度表示以训练检测器。 INTRODUCTION 在CNN的训练和测试中存在技术问题：普遍的CNN需要固定的输入图像大小（例如，224×224），其限制了输入图像的宽高比和比例。 Cropping Warping-&gt;unwanted geometric distortion(不需要的几何失真) 那么为什么CNN需要固定输入大小？ CNN主要由两部分组成：卷积层和跟随的完全连接的层。 事实上，卷积层不需要固定的图像大小，并且可以生成任何大小的特征图 另一方面，根据定义：完全连接的层需要具有固定尺寸/长度输入。所以固定尺寸完全来自于 全连接层 我们提出了一个spatial pyramid pooling（空间金字塔池化层）来去掉额昂罗固定输入的约束。 具体来说，我们在最后一个卷积层的顶部添加一个SPP层。 SPP层汇集特征并产生固定长度的输出，然后馈送到完全连接的层（或其他分类器）。 SPP对于深度CNN有着一些显著的特性： 1）SPP能够生成固定长度的输出，而不管输入大小，而在以前的深度网络[3]中使用的滑动窗口池不能; 2）SPP使用多级空间仓，而滑动窗口池仅使用单个窗口大小。 多层池化已被证明对于对象变形是鲁棒的[15]; 3）由于输入尺度的灵活性，SPP可以在可变尺度上提取的特征。 实验表明，这种多尺寸训练与传统的单尺寸训练一样收敛，并导致更好的测试精度。 SPP的优点是与特定的CNN设计是正交的。 Caltech101: L. Fei-Fei, R. Fergus, and P. Perona, “Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories,” CVIU, 2007. VOC 2007: M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results,” 2007. 但是R-CNN中的特征计算是耗时的，因为它对每个图像的数千个wraped区域的原始像素重复应用深卷积网络。而本文提出的方法可以在一整张图像上只跑一次卷积层 DEEP NETWORKS WITH SPATIAL PYRAMID POOLING 输入图像中的这些形状激活在相应位置的feature map The Spatial Pyramid Pooling Layer Bag-of-Words (BoW) approach-&gt;用来将生成的特征进行pool从而产生固定长度的向量。 空间金字塔池提高BoW，因为它可以通过在局部空间仓中汇集来 维护空间信息 。 “global pooling” operation a global average pooling a global average pooling Training the Network Single-size training Multi-size training SPP-NET FOR IMAGE CLASSIFICATIONSPP-NET FOR OBJECT DETECTION 对于R-CNN来说，Feature extraction is the major timing bottleneck in testing. 对于我们的SPP-net来说，我们从一整张图片中值提取一次特征。 On the contrary, our method enables feature extraction in arbitrary windows from the deep convolutional feature maps. Detection Algorithm]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>目标检测</tag>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习论文笔记：Rich feature hierarchies for accurate object detection and semantic segmentation]]></title>
    <url>%2Fblog%2F4241353321%2F</url>
    <content type="text"><![CDATA[Abstract mAP: mean average precision，平均准确度 我们的方法结合两个关键的见解： 第一：采用高容量的卷积神经网络来从上到下的进行region proposal，从而实现定位和分割物体。 当标记的训练数据稀缺时，可以先对辅助数据集（任务）进行受监督的预训练， 随后是基于域进行特定调整，产生显着的性能提升。 Introduction 关于各种视觉识别任务的上一个十年的进展主要基于SIFT和HOG的使用 实现这个结果需要解决两个问题： 利用深度网络将对象定位 仅利用少量的注释检测数据来训练训练高容量模型。 我们通过在“使用区域识别”范例内操作，来解决CNN定位问题 在测试时，我们的方法为输入图像生成大约2000个类别无关区域提案，使用CNN从每个proposal中提取固定长度的特征向量，然后使用类别特定的线性SVM对每个区域进行分类。 检测中面临的第二个挑战是标记的数据不足，目前可用的数据数量不足以训练大型CNN。这个问题的常规解决方案是使用无监督预训练，然后是监督 fine-tuning。 我们发现，对于CNN，有很大比例的参数（94%）可以在检测精度的适度降低的情况下被去除。 我们证明一个简单的 边界框回归方法（bounding box regression） 显着减少误定位，这是主要的误差模式(error mode)。 在开发技术细节之前，我们注意到，因为R-CNN在是区域上操作，所以很自然将其扩展到语义分割（semantic segmentation）的任务。 Object detection with R-CNN 我们的对象检测系统由三个模块组成: 首先生成类别独立(category-independent)区域proposal。 这些proposal定义了可用于检测器的候选检测集合。 第二个模块是大卷积神经网络，从每个区域提取固定长度的特征向量。 第三个模块是一类特定类型的线性SVM。 Module design Region proposals: 目前有很多用来生成category-independent的region proposal的方法： Objectness selective search category-independent object proposals constrained parametric min-cuts (CPMC) multi-scale combinatorial grouping detect mitotic cells by applying a CNN to regularly-spaced square crops, which are a special case of region proposals.(通过将CNN应用于规则间隔的方形作物来检测有丝分裂细胞，这是区域提案的特殊情况。) 虽然R-CNN与特定区域建议方法无关，但我们使用选择性搜索(selective search)来实现与先前检测工作的受控比较 Feature extraction:我们从每个区域提案中提取一个4096维特征向量，特征通过前向传播对227×227 RGB图像通过 五个卷积层和两个完全连接的层 计算。 无论候选区域的大小或宽高比如何，我们都会将其周围的紧密边界框中的所有像素装到所需的大小(227x227像素尺寸)。 Test-time detection 在测试时，我们对测试图像运行选择性搜索以提取大约2000个区域建议（我们在所有实验中使用选择性搜索的“快速模式（fast mode）”）。 给定图像中的所有得分区域，我们应用贪心非最大抑制(greedy non-maximum suppression)（对于每个类独立地），如果与的饭较高的区域有重叠，且IoU大于学习到的阈值，则该拒绝区域。 Run-time analysis.两个属性使检测更高校。 首先，所有CNN参数在所有类别中共享。 第二，CNN计算的特征向量与其他常见方法（例如具有视觉词袋编码的空间棱金字塔）相比是 低维的 。 唯一的类特定(class-specific)计算是特征和SVM权重之间的点积和非最大抑制。 Training 除了用随机初始化的21路分类层（对于20个VOC类加上背景）替换CNN的ImageNet特定的1000路分类层之外，CNN架构是不变的。 我们将所有region proposal与一个ground-truth重叠为IoU&gt;0.5，作为该框类的阳性，其余作为阴性。 我们以0.001的学习速率（初始预训练速率的1/10）开始SGD，这允许精细调整进行，而不是破坏初始化。 一旦提取特征并应用训练标签，我们对每个类优化一个线性SVM。 由于训练数据太大，无法记忆，我们采用标准 hard negative mining method 。 Results on PASCAL VOC 2010-12Visualization, ablation, and modes of errorVisualizing learned features pool-5，是网络第五个也是最后一个卷基层的max-pool层的输出。（是一个max-pooling层） The pool-5 feature map is 6 × 6 × 256 = 9216维。 忽略边界效应，每个pool-5单元在原始227×227像素输入中具有195×195像素的接收场。 Ablation studies Fc6与pool-5全连接，为了计算特征，他它将 4096×9216的权重矩阵乘以pool-5的feature map （重新形成为9216维矢量），然后添加偏差矢量。 Fc7是网络的最后一层，通过将由fc 6计算的特征乘以 4096×4096 权重矩阵，并类似地添加偏置矢量和应用半波整流来实现。 大多数CNN的表示能力来自它的卷积层，而不是来自大得多的密集连接的层。 All R-CNN variants strongly outperform the three DPM baselines Detection error analysisBounding box regressionSemantic segmentation full fg full+fg The fg strategy slightly outperforms full, indicating that the masked region shape provides a stronger signal, matching our intuition. Conclusion 之前最好的性能系统是将多个低级图像特征与来自对象检测器和场景分类器的高级上下文组合在一起的复杂集合。 本文提出了一个简单和可扩展的对象检测算法，与PASCAL VOC 2012上的最佳以前的结果相比提供30％的相对改进。 我们推测“supervised pre-training/domain-speciﬁc ﬁne-tuning”范例将对各种数据缺乏的视觉问题高度有效。]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>目标检测</tag>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行人检测论文笔记：Fused DNN - A deep neural network fusion approach to fast and robust pedestrian detection]]></title>
    <url>%2Fblog%2F2553947436%2F</url>
    <content type="text"><![CDATA[相关知识点 L1范数 也称为最小绝对偏差（LAD），最小绝对误差（LAE）。它基本上最小化目标值(Yi)和估计值(f(xi))之间的绝对差(S)的和 L2范数也称为最小二乘。它基本上最小化目标值(Yi)和估计值(f(xi))之间的差(S)的平方的和 Abstract 所提出的网络融合架构允许多个网络的并行处理来提高速度。 首先是一个深度卷积网络被训练为一个物体检测器来生成所有有可能的不同尺寸和遮挡的行人候选集。 然后，多个深度神经网络被并行使用来之后提炼这些行人候选集。 我们引入基于软拒绝的网络融合方法将来自所有网络的软度量融合在一起，以产生最终置信分数。 此外，我们提出了一种用于将逐像素语义分割网络（ pixel-wise semantic segmentation network）集成到网络融合架构中作为行人检测器的加强的方法。 Introduction Tradeoff between accuracy and speed. 其他因素，如拥挤的场景，非人堵塞物体(non-person occluding objects)或不同的行人外观（不同的姿势或服装风格）也使这个Real-time行人检测问题具有挑战性。 行人检测的一般框架可以分解为： region proposal generation, feature extraction, pedestrian verification Fused Deep Neural Network(F-DNN) 该架构包括行人pedestrian candidiate generator，其通过训练深卷积神经网络获得以，从而具有高检测率，虽然有大的假阳性率。 使用深度扩展卷积和上下文聚合的并行语义分割网络[30]为候选行人提供了另一个软的信任投票，它进一步与候选生成器和分类网络融合。 The Fused Deep Neural Network 提出的网络架构包括行人候选生成器，分类网络和像素级语义分割网络。 SSD: a single shot multi-box detector(单镜头多箱检测器)，行人候选生成器是一个single shot multi-box detector（SSD） 每个行人候选者与其定位BB坐标和置信度得分相关联。 我们提出了一种新的网络融合方法——称为基于软拒绝的网络融合（SNF）。并非是执行接受或拒绝候选者的硬二进制分类，而是基于来自分类器的候选者的 聚合度 来提升或折扣行人候选者的置信度分数。 我们进一步提出了一种利用具有语义分割（SS）的上下文聚集扩展卷积网络（context aggregation dilated convolutional network with semantic segmentation）作为另一个分类器并将其集成到我们的网络融合架构中的方法。但是在速度上会变得特别慢。 Pedestrian Candidate Generator SSD是具有截断VGG16(truncated VGG16)作为基础网络的前馈卷积网络。 SSD的结构： L2归一化技术用于缩小特征量 对于大小为m×n×p的每个输出层，在每个位置处设置不同尺度和纵横比的一组默认BB。 将3×3×p个卷积内核应用于每个位置以产生关于默认BB位置的分类分数和BB位置偏移。 训练的目标函数是： Classiﬁcation Network and Soft-rejection based DNN Fusion 分类网络由多个二元分类深层神经网络组成，这些网络在第一阶段的生成的行人候选集中训练。 SNF：考虑一个行人候选人和一个分类器。如果分类器对候选人有高的信任度，我们通过乘以大于1的置信因子乘以候选发生器来提高其原始分数。否则，我们以小于1的缩放因子减小其得分。我们将“置信”定义为至少为ac的分类概率。为了融合所有M个分类器，我们将候选者的原始信任得分与分类网络中所有分类器的信任缩放因子的乘积相乘。 SNF背后的关键思想是，我们不直接接受或拒绝任何候选行人，而是基于分类概率的因素来扩展它们。 Pixel-wise semantic segmentation for object detection reinforcement 为了执行密集预测，SS网络由完全卷积的VGG16网络组成，其适应于作为前端预测模块的扩展卷积，其输出被馈送到多尺度上下文聚合模块，该多尺度上下文聚合模块由完全卷积网络组成，其卷积层具有增加扩张因子。 输入图像被缩放并由SS网络直接处理，SS网络产生具有显示出行人类激活像素的一种颜色和显示出背景的其他颜色的二进遮罩。 我们使用以下策略来融合结果：如果行人像素占据候选BB区域的至少20％，我们接受候选者并保持其得分不变; 否则，我们应用SNF来缩放原始的信任分数。 Experiments and result analysisData and evaluation settingsTraining details and results 硬拒绝（Hard Rejection） 被定义为消除由任何分类器分类为假阳性的任何候选者。]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>目标检测</tag>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行人检测论文笔记：How Far are We from Solving Pedestrian Detection?]]></title>
    <url>%2Fblog%2F2397281138%2F</url>
    <content type="text"><![CDATA[文章疑问点 Human Baseline 的标准是如何确定的? Ground-truth是什么意思？ Groun-truth 指的是正确的标注（真实值） 在有监督学习中，数据是有标注的，以(x, t)的形式出现，其中x是输入数据，t是标注.正确的t标注是ground truth，错误的标记则不是。（也有人将所有标注数据都叫做ground truth）。 Intersection over Union（IoU）是什么？ Intersection over Union is an evaluation metric used to measure the accuracy of an object detector on a particular dataset. Any algorithm that provides predicted bounding boxes as output can be evaluated using IoU. As long as we have these two sets of bounding boxes we can apply Intersection over Union. An Intersection over Union score &gt; 0.5 is normally considered a “good” prediction. FPPI: False Positive Per Image Oracle Experiment: An oracle experiment is used to compare your actual system to how your system would behave if some component of it always did the right thing. Abstract 调查了当前最先进的方法与“完美单帧检测器”之间的差距。 基于Caltech数据集创建了一个人工的基准。 手工聚合了顶级检测器经常出现的错误。 刻画了定位，前景 vs 背景两方面的错误 针对定位错误：研究了训练集标记噪声对检测器性能的影响 前景 vs 背景错误：研究了convnets，讨论了哪些因素影响其性能 提供了一个新的、更纯净的训练/测试标注集。 Introduction 我们的结果显示localization是高置信度false positives的重要来源 PreliminariesCaltech-USA pedestrian detection benchmark 最流行的数据集：Caltech-USA、KITTI Caltech-USA有2.5小时、30Hz的从LA街道的一个check里面录制的 一共350000个标注、覆盖2300各单一的行人 测试集：4024帧 MR: miss rate Filtered channel features detector 截止到最近的主要会议（CVPR 15），最好的方法是 Checkerboards Checkerboards：是ICF的一种，ICF(Integral Channels Feature detector) 目前最好的执行convnets方法对底层检测建议很敏感，因此我们首先通过优化过滤的通道特征检测器来关注这些建议。 环境和光流可以提高检测（额外的提示） Analyzing the state of the artAre we reaching saturation? 在现在的基准上，我们还有多少提升空间？为了回答这个问题，我们提出可一个人工的基准线作为最低极限。 机器检测算法应该达到至少人类水平，最终超过人类水平。 人工基准线——为了公平比较，关注于单帧单目检测，注释器需要根据行人外表和单帧环境来注释。 Intersection over Union (IoU) ≥ 0.5 matching criterion。 在所有情况下人类基准线表现远远超过当前最好的检测器，说明对于自动方法来说，还有提升空间。 Failure analysisError sources 一个检测器可以有两类错误： 假阳性（检测到了背景，或者很弱的定位检测） 假阴性（低得分率或者错过某些行人检测，检测不全） FP聚类成11个分类 FN聚类成6个分类，其中side view 和 cyclists是由于数据集偏差导致的，用这些案例的外部图像增强训练集可能是一个有效的策略。 对于small pedestrains，发现低像素是主要困难来源，所以合理的利用所有像素，以及周围上下文是很必要的。 Oracle test cases 对于大多数执行最好的方法，localization和background-vs-forground误差对检测质量具有相等的影响。 他们同样重要。 Improved Caltech-USA annotations 原始注释是基于跨越多个帧内插稀疏注释（interpolating sparse annotations ），并且这些稀疏注释不一定位于评估的帧上。 我们的目标是两方面： 在一方面，我们希望提供对现有技术的更准确的评估，特别是适合于接近该问题的“最后20％”的评估。 另一方面，我们希望有训练注释，并评估改进的注释导怎么样更好的检测。 总之，我们的新注释与人类基线在以下方面不同：训练和测试集都被注释，忽略区域和闭塞也被注释，完整的视频数据用于决策，并且允许同一图像的多个修订。 Improving the state of the artImpact of training annotations Pruning benefits: 从原始到修剪注释的主要变化是删除注释错误，从修剪到新的，主要的变化是更好的对齐。 我们在MRN-2中看到，更强的检测器更好地受益于更好的数据，并且检测质量的最大增益来自移除注释错误。 Alignment benefits: 为了利用新的1×注释来利用9×剩余数据，我们在新的注释上训练模型，并使用该模型在9×部分上重新对准原始注释。 因为新的注释更好地对齐，所以我们期望该模型能够修复原始注释中的轻微位置和缩放错误。 结果表明，使用检测器模型来提高整体数据对准确实是有效的，并且更好地对准训练数据导致更好的检测质量（在MRO和MRN中）。 使用高质量注释进行训练可提高整体检测质量，这得益于改进的对齐和减少的注释错误。 Convnets for pedestrian detection AlexNet 和 VGG16都在ImageNet上进行了预先训练，并使用SquaresChnFtrs建议对Caltech 10×（原始注释）进行了微调。 可以看出，VGG显着地减少了背景误差，而同时稍微增加了定位误差。 虽然卷积在图像分类和一般物体检测中具有很强的结果，但是当在小物体周围产生良好的局部检测分数时，它们似乎有局限性。 边界框回归（和NMS）是当前架构的一个关键因素。 表明神经网络的原始分类能力仍有改进的余地。 Summary 相对于human baseline, there is a 10× gap still to be closed. 误差特性导致关于如何设计更好的检测器（在3.2节中提及;例如，对于人side-view的数据增加或在垂直轴上延伸检测器接收场）的具体建议。 我们通过衡量更好的注释对本地化准确性的影响，以及通过调查使用convnets来改善the background to foreground discrimination，来部分解决了一些问题。我们的研究结果表明，通过适当训练的ICF检测器可以实现显着更好的Alignment，并且，对于行人检测，Convent在localization上能力不强，但是可以通过边界框回归（bounding box regression）部分解决。 对于原始和新注释，所描述的检测方法都能达到最高性能。]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Pedestrian Detection</tag>
        <tag>行人检测</tag>
        <tag>Review</tag>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行人检测论文笔记：Robust Real-Time Face Detection]]></title>
    <url>%2Fblog%2F2903903730%2F</url>
    <content type="text"><![CDATA[知识点 傅里叶变换的一个推论： 一个时域下的复杂信号函数可以分解成多个简单信号函数的和，然后对各个子信号函数做傅里叶变换并再次求和，就求出了原信号的傅里叶变换。 卷积定理(Convolution Theorem)：信号f和信号g的卷积的傅里叶变换，等于f、g各自的傅里叶变换的积 整个过程的核心就是“（反转），移动，乘积，求和” 二维卷积 数学定义 二维卷积在图像处理中会经常遇到，图像处理中用到的大多是二维卷积的离散形式： 图像处理中的二维卷积，二维卷积就是一维卷积的扩展，原理差不多。核心还是（反转），移动，乘积，求和。这里二维的反转就是将卷积核沿反对角线翻转，比如：之后，卷积核在二维平面上平移，并且卷积核的每个元素与被卷积图像对应位置相乘，再求和。通过卷积核的不断移动，我们就有了一个新的图像， 这个图像完全由卷积核在各个位置时的乘积求和的结果组成。 巴拿赫空间：更精确地说，巴拿赫空间是一个具有范数并对此范数完备的向量空间。 许多在数学分析中学到的无限维函数空间都是巴拿赫空间。 巴拿赫空间有两种常见的类型：“实巴拿赫空间”及“复巴拿赫空间”，分别是指将巴拿赫空间的向量空间定义于由实数或复数组成的域之上。 Overcomplete： 对于Banach space X中的一个子集，如果X中的每一个元素都可以利用子集中的元素在范数内进行有限线性组合来良好近似，则该系统X是完备Complete的。 该完备系统是过完备（Overcomplete）的，如果从子集中移去一个元素，该系统依旧是完备的，则该系统称为过完备的。 在不同的研究中，比如信号处理和功能近似，过完备可以帮助研究人员达到一个更稳定、更健壮，或者相比于使用基向量更紧凑的分解。 如果 # (basis vector基向量)&gt;输入的维度，则我们有一个overcomplete representation. ROC曲线：在信号检测理论中，接收者操作特征曲线（receiver operating characteristic curve，或者叫ROC曲线）是一种坐标图式的分析工具，用于 (1) 选择最佳的信号侦测模型、舍弃次佳的模型。 (2) 在同一模型中设定最佳阈值。 从 (0, 0) 到 (1,1) 的对角线将ROC空间划分为左上／右下两个区域，在这条线的 以上的点 代表了一个 好 的分类结果（胜过随机分类），而在这条线 以下的点 代表了 差 的分类结果（劣于随机分类）。 完美的预测是一个在左上角的点. 曲线下面积（AUC）：ROC曲线下方的面积，若随机抽取一个阳性样本和一个阴性样本，分类器正确判断阳性样本的值高于阴性样本之机率=AUC。简单说：AUC值越大的分类器，正确率越高。 Abstract 介绍一个脸部检测框架。 三个贡献： 引入新图像表示——称为“积分图像”，其允许我们的检测器非常快速地计算所使用的特征。 提出一个利用AdaBost学习算法构建的简单有效的分类器，来从极大潜在特征集中选出很少的关键视觉特征。 在级联中组合分类器，从而快速丢弃图像的背景区域，同时在有可能的面部区域上花费更多的计算。 Introduction Haar Basis 函数： Integral image: 类似于计算机图形学中利用求和区域表来进行纹理映射。 Haar-like features：就是mount两个或多个区域的像素值之和的差值。 AdaBoost：自适应增强， 具体说来，整个Adaboost 迭代算法就3步： 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。 训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。 级联检测过程的结构基本上是简并决策树的结构 Features 基于特征的系统操作肯定比一个基于像素的系统更更快 （Two-rectangle feature）两矩形特征的值是两个矩形区域内的像素之和的差 (Three-rectangle feature)三矩形特征计算从中心矩形中的和减去的两个外部矩形的和。 (Four-rectangle feature)四矩形特征计算矩形对角线对之间的差异。 矩阵特征=从灰色矩形中的像素的和中减去位于白色矩形内的像素的和。 Integral Image 矩阵特征可以通过图像的中间表示来快速计算，从而成为Integral Image. 积分图的每一点（x, y）的值是原图中对应位置的左上角区域的所有值得和。 积分图每一点的（x, y）值是： 位置x，y处的积分图像包含x，y（包括端点）上方和左侧的像素的和: ii(x, y) is the integral image i(x, y) is the original image s（x，y）是累积行和 s(x, −1) = 0, ii(−1, y) = 0) 积分图可以只遍历一次图像即可有效的计算出来 使用积分图像，可以在四个阵列参考中计算任何矩形和。 Two-rectangle feature：需要6个阵列参考 Three-rectangle feature：需要8个阵列参考 Four-rectangle feature：需要9个阵列参考 在线性运算（例如f.g）的情况下，如果其逆被应用于结果，则任何可逆线性算子可以应用于f或g。 例如在卷积的情况下，如果导数运算符被应用于图像和卷积核，则结果必须被双重积分. 如果f和g的导数稀疏（或可以这样做），卷积可以显着加速。 类似的一个认识是：如果其逆被应用于g，则一个可逆线性算子可以应用于f。 在该框架中观察，矩形和的计算可以表示为点积i·r，其中i是图像，r是box car图像（在感兴趣的矩形内的值为1，外面是0）。 此操作可以重写： 积分图像实际上是图像的二重积分（首先沿行，然后沿列）。 矩形的二阶导数（第一行在行中，然后在列中）在矩形的角处产生四个delta函数。 第二点积的评估通过四个阵列访问来完成。 Feature Discussion 与可操纵滤波器（Steerable filters）等替代方案相比，矩形特性有点原始。 可控滤波器对边界的详细分析，图像压缩和纹理分析的非常有用。 由于正交性不是这个特征集的中心，我们选择生成一个非常大而且各种各样的矩形特征集。 从经验上看，似乎矩形特征集提供了丰富的图像表示，能支持有效的学习。 为了利用积分图像技术的计算有事，考虑用更常规的方法去计算图像金字塔。 像大多数面部检测系统一样，我们的检测器在许多尺度扫描输入; 从以尺寸为24×24像素检测面部的基本刻度开始，在12个刻度以大于上一个的1.25倍的因子扫描384×288像素的图像。 Learning Classification Functions 给定检测器的基本分辨率是24×24，矩形特征的穷尽集是相当大的，160000. 我们的假设是，由实验证明，非常少数的矩形特征可以组合形成一个有效的分类器。 主要的挑战是找到这些功能。 Adaboost：将多个弱分类器组合成一个强分类器。（一个简单学习算法叫做weak learner）。 传统的的AdaBoost过程可以容易地解释为贪心特征选择过程。 一个 挑战 是将大的权重与每个良好的分类函数相关联，并将较小的权重与较差的函数相关联。 AdaBoost是一个用于搜索少数具有显着品种的良好“特征”的有效程序。 将一个weak learn限制到分类函数几何中，每一个函数都只依赖于一个单一的特征。 若学习宣发选择单一的能够最好分开正和负样本的矩形特征。 对于每一个特征，weak learner决定最优分类函数阈值，从而可以使得最少数目的样本被错分。 一个弱分类器h(x, f, p, θ)因此包含一个特征f，一个阈值θ，一个显示不等式方向的极性p： 这里x是一个图片24*24像素的子窗口。 我们使用的弱分类器（阈值单一特征）可以被视为单节点决策树。 Boosting 算法 ：T是利用每个单个特征构造的假设，最终假设是T个假设的加权线性组合，其中权重与训练误差成反比。 给定样本图片(x1, y1), (x2, y2), …, (xn, yn)。其中yi=0, 1分别为负样本和正样本。 初始化权值w1, i=1/(2m), 1/(2l)分别当yi=0, 1。其中m和l分别是负样本和正样本的数量。 For t=1, …, T: 归一化权重, 根据加权错误选择最佳弱分类器： 定义 ht(x) = h(x, ft, pt,θt) 其中ft, pt, 和 θt 是εt的最小值. 更新权值：其中ei=0当样例xi被正确的分类，否则ei=1，并且 最后的强分类器是： 其中 Learning Discussion 弱分类器选择算法过程如下： 对于每个特征，根据特征值对样例进行排序。 该特征的AdaBoost最佳阈值可以在该排序列表上的单次通过中计算。 对于排序列表中的每个元素，四个和被维护和评估： 正实例权重T+的总和。 负实例权重T-的总和。 当前示例S+之下的正权重的和。 当前示例S-之下的负权重的和。 在排序一个划分当前和上一示例之间的范围的阈值的错误是： Learning Results 在现实应用中，假正例率必须接近1/1000000。 所选择的 第一特征 似乎集中于属性即眼睛的区域通常比鼻子和脸颊的区域更暗。 所选择的 第二特征 依赖于眼睛比鼻梁更暗的特性。 提高性能最直接技术是添加更多的特征，但这样直接导致计算时间的增加。 Receiver operating characteristic (ROC)曲线： The Attentional Cascade 本节描述了用于构造级联的分类器的算法，其实现了提高的检测性能，同时从根本上减少了计算时间。 阈值越低，检测率越高，假正例率越高。 从双特征强分类器开始，可以通过 调整强分类器阈值 以最小化假阴性来获得有效的面部滤波器。 可以调整双特征分类器以50％的假阳性率来检测100％的面部。 整体的检测过程形式是简并决策树的形式，我们称之为“级联”。 在任何点上的否定结果立即导致对该子窗口的拒绝。 更深的分类器面临的更困难的例子,将整个ROC曲线向下推。 在给定的检测率下，较深的分类器具有相应较高的假阳性率。 Training a Cascade of Classifiers Given a trained cascade of classifiers, the false positive rate of the cascade is： The detection rate is: The expected number of features which are evaluated is: 用于训练后续层的负样例集合是通过运行检测器收集通过在不包含任何面部实例的一组图像上而找到的所有错误检测来获得。 构建一个练级检测器的训练算法： Simple ExperimentDetector Cascade Discussion 将检测器训练为分类器序列的隐藏好处是:最终检测器看到的有效数目的负样例数目可能非常大。 在实践中，由于我们的检测器的形式和它使用的特性是非常高效的，所以在每个尺度和位置评估我们的检测器的 摊销成本 比在整个图像中找到并分组边缘更快。 ResultsTraining Dataset 事实上，包含在较大子窗口中的附加信息可以用于在检测级联中较早地拒绝non-face。 Structure of the Detector Cascade 最终的检测器是38层分级器，包括总共6060个特征。 级联中的第一个分类器是使用两个特征构造的，在检测100%的面部时可以拒绝50%的non-faces. 下一个分类器具有十个特征，并且在检测几乎100％的面部时拒绝80％的非面部。 接下来的两层是25个特征分类器，其后是三个50特征分类器，再之后是具有根据表2中的算法选择的各种不同数目的特征的分类器。 添加更多层，直到验证集上的假阳性率接近零，同时仍保持高的正确检测率。 Speed of the Final Detector 级联检测器的速度直接与每个被扫描的子窗口的特征数量相关。 Image Processing 用于训练的所有示例子窗口被 方差归一 化以使不同照明条件的影响最小化。 可以使用 一对积分图像 来快速计算图像子窗口的方差。 在扫描期间，可以通过对特征值进行后乘，而不是对像素进行操作来实现图像归一化的效果。]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行人检测论文笔记：Ten Years of Pedestrian Detection, What Have We Learned?]]></title>
    <url>%2Fblog%2F287090227%2F</url>
    <content type="text"><![CDATA[Abstract 这种新的决策林探测器在挑战性的Caltech-USA数据集上实现了当前最好的已知性能。 Introduction 更重要的是，这是一个有着已建立的基准和评估指标的良好定义的问题。 用于对象检测的的主要范例有——”Viola＆Jones变体“，HOG + SVM模板，可变形部分检测器（DPM）和卷积神经网络（ConvNets）都已经被探索用于此任务。 Datasets INRIA, ETH, TUD-Brussels, Daimler, Caltech-USA, and KITTI是使用最广的数据集。 INRIA：INRIA是最古老的，因此具有相对较少的图像。 然而，从不同设置（城市，海滩，山脉等）的行人的高质量注释，这是为什么它被普遍选择用来训练。 Daimler没有被所有的方法考虑，因为它缺乏颜色通道。 Daimler stereo，ETH和KITTI提供立体声信息。 所有数据集但INRIA都是从视频获取的，因此可以使用光流作为附加提示。 今天，Caltech-USA和KITTY是行人检测的主要基准。 两者都相对较大和具有挑战性。 Main approaches to improve pedestrian detection 40+种行人检测的方法： 我们不是讨论方法的个体特性，而是识别区分每种方法（表1的对号）的关键方面，并对其进行分组。 我们在下面的小节讨论这些方面。 Training dataSolution families 总体上，我们注意到在40多种方法中，我们可以辨别三个家庭： DPM变体（MultiResC [33]，MT-DPM [39]等） 深度网络（JointDeep [40]，ConvNet [13] ]等） 决策林（ChnFtrs，Roerei等）。 在表1中，我们将这些家族分别识别为DPM，DN和DF。 Better classiﬁers 特征和分类器之间的没有明确的界限 Additional data 一些方法探索在训练和测试时间利用额外的信息来改进检测。 他们考虑立体图像[45]，光流（使用以前的帧，例如MultiFtr + Motion [22]和ACF + SDt [42]），跟踪[46]或来自其他传感器 。 到目前为止，仅基于单个单目图像帧的方法已经能够跟上由附加信息引入的性能改进。 Exploiting context 上下文为行人检测提供了一致的改进，虽然改进的规模比额外的测试数据（§3.4）和深层架构（§3.8）要低。 大部分检测质量必须来自其他来源。 Deformable parts 对于行人检测，结果是有竞争性的，但不显着。 对于行人检测，除了遮挡处理的情况之外，仍然没有关于部件和部件的必要性的明确证据。 Multi-scale models 最近已经注意到，不同分辨率的训练不同模型系统地将性能提高1〜2MR百分点 尽管不断改进，他们对最终质量的贡献是相当小的。 Deep architectures 尽管有共同的叙述，仍然没有明确的证据表明深层网络有利于行人检测的学习功能 最成功的方法使用这样的架构来模拟部件，遮挡和上下文的更高级别方面。 获得的结果与DPM和决策林方法相同，使得使用这样涉及的结构的 优点仍不清楚 。 Better features 特征更多，具有更丰富和更高维的表示，分类任务变得更容易，从而改善结果。 越来越多样化的特性已经显示系统地提高性能。 尽管通过添加许多渠道的改进，顶级性能检测器仍然达到仅有10个通道： 6个梯度方向， 1个梯度幅度 3个颜色通道 我们命名这些 HOG + LUV 。 应当注意，还没有更好的用于行人检测的特征可以通过深度学习方法获得。 下一个科学的步骤将是开发一个更深刻的理解，什么使好的功能更哈珀，以及如何设计更好的特征。 Experiments 基于我们在上一节中的分析，在对检测质量的影响方面，三个方面似乎是最有希望的： 更好的特征（§3.9 附加数据（§3.4） 上下文信息（§3.5） Reviewing the eﬀect of features(特征的影响) DCT: (discrete cosine transform)离散余弦变换 自VJ以来的许多进展可以通过使用基于定向梯度和颜色信息的更好的特征来解释。 对这些众所周知的特征（例如，基于DCT的投影）的简单调整仍然可以产生显着的改进。 Complementarity of approaches 在重新审视4.1节中单帧特征的影响之后，我们现在考虑更好的特征（HOG + LUV + DCT），附加数据（通过光流）和上下文（通过人对人的交互）的互补。 我们的实验表明，即使从强检测器开始，添加额外的特征，流量和上下文信息在很大程度上是互补的（增加12％，而不是3 + 7 + 5％）。 Conclusion 虽然这些功能中的一些可能是由学习驱动的，但它们主要是通过尝试和错误手工制作的。 Better features + optical flow + context的结合可以在Caltech-USA上产生最好的检测性能。 The main challenge ahead seems to develop a deeper understanding of what makes good features good, so as to enable the design of even better ones.]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Pedestrian Detection</tag>
        <tag>行人检测</tag>
        <tag>Review</tag>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习读书笔记：DeepLearningBook - Chapter 9 - Conventional Networks]]></title>
    <url>%2Fblog%2F783616645%2F</url>
    <content type="text"><![CDATA[Chapter 9 Convolutional Networks（卷积神经网络） 卷积网络仅仅是在其至少一个层中使用卷积代替一般矩阵乘法的神经网络。 The Convolution Operation The convolution operation is typically denoted with an asterisk: 在卷积网络术语中，卷积的第一个参数（在本例中为函数x）通常称为 输入 ，第二个参数（在本例中为函数w）作为 内核 。 输出 有时称为 特征映射(feature map) 。 在机器学习应用中， 输入 通常是多维数据数组，并且 内核 通常是由学习算法调整的多维参数数组。 我们将这些多维数组称为 张量（tensors） 。 这意味着在实践中，我们可以实现无限求和作为对有限数量的数组元素的求和。 二维卷积：two-dimensional kernel K 卷积是可交换的，这意味着我们可以等价地写，但这样会带来 kernel-ﬂipping后一种会更更容易用机器学习库来实现。 许多神经网络库实现了一个称为互相关（cross-correlation）的相关函数，它与卷积相同，但是没有翻转（flipping）内核： 卷积的一个例子： 离散卷积可以被看作是乘以矩阵的乘法。 Toeplitz矩阵：常对角矩阵（又称特普利茨矩阵）是指每条左上至右下的对角线均为常数的矩阵，不论是正方形或长方形的。 在二维中，双块循环矩阵（doubly block circulant matrix）对应于卷积。 卷积通常对应于非常稀疏的矩阵。 任何与矩阵乘法一起作用并且不依赖于矩阵结构的特定属性的神经网络算法都应该与卷积一起工作，这样就不需要对神经网络的任何进一步的改变。 Motivation 卷积利用三个重要的想法，可以帮助改进机器学习系统: 稀疏的连接（sparse interactions） 参数共享（parameter sharing） 等值表示（equivariant representations） 此外卷积可以处理各种大小输入。 Sparse Interactions：这是通过使内核小于输入来实现的。 我们需要存储更少的参数，这既减少了模型的内存需求，又提高了其统计效率。 计算输出需要更少的操作。 在深卷积网络中，较深层中的单元可以与输入的较大部分间接交互，This allows the network to eﬃciently describe complicated interactions between many variables by constructing such interactions from simple building blocks that each describe only sparse interactions. Sparse connectivity, viewed from below Sparse connectivity, viewed from above 在卷积网络的较深层中的单元的接收场大于在浅层中的单元的接收场。这意味着即使卷积网络中的直接连接非常稀疏，更深层中的单元也可以间接地连接到所有或大部分输入图像。 Parameter sharing：参数共享是指对模型中的多个函数使用相同的参数。 也可以叫 Tied Weights（捆绑权值），因为应用于一个输入的权重的值与在其他地方应用的权重的值有关。 在卷积神经网络中，卷积核中的每一个元素都会在input中的每一个位置使用。 在存储器要求和统计效率方面，卷积比密集矩阵乘法显着更有效。 黑色箭头表示在卷积模型中3元素核的中心元素的使用。 Equivariant：说一个函数是等变的意味着如果输入改变，输出以相同的方式改变。 一个函数f(x)与函数g 等变 如果f(g(x)) = g(f(x)). 当处理时间序列数据时，这意味着卷积产生一种时间线，显示输入中不同特征的出现。 This is useful for when we know that some function of a small number of neighboring pixels is useful when applied to multiple input locations. 卷积不是自然地等同于一些其他变换，例如图像的尺度或旋转的变化。 卷积是描述在整个输入上应用小的局部区域的相同线性变换的变换的非常有效的方式。 Pooling 卷积网络的一个典型层由三个阶段组成: 在第一阶段，该层并行执行几个卷积以产生一组线性激活（linear activation）。 在第二阶段，每个线性激活通过非线性激活函数，例如整流线性激活函数。这一阶段成为 detector stage. 在第三阶段，我们使用池化函数（pooing function）来进一步修改层的输出。 pooling function是用附近的汇总统计来替换特定位置的网络的输出。 在所有情况下，池化有助于使表示变得对于相对于 输入的小平移 几乎不变（invariant）。 如果我们更关心某些特征是否存在而不是完全在哪里，那么本地变换的不变性可以是非常有用的属性。 池的使用可以被视为添加一个无限强的先验，层所学习的函数必须对小的变换是不变的。 如果我们在单独的参数化的卷积输出上进行赤化，则特征可以学习到对哪一种变换进行不变。 利用卷积和pooling的卷积网络架构： Convolution and Pooling as an Inﬁnitely Strong Prior 弱先验是具有高熵的先验分布，例如具有高方差的高斯分布。 强先验具有非常低的熵，例如具有低方差的高斯分布。这样的先验在确定参数在哪里结束方面起更积极的作用。 总的来说，我们可以认为卷积可以用来为一个层的参数引入一个无限强的先验概率分布。 同样，池的使用是保证每个单位对小的变换保持不变性的无限强的先验。 但是将卷积网视为具有无限强的先验的完全连接的网络可以给我们一些关于卷积网如何工作的见解。 一个关键的见解是卷积和池化可能导致欠拟合。 从这个观点的另一个关键的见解是，我们应该只比较卷积模型与统计学习性能基准中的其他卷积模型。对于许多图像数据集，对于排列不变的模型存在单独的基准，并且必须通过学习发现拓扑的概念，以及具有由他们的设计者硬编码到其中的空间关系的知识的模型。 Variants of the Basic Convolution Function 首先，当我们在神经网络的上下文中提到卷积时，我们通常实际上意味着一种由许多卷积应用并行组成的操作。 这是因为单个内核的卷积只能提取一种特征，虽然在许多空间位置。 通常我们希望我们网络的每一层都能在许多位置提取多种特征。 此外，输入通常不仅仅是是一个网格的真是数据，反而是矢量值的观测网格。 这些多通道操作可交换，仅当每个操作具有与输入通道相同数量的输出通道。 我们可能想跳过内核的一些位置，以减少计算成本，我们可以认为这是下采样全卷积函数的输出。 如果我们只想对输出中每个方向的每s个像素进行采样，那么我们可以定义下采样卷积函数c：我们将s称为这个下采样卷积的步幅。 也可以为每个运动方向定义一个单独的步幅。 零填充： 任何卷积网络实现的一个基本特征是具备隐含地对输入V进行零填充以便使其更宽的能力。 零填充输入允许我们独立地控制内核宽度和输出的大小。 没有零填充，我们被迫选择快速缩小网络的空间范围并且使用小内核 - 这两个方案，显着地限制了网络的表达力。 三种zero-padding的情况： valid convolution same convolution full convolution Unshared convolution Tiled convolution：在卷积层和本地连接层之间提供了折中。我们不是在每个空间位置学习一组独立的权重，而是学习一组内核，从而我们在空间移动时旋转内核。 这三个操作做够计算所有训练一个前向卷积网络需要的梯度，以及基于卷积的转置来训练具有重建函数的卷积网络。 卷积 从输出到权值反向传播 从输出到输入反向传播 Structured Outputs 卷积网络可以用于输出高维度的结构化对象，而不仅仅是预测分类任务的类标签或回归任务的实际值。 Pixel labeling：像素标记 循环周期卷积网络（recurrent convolutional network）： Data Types 与卷积网络一起使用的数据通常由几个通道组成，每个通道是在空间或时间的某个点观察不同的量。 卷积网络的一个优点是它们还可以处理具有变化的空间范围的输入。 有时，网络的输出允许具有 可变大小以及输入 ，例如如果我们想要为输入的每个像素分配类标签。 在这种情况下，不需要额外的设计工作了 。 在其他情况下，网络必须产生一些固定大小的输出，例如，如果我们要为整个图像分配单个类标签。 在这种情况下，我们必须进行一些额外的设计步骤 ，例如插入一个池化层，其池区域的大小与输入的大小成比例，以 保持固定数量 的池化输出。 Eﬃcient Convolution Algorithms 现代卷积网络应用通常涉及包含超过一百万个单元的网络。 卷积等效于使用傅立叶变换将输入和内核两者转换到频域，执行两个信号的逐点乘法，并使用逆傅里叶变换转换回到时域。 当内核是可分离的，原始的卷积是效率低下的。 设计更快的执行卷积或近似卷积的方法，而不损害模型的准确性是一个活跃的研究领域。 Random or Unsupervised Features 通常，卷积网络训练中最昂贵的部分是学习特征。 降低卷积网络训练成本的一种方式是使用未以受监督方式训练的特征。 有三种不需要监督学习来获得卷积内核的策略： 一个是简单地 随机初始化 它们。 另一个是用手设计它们，例如通过设置每个内核以在特定方向或尺度检测边缘。 最后，可以使用无监督标准来学习内核。 随机滤波器在卷积网络中通常工作得很好 一个折中的方法是学习特征，但使用： 每个梯度步骤不需要完全正向和反向传播的 方法。 与多层感知器一样，我们使用 贪婪层式预训练 ，独立地训练第一层，然后从第一层提取一次所有特征，然后利用这些特征隔离的训练第二层，等等。 不是一次训练整个卷积层，我们可以训练一个小补丁的模型，如用k-means。 然后，我们可以使用来自这个patch-based的模型的参数来定义卷积层的内核。 今天，大多数卷积网络以纯粹监督的方式训练，在每次训练迭代中使用通过整个网络的完全正向和反向传播。 The Neuroscientiﬁc Basis for Convolutional NetworksConvolutional Networks and the History of Deep Learning 为了处理一维，顺序数据，我们接下来转向神经网络框架的另一个强大的专业化： 循环神经网络（Recurrent neural networks） 。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行人检测论文笔记：Histograms of Oriented Gradients for Human Detection]]></title>
    <url>%2Fblog%2F3084343215%2F</url>
    <content type="text"><![CDATA[相关知识点 从TP、FP、TN、FN到ROC曲线、miss rate TP：true positive，实际是正例，预测为正例 FP：false positive，实际为负例，预测为正例 TN：true negative，实际为负例，预测为负例 FN：false negative，实际为正例，预测为负例 fnr+tpr=1, fpr+tnr=1 miss rate = FNR = 1 - true positive 对于一个确定的阈值t，FPR和TPR是确定的，得到一个(fpr,tpr)元组。 当t增加， # FP也减小， # TN增加，则fpr减小； 当t增加， # TP减小， # FN增加，则tpr减小。 也就是说，当阈值t从0变化到1，fpr和tpr也单调减小，从(1，1)减小到(0,0) miss rate = 1 - true positive rate，那么对应的YoX图像，也就是miss rate - false positive rate图像，就应当是单调下降的曲线。 Abstract 定向梯度直方图（HOG）描述符的网格显著优于现有的人体检测特征集。 在重叠描述符块中的 精细尺度梯度 ， 精细定向分箱 ， 相对粗略的空间分箱 和 高质量局部对比度标准化 对于良好的结果都是重要的。 新的数据集 Introduction 第一需求：robust feature set. 我们研究了人类监测的特征集问题，发现 本地归一化的定向梯度直方图（HOG） 描述符提供优异的性能相对于其他现有特征集包括小波。 提出的描述符让人联想到 边缘方向直方图 [4,5]， SIFT描述符 [12]和 形状上下文 [1]，但与它们的不同点是：HOG描述器是在一个网格密集的大小统一的细胞单元（dense grid of uniformly spaced cells）上计算，而且为了提高性能，还采用了重叠的局部对比度归一化（overlapping local contrast normalization）技术。 Previous WorkOverview of the Method The method is based on evaluating well-normalized local histograms of image gradient orientations in a dense grid. 基本思想是，在一副图像中，局部目标的 表象 和 形状 （appearance and shape）能够被梯度或边缘的方向密度分布很好地描述，即使没有对应的梯度或边缘位置的精确知识。 具体的实现方法是：首先将图像分成小的连通区域，我们把它叫细胞单元（cell）。然后采集细胞单元中各像素点的梯度的或边缘的方向直方图。最后把这些直方图组合起来就可以构成特征描述器。 为了提高性能，我们还可以把这些局部直方图在图像的更大的范围内（我们把它叫区间或block）进行对比度归一化（contrast-normalized），所采用的方法是：先计算各直方图在这个区间（block）中的密度，然后根据这个密度对区间中的各个细胞单元做归一化。 通过这个归一化后，能对光照变化和阴影获得更好的效果。 整体的物体检测链： 这些基于稀疏特征的表示的成功有点遮蔽了HOG作为密集图像描述符的能力和简单性。 HOG/SIFT表示方法有几个优点。 由于HOG方法是在图像的局部细胞单元上操作，所以它对图像几何的（geometric）和光学的（photometric）形变都能保持很好的不变性，这两种形变只会出现在更大的空间领域上。 他捕捉了局部形状非常具有特征性的边和梯度特征。 在局部表示中对局部的几何和光度变换的不变性更容易控制。 如果它们远小于局部空间或方向仓尺寸，则平移或旋转几乎没有差别。 作者通过实验发现，在粗的空域抽样（coarse spatial sampling）、精细的方向抽样（fine orientation sampling）以及较强的局部光学归一化（strong local photometric normalization）等条件下，只要行人大体上能够保持直立的姿势，就容许行人有一些细微的肢体动作，这些细微的动作可以被忽略而不影响检测效果。综上所述，HOG方法是特别适合于做图像中的行人检测的。 Data Sets and Methodology hard examples Detection Error Tradeoff (DET) curves on a log-log scale. miss rate(1-Recall / FN/(TP+FN)) verses FPPW. 值越低越好。 DET图和ROC图提供的信息一眼，但是前者允许小概率更容易的去分布。 FPPW：NUMBER_OF_FALSE_POSITIVE/NUMBER_OF_WINDOWS 我们的DET曲线通常相当浅，所以即使非常小的缺失率的改善也等同于在不变缺失率下的情况下FPPW中的大增益。 Overview of Results Generalized Haar Wavelets. PCA-SIFT. Shape Contexts. Implementation and Performance Study 默认检测器： RGB colour space with no gamma correction [−1, 0, 1] gradient filter with no smoothing linear gradient voting into 9 orientation bins in 0◦ –180◦ 16×16 pixel blocks of four 8×8 pixel cells Gaussian spatial win- dow with σ = 8 pixel L2-Hys (Lowe-style clipped L2 norm) block normalization block spacing stride of 8 pixels (hence 4-fold coverage of each cell) 64×128 detection window; linear SVM classifier. 主要的结论是，为了良好的性能，应该使用细尺度导数（基本上没有平滑），许多定向仓,中等大小，强归一化，重叠的描述符块。 Gamma/Colour NormalizationGradient Computation 最通常用的方法就是简单的应用一个一维的离散的梯度模版分别应用在水平和垂直方向上去。可以使用如下的卷积核进行卷积： Spatial / Orientation Binning(方向单元划分) 每个块内的每个像素对 方向直方图 进行投票 每个像素基于以其为中心的梯度元素的方向计算边缘取向直方图通道的 加权投票，并且投票被累积到在称为 单元 的局部空间区域上的 方向仓 中。 每个块的形状可以是矩形或圆形的 方向直方图的方向取值可以是0-180度或者0-360度，这取决于梯度是否有符号。无符号梯度（0-180º），有符号梯度（0-360º） 为了减少混叠，投票在相邻仓中心之间以取向和位置双向内插。 至于投票的权重，可以是梯度的幅度本身或者是它的函数。投票是像素处的梯度幅度的函数，或者是幅度本身、其平方、其平方根或者表示像素的边缘的软出现/缺失的幅度的限幅形式。在定向编码对于良好的性能是至关重要的。 梯度幅度本身通常产生最好的结果。其它可选的方案是采用幅度的平方或开方，或者幅度的裁剪版本。 Dalal和Triggs发现在人的检测实验中，把方向分为 9个通道(bin) 效果最好 Normalization and Descriptor Blocks 由于照明的局部变化和前景-背景对比度，梯度强度在可以在很宽范围内变化。所以梯度强度必须要局部地归一化，这需要把方格(cells)集结成更大、在空间上连结的区() 有效的局部对比度归一化 对于良好的性能是必不可少的。 我们评估了多种不同的 归一化schemes(normalization schemes) ，他们大多数都是基于将单元格（cells）分组成更大的空间块（spatial blocks） 并且对比地单独对每个块进行归一化。 最终描述符 是来自检测窗口中的所有块的归一化单元响应的所有分量的 向量 。 R-HOG：R-HOG块和SIFT描述符有许多相似之处，但是他们用途十分不同。 R-HOG跟SIFT描述器看起来很相似，但他们的不同之处是： R-HOG是在单一尺度下、密集的网格内、没有对方向排序的情况下被计算出来； 而SIFT描述器是在多尺度下、稀疏的图像关键点上、对方向排序的情况下被计算出来。 R-HOG是各区间被组合起来用于对空域信息进行编码，而SIFT的各描述器是单独使用的。 它们在密集网格中以单个尺度计算而没有主要取向对准，并且用作隐式地去编码相对于检测窗口的空间位置的较大代码矢量的一部分，而SIFT在稀疏集合的 尺度不变关键点处 被计算，旋转以对准它们的主导方向，并单独使用。 SIFT被优化用于稀疏宽基线匹配，R-HOG用于空间形式的密集鲁棒编码。 R-HOG区块一般来说是多个方格子组成的，由三个参数表示： 每个区块(block)有多少方格(cell)、 每个方格(cell)有几个像素(pixel)、 每个方格(cell)直方图有多少频道(bin)。 对于人体检测，3x3的单元块，6x6的像素单元块儿表现最好，同时直方图是9通道。 当其太小（1×1单元块，即，单独取向上的归一化）时，有价值的空间信息被抑制。 在对直方图做处理之前，给每个区间加一个高斯空域窗口是非常必要的，因为这样可以降低边缘的周围像素点的权重。 C-HOG 每个空间单元包含梯度加权取向单元的堆叠而不是单个取向无关的边缘计数。 对数极坐标网格最初是由允许附近结构的精细编码与较宽上下文的粗略编码相结合的思想，以及从灵长类动物的 视野 到 V1皮层 的变换是 对数的 然而，具有非常少的径向箱的小描述符反而能给出最好的性能，因此在实践中 几乎没有不均匀性或上下文 。 我们评估了C-HOG几何的两个变体，一个具有 单个圆形中心细胞 （类似于[14]的GLOH特征），以及中心细胞被分成 角形扇区的形状上下文 。 C-HOG的4个参数： the numbers of angular(角度盒子的个数）； the numbers of radias(半径盒子个数) the radius of the central bin in pixels（中心仓的半径（以像素为单位）） the expansion factor for subsequent (半径的伸展因子） 为了良好的性能，最佳的参数设置为：4个角度盒子、2个半径盒子、中心盒子半径为4个像素、伸展因子为2 4像素是中央bin的最佳半径，但3和5给出类似的结果。 C-HOG看起来很像基于形状上下文（英语：Shape context）的方法，但不同之处是：C-HOG的区间中包含的细胞单元有多个方向通道，而基于形状上下文的方法仅仅只用到了一个单一的边缘存在数。[4] Block Normalization schemes：引入v表示一个还没有被归一化的向量，它包含了给定区间（block）的所有直方图信息。vk 表示 v 的 k 阶范数，这里的 k={1,2}。用 e 表示一个很小的常数。一共4种不同的块规范化schemes L2-morm, L2-Hys, 它可以通过先进行L2-norm，对结果进行截短（clipping），然后再重新归一化得到。 L1-norm, L1-sqrt,L1-norm followed by square root 作者发现：采用L2-Hys, L2-norm, 和 L1-sqrt方式所取得的效果是一样的，L1-norm稍微表现出一点点不可靠性。 Centre-surround normalization. Detector Window and ContextClassifier最后一步就是把提取的HOG特征输入到SVM分类器中，寻找一个最优超平面作为决策函数。作者采用的方法是：使用免费的SVMLight软件包加上HOG分类器来寻找测试图像中的行人。 Discussion 在甲酸梯度前进行任何程度的平滑处理都会毁掉HOG的结果，因为许多可供的图像信息都是从细尺度的突出边界形成的。 详单，梯度应该在当前金字塔层的最细可供尺度上被计算，修改或者用于方向投票并且只有在那之后在模糊空间。 其次，强的局部对比正常化对于良好的结果至关重要，传统的中心环绕样式方案不是最好的选择。 更好的结果可以通过相对于不同的局部支持对每个元素（边缘，单元）进行几次标准化，并将结果作为独立信号来实现。 Summary and Conclusions 我们研究了各种描述符参数的影响，并得出结论，在重叠描述符块中的 精细尺度梯度， 精细定向binning， 相对粗糙的空间binning 高质量局部对比度归一化 对于良好的性能都是重要的。]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Pedestrian Detection</tag>
        <tag>行人检测</tag>
        <tag>Object Detection</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行人检测论文笔记：Fast Feature Pyramids for Object Detection?]]></title>
    <url>%2Fblog%2F2735857030%2F</url>
    <content type="text"><![CDATA[相关知识点 Overcomplete Representations: Overcomplete：Such a complete system is overcomplete if removal of a $\phi {j}$ from the system results in a system (i.e., ${\phi {i}}_((i\in J\backslash {j))}$) that is still complete. In different research, such as signal processing and function approximation, overcompleteness can help researchers to achieve a more stable, more robust, or more compact decomposition than using a basis.[2] Image pyramid：影响金字塔 影像金字塔由原始影像按一定规则生成的由细到粗不同分辨率的影像集。 指在同一的空间参照下，根据用户需要以不同分辨率进行存储与显示，形成分辨率由粗到细、数据量由小到大的金字塔结构。 图像编码和渐进式图像传输 从图中可以看出, 从金字塔的底层开始每四个相邻的像素经过重采样生成一个新的像素, 依此重复进行, 直到金字塔的顶层。重采样的方法一般有以下三种: 双线性插值、最临近像元法、三次卷积法。 金字塔是一种能对栅格影像按逐级降低分辨率的拷贝方式存储的方法。通过选择一个与显示区域相似的分辨率，只需进行少量的查询和少量的计算，从而减少显示时间。 Gradient Histograms: Abstract The computational bottleneck of many modern detectors is the computation of features at every scale of a finely-sampled image pyramid. The approach is general and is widely applicable to vision algorithms requiring fine-grained multi-scale analysis. Introduction Multi-orientation decompostion: 多向分解，是图像处理中的基本技术。 在每个尺度和方向分别分析图像结构的想法起源于多个来源: 哺乳动物视觉系统生理学的测量 有关视觉信息的统计和编码的原则性推理 谐波分析 谐波分析（多速率滤波) 灵长类视觉系统显示：条纹皮层细胞（粗略的等价于图像的小波展开）在数量上超过视网膜神经节细胞（图像像素的近似表示）10^2到10^3. 这些表示相对于视点，照明和图像变形的变化的鲁棒性是Overcomplete Representations 优越性能的促成因素。 不幸的是，更高的检测正确率通常伴随着更高的计算开销。 在计算开销和为了提高检测和降低错误率而使用更复杂的表示之间是没有必要做权衡的。 自然图像具有分形统计，使得我们可以利用这一点来更可靠的预测图像跨尺度结构。 我们证明了我们提出的快速特征金字塔与三个不同的检测框架的有效性： 积分通道特征（ICF） 聚集通道特征（积分通道特征的新颖变体） 可变形零件模型（DPM） Related Work Scale Space Theory: 尺度空间理论 Cascades, coarse-to-fine search, distance transforms, etc., 全部都关注于对提前计算好的图像特征来优化分类速度。 本方法专注于快速特征金字塔的构建，因此与上面的方法可以起到互补的效果。 行人检测的最佳执行方法和PASCAL VOC是基于多尺度特征金字塔的滑动窗口; 快速特征金字塔非常适合于这种滑动窗口检测器。]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Pedestrian Detection</tag>
        <tag>行人检测</tag>
        <tag>Object Detection</tag>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行人检测论文笔记：Pedestrian Detection - An Evaluation of the State of the Art]]></title>
    <url>%2Fblog%2F3091185356%2F</url>
    <content type="text"><![CDATA[知识点 对数正态分布（lognormally distributed）：对数为正态分布的任意随机变量的概率分布。 如果 X 是正态分布的随机变量，则 exp(X)为对数正态分布. 如果 Y 是对数正态分布，则 ln(Y) 为正态分布。 如果一个变量可以看作是许多很小独立因子的乘积，则这个变量可以看作是对数正态分布。 对数正态分布的概率密度函数为： 对数平均：对数平均与几何平均相等，并且比算数平均，对于对数正态分布数据的典型值更具代表性 二个数字的对数平均小于其算术平均，大于几何平均，若二个数字相等，对数平均会等于算数平均及几何平均。 Histogram of Oriented Gradients for Objection Detection.(HOG)步骤： Sampling positive images Sampling negative images Training a Linear SVM Performing hard-negative mining Re-training your Linear SVM using the hard-negative samples Evaluating your classifier on your test dataset, utilizing non-maximum suppression to ignore redundant, overlapping bounding boxes NMS:Non-maximum Suppression(非极大值抑制):可看成一种局部极大值搜索，这里的局部极大值要比他的邻域值都要大。这里的邻域表示有两个参数：维度和n-邻域。 LBP: Local Binary Patterns Abstract 单目图像的行人检测方法持续的在发展。 多种数据集+广泛变化的评估方法-&gt;导致方法之间直接的比较很困难。 三个贡献： 数据集； 精炼的pre-image评估方法； 对现有state-of-art检测器进行比较评估。 行人检测在 低像素 和 部分遮挡 行人情况下依旧表现失望。 Introduction 对现有行人检测方法的一些疑问： Do current detectors work well? What is the best ap- proach?” “What are the main failure modes? What are the most productive research directions? Contributions Data set. 350,000行人标注框BB 250,000帧 遮挡和时间上相似的也被标注。 对行人等级、遮挡、位置进行了统计 Evaluation methodology. Evaluation. 评估了16个有代表性的先进的行人检测器 两个团队发布的surveys与作者的工作是互补的。 Geronimo——先进的司机助手系统中的行人检测。 Enzweiler and Gavrila——发布Daimler检测数据集 The Caltech Pedestrian Data SetData Collection and Ground Truthing 当一个标注器在至少两帧中在同一个行人上标注了边界框，则边界框利用3次插值在中间帧进行标注 BB-full BB-vis: 被遮挡行人可见区域标注框 三种标注： Person People Person? Data Set Statistics 行人的高度、宽度都类似对数正态分布。 如果多变量中的每个变量符合对数正态分布，则这些变量的线性组合也符合正态分布。 BB 横纵比 w/ h, log(w /h) = log(w)-log(h). 由于行人姿势（手、肘）的原因，会导致行人宽度变化。 h=Hf/ d. H=1.8m, d=1800 /hm 大部分行人都被观察为medium scale，为了安全系统，检测也必须发生在这个scale。 通过对遮挡情况的统计，总体来说，遮挡情况远远没有统一， 利用这个发现可以挡住提高行人检测器的性能。 不仅遮挡是高度不一致的， 遮挡的类型也是有明显额外的结构。 通过对比ground truth和HOG检测器检测出的行人位置进行同样的统计对比，有如下的图，可以发现，利用行人位置这一约束条件可以合理的加快检测但是只能适当的减少假正例。 Training and Testing Data 四种训练/测试情景 Scenario ext0: Scenario ext1: Scenario cal0: Scenario cal1: 作者应该在检测器开发过程中使用 ext0/cal0，并且只在完成所有参数后再 ext1/cal1下进行评估。 Comparison of Pedestrian Data Sets Imaging setup. Data set size. Data set type. Pedestrian scale. Data set properties. Evaluation MethodologyFull Image Evaluation 对阈值小于0.6的评估就不用关心了。 具有最高置信度的检测首先被匹配；如果一个被检测到的BB匹配到多个ground truth边界框，则具有最高覆盖率的匹配将被使用。 没有被匹配到的BBdt算作假正例，没有被匹配道德BBgt被称为假负例。 通过改变检测置信度的阈值，画出miss rate-FPPI的曲线图来比较各个检测器。这种图比准确率-召回率的图更好，因为对于汽车应用，as typically there is an upper limit on the acceptable false positives per image rate independent of pedestrian density. 利用 对数平均 遗漏率 来总结检测器的性能。在（10^-2~10^0）范围的对数空间中，通过9个FPPI率平均计算miss rate来得到 Filtering Ground Truth BBig: 被选择忽略的Ground truth. 被忽略的区域。 将BBgt设为被忽略和丢弃掉这个样本是不一样的；后者代表这个样本是一个假正例。 Filtering Detections 考虑三种可能的过滤策略： strict filtering: 在匹配之前删除所选范围之外的所有检测。 postfiltering: 在所选评价范围外的检测允许与范围内的BBgt匹配。 expanded filtering: 类似于严格过滤，除了在评估之前去除扩展评估范围之外的所有检测 Expanded filtering 在 strict filtering 和 postfiltering之间做了很好的妥协。 在整个评估工作中，我们使用expanded filtering(r=1.25)。 Standardizing Aspect Ratios 标准化GT和DT的aspect ratio，这样做会从检测器设计中删除无关的任意选择，并有助于性能比较。 一般来说，探测器的长宽比取决于开发过程中使用的数据集，通常在训练后选择。 我们建议将所有BB标准化为0.41的长宽比（Caltech数据集中的对数 - 平均长宽比）。 我们保持BB高度和中心固定，同时调整宽度. 重要的是检测器和ground truth纵横比匹配。 Per-Window versus Full Image Evaluation PM 评估方法通常用来比较分类器（检测器的返利）或者用来评估系统对于自动兴趣区域生成的性能。 PW结果是从其 原始出版物 中产生的。 全图像结果是通过评估同一行人但在其 原始图像上下文 中获得的。 将一个二分类转化为一个检测器所做的选择包括： 包括空间和尺度跨度 非最大抑制的选择。会影响图像的性能。 在PW评估期间测试的窗口通常不同于在全图像检测期间测试的窗口， 假阳性可能来自对身体部位或不正确的尺度或位置的检测 假阴性可能源于被测试的窗户和真实的行人位置或来自NMS之间的轻微不对准。 检测算法Survey of the State of the Art Papageorgiou and Poggio [16]提出了第一个滑动窗口检测器。将支持向量机（SVM）应用于多尺度Haar小波的过度完备字典。 Viola和Jones( VJ )[44]基于这些想法，引入了用于快速特征计算的积分图像和用于有效检测的级联结构，以及利用AdaBoost进行自动特征选择。这些想法继续作为现代探测器的基础。 随着基于梯度的特征的采用带来了巨大的收益。 受SIFT [45]启发，Dalal和Triggs [HOG][7]通过显示相对于基于强度的特征的实质性增益，普及了用于检测的定向梯度特征的直方图（HOG）。 现在，HOG特征的变体的数量已经大大增加，几乎所有现代检测器以某种形式利用它们。 Shape features（形状特征）也是一个用于检测经常用到的线索. Boosting用于学习头部，躯干，腿部和全身检测器. Shapelets: 是从局部区块中的梯度辨别地学习的形状描述符. Boosting用来将多个Shapelet结合成一个整体的检测器。 Motion是人类感知的另一个重要提示; 然而，成功地将运动特征结合到检测器中已证明对于移动的相机具有挑战性。 虽然没有迹象表明单个特征由于HOG，但附加特征可以提供一些互补信息。 Evaluated Detectors 直接从作者处得到提前训练好的检测器。 这些检测器通常遵循滑动窗口范例，其需要对检测窗口进行特征提取，二分类和密集多尺度扫描，随后进行非极大值抑制。 Features：几乎所有的现代检测器都使用了都写形式的梯度直方图。 Learning：因为它们的理论保证，可扩展性和良好的性能，支持向量机[16]和boosting[44]是最受欢迎的选择。 Boosting自动执行特征选择。一些检测器（在“特征学习”列中用标记指示）在分类器训练之前或与分类器训练一起学习更小或中等大小的特征集合。 Detection details：两种主要的非最大抑制方法： Mean shift(MS)模型估计 Pairwise max(PM)抑制：根据充分的重叠丢弃可信度较低的每对检测 PM*：允许检测去匹配另一检测的任意子区域。 Implementation notes Performance EvaluationPerformance on the Caltech Data Set Overall：绝对性能很弱。 Scale Occlusion Reasonale：性能在中等规模或部分封闭的行人的检测很差，而对于远距离或在重度封闭的情况下，它的性能特别差。这促使我们评估超过50像素高的行人在没有或部分遮挡（这些在没有很多上下文的情况下清晰可见）的性能。 我们将这称为合理的评估设置。 Localization Evaluation on Multiple Data SetsStatistical Significance（统计显著性） 关键的洞察力是将每个数据集上的绝对性能转换为算法排名，从而消除不同数据集难度的影响。 我们使用非对数Friedman检验和posthoc分析来分析统计学显着性. 对于我们的分析，我们使用 非参数Friedman测试 以及 Shaffer posthoc test 我们基于其对数平均丢失率（在合理的评估设置下测试）对每个数据折叠上的检测器进行排名。 该程序为14个检测器得到做总共28个排名。 Runtime Analysis 总的来说，运行时和精度之间似乎没有很强的相关性。 Discussion 应该注意，单帧性能是整个系统性能的下限，跟踪，上下文信息和额外传感器的使用可以帮助减少假警报并提高检测率（参见[2]）。]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Pedestrian Detection</tag>
        <tag>行人检测</tag>
        <tag>Review</tag>
        <tag>综述</tag>
        <tag>Caltech</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行人检测论文笔记：Pedestrian Detection - A Benchmark]]></title>
    <url>%2Fblog%2F4124170924%2F</url>
    <content type="text"><![CDATA[知识点 k折交叉验证 Non-Maximum Suppression：非极大值抑制算法，非极大值抑制（NMS）可以看做是抑制不是极大值的元素，搜索局部的极大值的搜索问题，NMS是许多计算机视觉算法的部分。 这个局部代表的是一个邻域，邻域有两个参数可变，一是邻域的维数，二是邻域的大小。 在行人检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是行人的概率最大），并且抑制那些分数低的窗口。 Abstract 引进了一个新的数据集——Caltech。 提出了了个更高的评估标准。 证明了平常用的逐个窗口检测的方法是有瑕疵的，在完整的图片上会预测失败。 衡量了现有的检测系统。 分析了一般的常见失败情况。 Introduction INRIA数据集。 现有数据集的缺陷。 贡献（4方面）。 Dataset 介绍了Caltech数据集的数据内容，标记等。 Scale(等级，范围)根据行人的图片大小，将行人分为3个范围：near（80或者更多像素）、medium（30-80像素之间）、far（30像素或更少）。 大约68%的行人位于中等大小范围。 对于medium范围的加测对于汽车应用是十分重要的。 我们应当在整个工作中利用ner/ medium /far之间的区别。 Occlusion(遮挡) 遮挡的行人通过两个框来标注。 29%的行人从来没有被挡住 53%的呗挡在一部分帧 19%的在所有帧中都被挡 Position(位置)：由于视点和地表形状的原因约束着行人值出现在图片的特定区域，经过分析，行人文职更加集中而不是突然出现的。 数据捕捉了超过11种场景:0-5用来作为训练，6-10用来作为测试 设置了三个具体的训练/测试场景 Scenario-A：在所有外部数据上进行训练，在会话6-10上进行测试。这样允许在已经存在的方法上不进行重新训练就能进行广泛的调查。 Scenario-B：利用会话0-5进行6折交叉验证，每次使用5个session来进行训练，第6个进行测试，然后在验证集上融合结果，在政策训练集上汇报检测器的表现。 Scenario-C：用0-5会话来训练，用6-10会话来测试。（完整测试） 与现有的数据集的比较： 广泛使用的‘人’数据集：MIT LabelMe的子集和PASCAL VOC数据集。 现有数据集可以分为两类：一类是人数据集包含了人的各种姿势，另一类是行人数据集包含了垂直的人（站立或者行走），但主要是从一个较为限制的视点进行观察的。 从摄影师处收集的数据集都存在 选择偏差 ，但是监控视频有着有限的背景，移动拍摄的数据会极大的排除了选择偏差。 INRIA偏向于打的，大部分未遮挡的行人 其他相关的数据集有：DC，ETH Caltech数据集最先进和重要的方面，而且这是目前第一个数据集与时间相对应的标注框和详细遮挡标签。 评估方法 现有的已建立的评估行人检测方法是有瑕疵的。 pre-window VS pre-image pre-window：逐窗口检测器在图像上被密集扫描并且邻近的检测被合并，比如使用NMS。 一个典型的假设是：较好的pre-window分数会在一整个图片上带来更好的表现；然而在实际中pre-window表现在预测pre-image性能时失败。 不是所有检测系统都是基于华东窗口的，而且pre-window方法对这类系统的评估是不可能的。 Pre-image evaluation 利用PASCAL物体检测挑战中的修改过的scheme版本进行单帧检测。 一个检测系统需要输入一个图像并且为每个检测返回一个边界框或者一个分数或者一个置信度。这个系统应该可以执行多等级检测以及必要的NMS或者其他后期处理。 评估应该在最后生成的被检测到的边界框中执行。 PASCAL估计：重叠区域必须超过50%： 为了比较方法，通过变化检测置信度的阈值，我们画出了纵坐标miss rate，横坐标每张图像假正例（FPPI）的图像。对于某些任务，更倾向于使用查准-召回曲线，比如汽车应用，典型的已经有一个可接受的FPPI上限，并且独立于行人行人密度。 引入ignore regions。这一区域不需要匹配，匹配上不算是TP，没有匹配上也不算FN。 只有完整的标注框才能用来匹配，不是可见的标注框，甚至对于部分遮挡的行人。 Evaluation Results Overall Scale Occlusion Aspect ratio]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Pedestrian Detection</tag>
        <tag>行人检测</tag>
        <tag>Review</tag>
        <tag>综述</tag>
        <tag>Caltech</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[南清北复交北航哈工大中科院华科保研记]]></title>
    <url>%2Fblog%2F3356325341%2F</url>
    <content type="text"><![CDATA[前言7月23号从中科院软件所参加完夏令营回来，我的漫长的保研路也算是告一段落。 8月12号东软实训结束，8月13号坐上回家的火车，8月14号到家，然后就一直吃喝睡到今天，拿回来的几本书也没看几眼，本来打算着回来继续充实一下，去备战9，10月份的推免，现在看来时间又都荒废了……开学还是乖乖到学校吧，再这样下去一直待在家感觉要成废人一个了，我还是喜欢忙碌充实的感觉。 一直想着要把这次宝贵的保研经历记录一下，好给学弟学妹一个参考。学弟学妹们可以结合自身情况，大概了解一下保研流程，部分学校保研考核要求，从而少走一些弯路，去到自己理想中的学校。 个人基本情况把我的个人情况大概说一下，学弟学妹可以对比自己情况，有个参考。 绩点排名：5/262 四级：611分 六级：503分 科研竞赛情况：无科研，无论文，无国奖，无励志奖学金，无ACM，仅有一个蓝桥和美赛SP（其实没什么价值），总的来说也就是成绩好一点，但有时候高的成绩排名会是一个很好的敲门砖。 为什么我要保研本来我是做iOS开发的，想要毕业后直接找工作，不准备读研的。但是直到大三上学期的时候通过和学长学姐交流，我才了解到保研夏令营这一回事，由于学长学姐都去了很好的高校读研，而且当时我的排名还不错，于是我思考了好长时间，权衡了读研和找工作的利弊，我决定毕业后继续读研。如果有同样疑惑的学弟学妹，我的建议是：能出国的话，那肯定出国；能保研的话，那肯定最好不要放弃这个来之不易的机会，当然这也不是绝对的。 我的保研路报名参加过的夏令营这一部分是我报名相应学校夏令营后，对方给予我入营资格的学校，以下是按参加顺序记录： 1. 南大计算机 网址：南京大学计算机系2016“本科生开放日”申请流程 时间：5月13日-5月15日 入营条件：985院校的话，绩点排名前5%基本可以入营 吃住补助：LAMDA实验室报销车票，住宿费，但是南大不报销车票，但管吃管住，住的很高级的宾馆，条件特别好。 参营记录： 南大的夏令营是开的最早的一个计算机夏令营，正因为开的早，很多同学都蠢蠢欲试，导致南大夏令营会有很多同学报名，这次有1000多个报名的，但是最后入营的只有300人，但只要绩点排名前5%基本可以入营。 如果你想要之后从事”机器学习与数据挖掘“相关领域的科研工作，那么南大的LAMDA实验室是一个再好不过的选择了，由周志华教授作为带头人，南大的这个实验室在国内外上知名度很高，科研能力也很厉害。 但是LAMDA实验室是单独招生的，每年会有两批保研生的申请，所以要想申请LAMDA实验室，还需要在申请南大夏令营之外单独申请这个实验室的面试考核。申请的时候需要一份简历，一份研究动机说明，一份成绩单，这三份材料是让对方了解你的唯一途径，所以需要好好准备润色，虽然我是在截止日期前一天才急急忙忙的才提交的，但还是进入了初选，拿到了LAMDA实验室的面试资格。LAMDA实验室会在南大夏令营前一天组织面试，而且给报销往返车票及一晚的住宿费，还是很不错的。 下面说一下LAMDA大概的面试流程及问题。在申报LAMDA实验室之前会填三个导师志愿，我当时填的周志华、吴建鑫、俞扬，但到了现场才知道LAMDA会把你的材料分别送给所填报的三个老师，他们分别决定是否允许你参加他们每个人的面试，如果有老师允许你面试，那么你就拿到了面试资格，否则的话，说明没有老师看上你的简历，你也就没有面试资格。所以在填报导师的时候，除非自己简历特别光鲜，否则那种特别牛老师就不要填了，相当于浪费一个机会。在面试的时候，你会先去面试选中你的老师，面试完之后，你其实可以再去面试其他该实验室的老师，从而增加你进入该实验室的机会，即使你当时没有报他的研究生。我是5月12号参加的面试，当时只有吴建鑫老师给了我面试机会，面试完他之后我又去找了詹德川老师，最后我是被詹德川老师录取。面试吴建鑫老师的时候，问题如下： 方差的计算方法，他会提前写好一个方差表达式问你对不对，如果不对的话请写出正确的表达式； 方差中的n-1含义； 如果写一个程序计算方差，那么计算一次内存访问几次； 本科做过的项目，项目内容； 机器学习了解多少，看过什么； 了不了解本人是做什么研究的。 面试詹德川老师的时候，我不知道其他人是什么情况，但我是全程英文面试，问到的问题如下，注意：以下问题需要英文作答，实在英语回答不了，可以中文，不必有太大压力： 自我介绍一下； 介绍一下做过的项目； 介绍一下梯度下降法是什么； 介绍一下牛顿迭代是什么； 什么是特征值，特征值的含义； 唠嗑。 面试完当天晚上就会有邮件通知是否面试通过，最后我是被詹德川老师录取，但是通过LAMDA面试并不代表你就能100%能进入LAMDA读研了，最重要的一关是拿到南大夏令营的优秀营员，只有这样，通过两轮考核才能进入LAMDA。 下面说一下南大夏令营的流程，主要是三部分：各个实验室介绍、机试、面试，其中最最重要的是机试，只要机试通过，90%就能拿到优秀营员。 机试这次一共4道题，以前听说6道题，只要AC出其中的两道题就肯定没问题了，多做无用，有罚时，考核方式：OJ，题型：算法+数据结构，难度：ACM一般难度的题。这次的具体题目如下： 最大子串和； 无向图最长路径 表达式求值； 给一棵树求最长的路径。 面试问题如下（由于分组面试，每组没人问题不一样）： 解释一下什么是时间复杂度； 快排的时间复杂度； 快排最坏时间复杂度为什么是$O(n^2)$，如何优化快排最坏时间复杂度； 看成绩单，问如何进行文献检索 英文回答：如何利用文献检索知识去检索一个机器学习的问题 15号上午面试完毕，提交完材料，填完导师志愿，下午就可以走了，导师志愿基本只有第一志愿有用，而且有些导师有一个点招权（反正我目前貌似只知道黄宜华老师有点招权，点招了我身边好多同学，但他们都拒绝了。。。他是搞大数据很厉害的一个老师，如果对这方面感兴趣，成绩排名又靠前的话，千万记住找这个老师报名，多一个机会！）。 南大之旅也就这样结束了，接下来会在官网上公布优秀营员的名单。 2. 复旦计算机 网址： 2016年复旦大学计算机科学技术学院和软件学院优秀大学生夏令营活动报名通知 时间：7月4日~7月8日 入营条件：对于东北大学计算机软件专业的同学，每个专业只会给一个入营资格，所以排名最高的那个人才能入营 吃住补助：报销单程的路费（无论什么以硬卧为准），管吃住，条件都还不错。 参营记录： 复旦大学计算机虽然排名没有东大计算机高，但是毕竟复旦，还在上海，所以还是很值得去试一试的，但上海的学校貌似都比较”傲娇“，给的名额真的特别少，考核也是很严格的，这次去了复旦才知道一共有600个人报名夏令营，最后只有50个人入营，而且最后在这50个人中只会发放14个拟录取，所以这个从这个录取比例可以看出竞争很激烈，还是需要认真的准备。 夏令营是在复旦大学张江校区举行，为期5天，主要活动包括： 学术报告。将邀请在学术研究方面有建树的教师进行学术报告，介绍计算机科学技术学院和软件学院最新的研究方向和研究成果，为期2天； 课题组的学术讨论。进入感兴趣的课题组和老师进行进一步的交流，参加课题组的科研活动； 可以提前联系导师，趁早去找老师唠一唠，让他认识你，了解你，而最好在面试之前就定下导师，否则面试会减分； 考核。上午机试，下午面试，面试的时候分两组，每组5个老师，老师手中会有你的机试成绩，所以机试好的话，给老师印象会很好，也就是说只要机试分数高，进复旦就容易很多了。 复旦搞计算机视觉和媒体（视频、图片）大数据分析的居多，基本很多老师都在围绕这个来展开科研工作的。 下面说一下机试。机试这次和以往完全不一样，虽然也是OJ，但这次是给你3个大题，每个大题中有3个小题，每个小题之间的区别就是约束条件和数据量级不同，对应的题目难易程度也是不一样的，所以和ACM的题型还不太一样。具体题目记得不太清楚，大概如下： 第一题类似迷宫问题，利用BFS求解，一个n*n个方格组成的方阵，里面可能有若干个门，每个门对应着一把钥匙，钥匙会出现在某个方格中，所以要想开门就必须先把钥匙拿到，你需要给出从起点到终点的可能路径之和，迷宫会有多种多样。三个小问分别是： 对于1*n的迷宫，求出问题的解。 对于n*n的迷宫，没有门，求出问题的解。 对于n*n的迷宫，有门，求出问题解。 第二题貌似是车过桥问题，由于桥有限高，所以车需要有不同的装载方案来过不同的桥，貌似需要求解出装载方案，这个题没有仔细看。 第三题很常规的一道ACM字符串题目，具体题目记不太清，但是主要考察你在特别大的数量级下能否在规定时间内求出解。 下面说一下面试。面试分两组，而且有专业面试和英语面试两个环节，每组同学到相应的组面试，面试问题大概如下： 专业面试： 自我介绍； 介绍项目，做过什么，项目具体内容是什么； 机器学习了解多少，如何学习的； 说一下神经网络的优点缺点； 自我感觉机试做的怎么样； 学院院长是谁； 选好导师没有。 英语面试： 自我介绍； 说一下媒体大数据是什么。 面试问题大概如上，面试还是很快地，面试完基本就可以走了，回去等着邮件通知是否通过即可，没有通过的还可以继续申报9月份的推免面试，这次是和本校学生一起竞争，所以竞争会更加激烈。还是一句话：”得机试者得天下“，虽然竞争激烈，但是只要机试分数高，胜算还是很大的。 3. 北航计算机 网址：报考2017年北航计算机学院硕士研究生 7月11~12日暑期宣传活动通知 时间：7月11日~7月12日 入营条件：对于985学校的学生，绩点排名前5%基本会有入营资格 吃住补助：什么都不报销！！！！不论吃的、住的！！！！ 参营记录： 从复旦回来一天后，我就到了北航参加北航计算机夏令营，北航听说这次入营的有500多人，所以不包吃住很正常，因为根本管不过来。 北航夏令营只有两天，第一天机试，第二天面试，空余时间可以提前找一找联系的老师。北航有一个免机试政策，就是有CCF（计算机职业资格认证考试）成绩的同学，只要成绩在200分以上，带着成绩单去就可以免机试，还是不错的，可以省去机试好好准备面试了。 下面说一下机试，北航机试一共就两个题，分两场，两场题目不一样，做完只会显示是否编译通过，不会有任何错误提示信息，所以做完你也不知道是不是能把所有样例都通过，比较坑。我所参加的那场题目如下： 找出一串数字中，连续递增子串的最大个数 哈弗曼树构造，编码 北航不同的是机试不通过的话，是无法参加面试，而且面试是需要交100块钱的。面试的话，你需要学会去引导老师，让他去问你知道的东西，这样你才能把自己的优势展现出来。 面试结束后，千万不要走，因为晚上会贴出拟录取名单，第二天还会给你发拟录取证明，这个是不能代领的，所以面试结束最好先别急着走，等你拿到拟录取名单，就可以安安心心回家了。 4. 哈工大计算机 网址：没有通知，我是当时加了一个哈工大保研群才知道哈工大计算机的保研面试安排，群号：212632913 时间：7月17号 入营条件：感兴趣都可以去面试 参营记录： 哈工大计算机推免面试是分面试点的，当时在东大有一个面试点，而且一两天后就会出结果和你签拟录取合同，从而省下你专门跑到哈尔滨面试，还是很人性化的。哈工大面试分三个老师分别面试，分别面试三个方面：逻辑思维、专业知识、动手能力，老师都很和蔼的，根本不用紧张。下面是面试问到的问题： 逻辑思维 给你一道逻辑题，让你选出正确答案； 家乡是哪儿，毕竟是在哈尔滨，怕有些同学适应不了环境； 高考成绩等唠嗑性问题。 专业知识 大学什么科目学的比较好 B树是什么，主要作用是什么； B树在数据库中如何应用； 给你很多学生的成绩，如何利用B树来进行检索； 反正基本围绕B树，因为是我引导的老师到这个问题上的； 机器学习了解多少。 动手能力 做过的项目； 涉及到的算法有什么。 面试完一两天基本就会出结果，我在签协议的时候，老师是这样和我说的：虽然我们不想招软件学院的学生，但是学校给的要求是：只要是985院校的学生，但凡不是特别差的，就都招了吧。。。所以，想要报哈工大或者想找一个保底的学校，最好不要放弃这个机会。 5. 中科院软件所 网址：中国科学院大学2016年全国大学生“软件与网络”夏令营通知 时间：7月18日-7月23日 入营条件：对于东北大学学生，绩点排名前20%基本都会有入营资格 吃住补助：报销去程车票，提供住宿（学生公寓、两人间、环境感觉不好），给100元的饭卡，可在食堂和超市消费。 参营记录： 原本我并没有报名软件所的夏令营，只是报名了计算所的夏令营，因为我以为中科院只能报名一个研究院。但是计算所好像不是特别欢迎软件专业的学生，所以对于计算所，软件学院入营的同学屈指可数，而计算机学院入营的同学有十几个吧，最后我也没有入营计算所。 没有入营计算所的我以为我的夏令营就这样结束了，看着身边很多同学去参加软件所的夏令营，我当时真的是特别后悔为什么脑子短路不报软件所的夏令营。但是在软件所开营的前一天，我和另一个同样没有报名的同学得到消息说没有报名可以去现场报名，于是我们当天晚上头脑一热，就买了去北京的硬座，连夜坐到北京，准备霸面。在前一天去北京的车上，我们提前联系了几个老师，说明了一下自己情况。 第二天早晨到北京后，我们直奔中科院。由于没有任何计划，也不知道去了联系哪个老师，如何临时报名，就一直在软件所里面呆坐着。幸运的是直到下午，在同学和一个软件所学姐的帮助下，我们找到了软件所研究生办事处主任李彩丽老师，提交了部分材料，办了手续，领了公寓钥匙，才算报了名。（顺便说一句，李彩丽老师人特别好，有什么问题她都会尽量帮忙的！）由于软件所夏令营持续到23号，而我和另一个同学当时22号还要去参加华科夏令营，我们经过了长时间的心理斗争，决定放弃华科的夏令营。至此，我的软件所夏令营才幸运地开始，所以，保研过程中的许多机会都需要去争取的，运气也是很重要的，即使有时候觉得不可能，也要试一试，说不定运气好就得到了这个机会！ 下面正式介绍一下软件所夏令营。软件所夏令营为期6天，来来回回基本就一个星期了。这6天里，第一天报道，然后接下来两天一样的听报告，但是在第二天听报告的下午需要填报两个实验室的志愿，这个志愿其实只有第一志愿有用，填完志愿后会当场统计人数，看有没有扎堆，如果有的话，可以当场改志愿，让每个实验室人数尽量均衡。关于各个实验室的好坏，这里有一篇挺公正的介绍——中科院软件所各实验室情况简要介绍，总的来说：软工中心最好，人机最不受欢迎吧，学弟学妹填志愿的时候要注意，当然最好的实验室报的人也最多，录取比例当然更低。我当时报名的是天基和国重。 报完志愿后，第二天各个实验室就都开始各自的考核了，有的有机试、笔试、面试，有的只有笔试和面试，由于我报的天基，那么我只能说一说天基的考核方式了。天基只有笔试和面试，笔试的话其实考的都是很基础的东西，包括OS、计网、机组（考了很多选择）、数学、数据结构、算法等，平常认真考试的话，基本没什么问题。 下面说一下面试，面试最主要的就是3分钟的个人介绍PPT，所有老师提问都是通过你的PPT来提问，所以这个PPT需要废话少说，把你最精彩的部分讲出来，但是所有部分需要尽量真实，最好不要给自己挖坑跳就可以了，自我介绍完会有一个英文问答题目，然后就是老师提问时间了，我被问到的问题有： 机器学习了解哪些算法； 逻辑回归和线性回归的区别是什么； 如果想要进王浩老师组（因为我提前联系的这个老师），想做什么？ 软件所面试大概就是这样了，接下来就可以回去等官网公示优秀营员了。这次的录取比例没有说的那么高，除国重实验室很多老师单独招生比较特殊外，所有实验室基本是50%的录取率。 需要提一下，天基里面只有王浩老师的研究组还不错，其他的话就千万不要考虑了！！！！ 报名未能参加的夏令营这一部分是我报名相应学校夏令营后，对方没有给予我入营资格或者由于某些原因我没有去的学校。 1. 清华计算机 网址：关于举办2016年暑期全国优秀大学生夏令营的预通知 时间：7月16日－7月18日 入营条件：我觉得基本只有专业第一可以去吧，除非你在科研竞赛特别优秀，否则基本没戏。注意这个只能是直博。 具体介绍：参考信息安全章博亨大神的日志：南大、清华、北大、上交、中科院、北航等高校夏令营保送经历 2. 北大信科 网址：北京大学信息科学技术学院关于举办2016年信息学科优秀大学生夏令营的通知（第一轮）-北京大学信息科学技术学院 时间：7月13日-7月15日 入营条件：北大信科除了对绩点排名要求前5%外，还需要有很高的综合素质，所以成绩不是唯一因素，这次入营的软件工程专业的同学有两个，分别排名第2名和第11名。 具体介绍：参考章博亨大神的日志：南大、清华、北大、上交、中科院、北航等高校夏令营保送经历，还有胡少晗大神的保研经历（目前还没有链接，有了的话我会更新） 3. 上交计算机 网址：上交计算机没有夏令营，只有一个直硕面试，这个是没有网址的，但是这个和上交软件夏令营是同时开始报名的，可以关注一下：关于上海交通大学“2017软件工程优才夏令营”的通知 时间：貌似是7月2号，挤不太清楚了，反正只有一天时间，可以当天去当天回。 入营条件：前面有提到，上交计算机只给一个专业一个直硕名额、一个直博名额，所以谁名次高，谁就有机会去（也不一定，还是报名试试吧）。 具体介绍：对于我个人来说，我当初以为我能获得面试资格，就提前联系了几个老师，其中和申瑞民老师和朱其立老师分别进行了视频面试，他们都同意只要我获得上交直硕面试资格，参加考核，就收我作为他们的研究生，可惜我并没有获得直硕面试资格，所以比较遗憾。（朱其立老师英文名是Kenny Zhu，这个老师可能从国外回来的老师，所以他和我Skype的是时候全程英文交流，还是需要准备一下，具体考核方式可以参照章博亨大神的日志上交那部分：南大、清华、北大、上交、中科院、北航等高校夏令营保送经历） 4. 中科院计算所 网址：“计算未来”全国大学生计算技术暑期研修班招生简章 时间：7月17-7月23日 入营条件：之前提到，计算所不太喜欢收软件学院的学生，所以每个专业只给一两个名额，谁名次高谁就可以去 具体介绍：参考章博亨大神的日志：南大、清华、北大、上交、中科院、北航等高校夏令营保送经历 5. 华科 网址：关于举办“2016年华中科技大学计算机学院优秀大学生夏令营”活动的通知 时间：7月21-7月22日 入营条件：985排名前15%，或者你在竞赛方面有突出表现+可以获得保研资格 具体介绍： 华科由于和软件所时间重了，所以我没有去，据说华科只有机试，3道题，一个半小时，人工判题，比较简单，985的同学去了基本可以拿到优秀营员，也是个不错的选择。 一些体会通过这次夏令营，有以下几个体会，也当做给学弟学妹的一些建议吧。 保研不像想象中的那么容易。很多985院校的同学以为只要成绩好，保研到好学校很容易，其实并不是，因为有很多你不认识的人，他们比你学校好，成绩优异，科研竞赛经历丰富。所以成绩并不能代表一切，它最多只能是一块儿敲门砖，把你带到你想进的夏令营，但当你进入夏令营后，决定你水平的不单单是成绩，更重要的是综合素质，比如基础知识、编程能力、语言表达能力等种种因素。如何利用你三年学到的知识拿到优秀营员才是关键。不要总想着拿成绩说事，try to prove it! 抓住一切可能的机会。保研的路上，你可能觉得身心俱疲，可能觉得这个机会没什么价值，可能觉得这个机会哪有那么容易获得，如果是那么也不会是留给我的，我还是一遍歇着吧。但我想说的是，千万不要因为你的懒惰，你的想当然，让一个又一个机会从你手中溜走，因为任何事情只有尝试后，你才有资格评价，而且很多时候，这个机会就是为你准备的，你不去争取，你就一无所获。就像阿姆的《Lose yourself》歌词所说：Look, if you had one shot, or one opportunity to seize everything you ever wanted. In one moment, would you capture it, or just let it slip? 我想选择前者总是没有错的。 机会留是给有准备的人。通过这次保研，我感受最深的就是机试，真的是”得机试者得天下“。机试一直是我的薄弱之处，虽然在努力刷题提高机试水平，但是由于我没有参加过ACM，意识到的时间晚，没有时间去准备机试，所以短短的时间是无法有质的飞跃，导致我在机试上摔了一次又一次，”成功地“与很多到手的机会失之交臂，痛悔不已。所以我希望学弟学妹们一定要好好准备机试，没事多刷题（C/C++，千万不要用Java），到时候才会有临危不乱，秒杀众生的感觉，只要夏令营机试过了，你也就基本没问题了。当然，面试也是要好好准备的，经常复习四大专业课，高数、高代、概率论等数学课，这样到时候大概过一遍就行了。还有一年时间，争取多参加一些科研和竞赛，这是很加分的，如果你能通过这些”套到“一个好导师，何乐而不为呢！最后要说的是，尽早确定下来将来读研的研究方向，早一点去看一些相关专业书，争取利用剩下的一年时间跟老师做一做相关方向的科研，这会对你的简历增加不少光彩！So go for it!]]></content>
      <categories>
        <category>经验</category>
      </categories>
      <tags>
        <tag>保研</tag>
      </tags>
  </entry>
</search>
